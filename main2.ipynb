{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web-based interface of the chatbot interface: https://gonzalorecio.com/chatbot/robot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  BertModel, BertTokenizer\n",
    "import requests\n",
    "from sentiments.BERTGRU_model import BERTGRUSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "URL_INTERFACE = 'https://gonzalorecio.com/chatbot/robot.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotFace:\n",
    "    '''Chatbot face according to sentiment analysis. '''\n",
    "    chatbot_mood_API = 'https://chatbot-mood.herokuapp.com/mood'\n",
    "    chatbot_status_API = 'https://chatbot-mood.herokuapp.com/internal_state'\n",
    "\n",
    "    def change_mood(self, mood):\n",
    "        req_obj = {'action': mood}\n",
    "        x = requests.post(self.chatbot_mood_API, data=req_obj)\n",
    "        # print(x)\n",
    "\n",
    "    def change_status(self, status):\n",
    "        req_obj = {'status': status}\n",
    "        x = requests.post(self.chatbot_status_API, data=req_obj)\n",
    "\n",
    "    def status_custom(self, text):\n",
    "        self.change_status(text)\n",
    "\n",
    "    def predict_sentiment(self, sentence):\n",
    "        model_path = 'BERT_sentiment.pt'\n",
    "        max_input_length =  512\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        device = 'cpu'\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        model = BERTGRUSentiment(bert,\n",
    "                                 hidden_dim=256,\n",
    "                                 output_dim=1,\n",
    "                                 n_layers=2,\n",
    "                                 bidirectional=True,\n",
    "                                 dropout=0.25)\n",
    "        model.to(device)\n",
    "        model.load_state_dict(torch.load(model_path,map_location=torch.device('cpu')))\n",
    "        model.eval()\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        tokens = tokens[:max_input_length - 2]\n",
    "\n",
    "        # Special tokens:\n",
    "        init_token = tokenizer.cls_token  # Initiate sentence\n",
    "        eos_token = tokenizer.sep_token  # End of sentence\n",
    "\n",
    "        # Special tokens idx\n",
    "        init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "        eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "\n",
    "        # Sentence index\n",
    "        indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "\n",
    "        # To tensor\n",
    "        tensor = torch.LongTensor(indexed).to(device)\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "        prediction = torch.sigmoid(model(tensor))\n",
    "\n",
    "        return prediction.item()\n",
    "\n",
    "    def mood_prediction(self, utterance):\n",
    "\n",
    "        score = self.predict_sentiment(utterance)\n",
    "\n",
    "        self.status_custom(utterance)\n",
    "\n",
    "        if 'how' or 'what' or 'why' or 'which' or 'when' in utterance:\n",
    "            self.change_mood('confused')\n",
    "        else:\n",
    "            if score <= 0.55 and score > 0.45:\n",
    "                self.change_mood('neutral')\n",
    "            elif score <= 1 and score > 0.55:\n",
    "                self.change_mood('happy')\n",
    "            elif score <= 0.45 and score > 0:\n",
    "                self.change_mood('sad')\n",
    "\n",
    "\n",
    "    def start_blinking(self):\n",
    "        self.change_mood('start_blinking')\n",
    "\n",
    "    def stop_blinking(self):\n",
    "        self.change_mood('stop_blinking')\n",
    "\n",
    "    def status_listening(self):\n",
    "        self.change_status('listening')\n",
    "\n",
    "    def status_thinking(self):\n",
    "        self.change_status('thinking')\n",
    "\n",
    "    def status_none(self):\n",
    "        self.change_status('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPEECH\n",
    "from speech import google_speech as speech\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelWithLMHead\n",
    "from google_trans_new import google_translator\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import webbrowser as web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speech import google_speech as speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# speech.speech_to_audio(lang=\"en-US\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola china \n"
     ]
    }
   ],
   "source": [
    "from google_trans_new import google_translator  \n",
    "translator = google_translator()  \n",
    "translate_text = translator.translate('สวัสดีจีน', lang_tgt='es')  \n",
    "print(translate_text)\n",
    "#output: Hello china"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    device ='cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self, lang='en', skip_instructions=False):\n",
    "\n",
    "        web.open(\"https://gonzalorecio.com/chatbot/robot.html\")\n",
    "        self.face = ChatbotFace()\n",
    "        print('Loading model and tokenizers...')\n",
    "        self.face.status_custom('Loading Models')\n",
    "        # Initialize chatbot interface\n",
    "\n",
    "        # Define chatbot voice\n",
    "        self.define_voice(lang=lang)\n",
    "\n",
    "        # Load model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "        self.model.eval()  # set model to evaluation\n",
    "        for param in self.model.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            self.model = self.model.to(device)\n",
    "        #self.tokenizer = AutoTokenizer.from_pretrained(\"ncoop57/DiGPTame-medium\")\n",
    "        #self.model = AutoModelWithLMHead.from_pretrained(\"ncoop57/DiGPTame-medium\")\n",
    "        self.face.status_none()\n",
    "        \n",
    "        # Display instructions:\n",
    "        if skip_instructions:\n",
    "            self.initial_instructions(lang=lang)\n",
    "\n",
    "        # Reset chatbot\n",
    "        self.reset_chatbot(lang=lang)\n",
    "        #self.speak('Welcome to our chatbot interface. In a few seconds you can start a conversation with me. Now I am loading the system, I will be ready in a second!')\n",
    "\n",
    "        # Welcome message\n",
    "        if lang == 'en':\n",
    "            self.face.status_custom('Hey I am CIRREL bot!')\n",
    "            self.speak('Hey my name is CIRREL, and I am a chatbot developed by Gonzalo Recio and Jana Reventós.   '\n",
    "                       'In a few seconds you can start a conversation with me.   '\n",
    "                       'In the blink of an eye, I will be ready.')\n",
    "        else:\n",
    "            self.face.status_custom('Hola, me llamo CIRREL!')\n",
    "            self.speak(\n",
    "                'Hola mi nombre es CIRREL, y soy un chatbot creado por Gonzalo Recio y Jana Reventós.   '\n",
    "                'En unos segundos podrás empezar una conversación conmigo.   '\n",
    "                'Estaré preparado en menos de un abrir y cerrar de ojos!')\n",
    "\n",
    "        print('Chatbot ready.')\n",
    "\n",
    "\n",
    "    def initial_instructions(self,lang='en'):\n",
    "        if lang == 'en':\n",
    "            text1 = 'Please read the following instructions (1-7) before starting the conversation:'\n",
    "        else:\n",
    "            text1 = 'Porfavor lea las siguientes instrucciones (1-7) antes de empezar la conversación:'\n",
    "        self.face.status_custom(text1)\n",
    "        self.speak(text1)\n",
    "\n",
    "        # Instruction 1\n",
    "        if lang == 'en':\n",
    "            inst1  = '1. This is a test for our CIR subject final project.'\n",
    "        else:\n",
    "            inst1 = '1. Esto es un test para nuestro proyecto de la asignatura de CIR.'\n",
    "        self.face.status_custom(inst1)\n",
    "        self.speak(inst1)\n",
    "\n",
    "        # Instruction 2\n",
    "        if lang == 'en':\n",
    "            inst2 = '2. We aim to test this conversational system with real users.'\n",
    "        else:\n",
    "            inst2 = '2. Nuestro objetivo es realizar una prueba de nuestro sistema de conversación con usuarios reales.'\n",
    "\n",
    "        self.face.status_custom(inst2)\n",
    "        self.speak(inst2)\n",
    "\n",
    "        # Instruction 3\n",
    "        if lang == 'en':\n",
    "            inst3 = '3. This session will be recorded to test the reaction of the users during the interaction with the chatbot.'\n",
    "        else:\n",
    "            inst3 = '3. Esta sesión será grabada para evaluar la reacción de los usuarios durante la interacción con el chatbot.'\n",
    "\n",
    "        self.face.status_custom(inst3)\n",
    "        self.speak(inst3)\n",
    "\n",
    "        # Instruction 4\n",
    "        if lang == 'en':\n",
    "            inst4 = '4. When you read the phrase: I AM LISTENING in the screen, you have to speak.'\n",
    "        else:\n",
    "            inst4 = '4. Cuando leas la frase: I AM LISTENING en la pantalla, deberás hablar.'\n",
    "        self.face.status_custom(inst4)\n",
    "        self.speak(inst4)\n",
    "\n",
    "        # Instruction 5\n",
    "        if lang == 'en':\n",
    "            inst5 = '5. Please wait while the chatbot is thinking an answer. You will read the word THINKING in the screen.'\n",
    "        else:\n",
    "            inst5 = '5. Porfavor, espera a que el chatbot pinse su respuesta. Vas a leer la palabra ZINKING en la pantalla.'\n",
    "        self.face.status_custom(inst5)\n",
    "        self.speak(inst5)\n",
    "\n",
    "        # Instruction 6\n",
    "        if lang == 'en':\n",
    "            inst6 = '6. Once the answer is ready, the chatbot will talk to you.'\n",
    "        else:\n",
    "            inst6 = '6. Una vez la respuesta este preparada, el chatbot te va a contestar.'\n",
    "        self.face.status_custom(inst6)\n",
    "        self.speak(inst6)\n",
    "\n",
    "        # Instruction 7\n",
    "        if lang == 'en':\n",
    "            inst7 = '7. Then, you can answer the chatbot again.'\n",
    "        else:\n",
    "            inst7 = '7. Cuando hayas escuchado la respuesta podrás volver a responder al chatbot.'\n",
    "        self.face.status_custom(inst7)\n",
    "        self.speak(inst7)\n",
    "\n",
    "    def define_voice(self,lang='en'):\n",
    "        if lang == 'es':\n",
    "            self.voice = 0  # 0: spanish female, 1: english female, 2: english male\n",
    "        else:\n",
    "            self.voice = 2  # 0: spanish female, 1: english female, 2: english male\n",
    "\n",
    "    def reset_chatbot(self, lang='en'):\n",
    "        self.chat_history_ids = self.tokenizer.encode(self.tokenizer.bos_token, return_tensors='pt')\n",
    "        self.translator = google_translator()\n",
    "        self.lang = lang\n",
    "\n",
    "    def listen_and_get_question(self):\n",
    "        self.face.status_listening()\n",
    "        lang = 'en-US' if self.lang == 'en' else 'es-ES'\n",
    "        question = speech.speech_to_audio()\n",
    "        self.question_original = question\n",
    "        if self.lang == 'es':\n",
    "            question = self.translate(question, lang='en')\n",
    "        self.face.status_none()\n",
    "        return question\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return self.tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "\n",
    "    def generate_answer(self, question):\n",
    "        self.face.status_thinking()\n",
    "        question_ids = self.tokenizer.encode(question + self.tokenizer.eos_token, return_tensors='pt')\n",
    "        input_ids = torch.cat([self.chat_history_ids.to('cpu'), question_ids.to('cpu')], dim=-1)\n",
    "        self.chat_history_ids = self.model.generate(input_ids.to(device), max_length=50,\n",
    "                                                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                                                    no_repeat_ngram_size=5,\n",
    "                                                    repetition_penalty=1\n",
    "                                                   )\n",
    "        answer_ids = self.chat_history_ids[:, input_ids.shape[-1]:][0]\n",
    "        text = self.decode(answer_ids)\n",
    "        if self.lang == 'es':\n",
    "            text = self.translate(text, lang='es')\n",
    "        self.face.status_none()\n",
    "        return text\n",
    "\n",
    "    def get_sentiment_analysis(self, text):\n",
    "        return True\n",
    "\n",
    "    def speak(self, text):\n",
    "        self.face.change_mood('happy')\n",
    "        speech.text_to_speech(text, speed = 170, vol=1.0, voice=self.voice)\n",
    "        self.face.change_mood('neutral')\n",
    "\n",
    "    def translate(self, text, lang):\n",
    "        translated_text = self.translator.translate(text, lang_tgt=lang)\n",
    "        if type(translated_text) == list:\n",
    "            return translated_text[0]\n",
    "        return translated_text\n",
    "\n",
    "    def no_understand(self):\n",
    "        text = \"Sorry, I couldn't understand you\"\n",
    "        if self.lang == 'es':\n",
    "            text = self.translate(text, lang='es')\n",
    "        self.face.change_mood('cofused')\n",
    "        self.speak(text)\n",
    "        self.face.change_mood('neutral')\n",
    "\n",
    "    def run_chat(self):\n",
    "\n",
    "        self.face.change_mood('neutral')\n",
    "        # Chatbot welcome\n",
    "        self.reset_chatbot(lang=self.lang)\n",
    "        question = ''\n",
    "        previous_answer =''\n",
    "        while (question != 'goodbye'):\n",
    "            question = self.listen_and_get_question()\n",
    "            if self.question_original is None:\n",
    "                print('Fallo')\n",
    "                self.no_understand()\n",
    "                continue\n",
    "            print('User:', self.question_original)\n",
    "            answer = self.generate_answer(question)\n",
    "            self.face.mood_prediction(answer)\n",
    "            self.face.status_custom(answer)\n",
    "            print('CIRREL chatbot:', answer)\n",
    "            self.speak(answer)\n",
    "            if answer != previous_answer:\n",
    "                previous_answer = answer\n",
    "                self.chat_history_ids = self.chat_history_ids[:, -50:]\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                if self.lang == 'en':\n",
    "                    self.face.status_custom('This conversation has ended')\n",
    "                    self.speak('This conversation has ended. It has been a pleasure talking with you. Thank you! ')\n",
    "                    print('Conversation ended')\n",
    "                else:\n",
    "                    self.face.status_custom('Esta conversación ha terminado.')\n",
    "                    self.speak('Esta conversación ha terminado. Ha sido un placer hablar contigo. Muchas gracias!')\n",
    "                    print('Conversación terminada.')\n",
    "                break\n",
    "        self.face.status_none()\n",
    "            # self.reset_chatbot(lang=self.lang)\n",
    "            # print(self.chat_history_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizers...\n",
      "Chatbot ready.\n",
      "Listening...\n",
      "Recognizing...\n",
      "Could not understand audio\n",
      "Fallo\n",
      "Listening...\n",
      "Recognizing...\n",
      "tiempo de\n",
      "User: tiempo de\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input, output and indices must be on the current device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9c02ab51e911>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mchatbot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChatbot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'es'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mchatbot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_chat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-371b8cb610e5>\u001b[0m in \u001b[0;36mrun_chat\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'User:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquestion_original\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_answer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmood_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_custom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CIRREL chatbot:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-75adc854f562>\u001b[0m in \u001b[0;36mmood_prediction\u001b[1;34m(self, utterance)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmood_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutterance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutterance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_custom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutterance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-75adc854f562>\u001b[0m in \u001b[0;36mpredict_sentiment\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Workspace\\Master\\CIR\\Final Project\\MAI-CIR\\sentiments\\BERTGRU_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0membedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# embedded text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# embedded = [batch size, sent len, emb dim]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         embedding_output = self.embeddings(\n\u001b[1;32m--> 843\u001b[1;33m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    844\u001b[0m         )\n\u001b[0;32m    845\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m         return F.embedding(\n\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input, output and indices must be on the current device"
     ]
    }
   ],
   "source": [
    "chatbot = Chatbot(lang='es')\n",
    "chatbot.run_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
