{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Did you see \"Titanic\"?\n",
      "DialoGPT: I did, but I don't think I can remember it.\n",
      "User: I saw it twelve times.\n",
      "DialoGPT: I saw it once.\n",
      "User: I have the DVD.\n",
      "DialoGPT: I have the CD.\n",
      "User: Let's go to your home.\n",
      "DialoGPT: I have a DVD.\n",
      "User: And then we can go to my home.\n",
      "DialoGPT: I'm going to go to your home.\n",
      "User: I always cry at the end.\n",
      "DialoGPT: I cry at the end.\n"
     ]
    }
   ],
   "source": [
    "user.to('cpu')\n",
    "user.eval()\n",
    "# Let's chat for 5 lines\n",
    "sentences = [\"Did you see \\\"Titanic\\\"?\",\n",
    "            \"I saw it twelve times.\",\n",
    "            \"I have the DVD.\",\n",
    "            \"Let's go to your home.\",\n",
    "            \"And then we can go to my home.\",\n",
    "            \"I always cry at the end.\"]\n",
    "for step in range(len(sentences)):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "#     new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    new_user_input_ids = tokenizer.encode(sentences[step] + tokenizer.eos_token, return_tensors='pt')\n",
    "    print(\"User:\", sentences[step])\n",
    "    \n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = user.generate(bot_input_ids, \n",
    "                                     max_length=1000, \n",
    "                                     pad_token_id=tokenizer.eos_token_id, \n",
    "                                     no_repeat_ngram_size=5,\n",
    "#                                      repetition_penalty=1.1\n",
    "                                     \n",
    "                                     )\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CUDA\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "# epsilon_by_frame = lambda frame_idx: max(epsilon_final, epsilon_start*(0.995**frame_idx))\n",
    "plt.plot([epsilon_by_frame(i) for i in range(5000)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_td_loss(batch_size):\n",
    "#     state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "#     state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "#     next_state = Variable(torch.FloatTensor(np.float32(next_state)), requires_grad=False)\n",
    "#     action     = Variable(torch.LongTensor(action))\n",
    "#     reward     = Variable(torch.FloatTensor(reward))\n",
    "#     done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "# #     q_values      = model(state)\n",
    "# #     next_q_values = model(next_state)\n",
    "\n",
    "# #     q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "# #     next_q_value     = next_q_values.max(1)[0]\n",
    "# #     expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "#     loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "# #     for param in model.parameters():\n",
    "# #             param.grad.data.clamp_(-1, 1)\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(model, state, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "        state   = Variable(torch.FloatTensor(state).unsqueeze(0), requires_grad=False)\n",
    "        q_value = model.forward(state)\n",
    "        action  = q_value.max(1)[1].item()\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        #action = random.randrange(env.action_space.n)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_step(action):\n",
    "    question = user.generate(action, max_length=1000, \n",
    "                             pad_token_id=tokenizer.eos_token_id, \n",
    "                             num_beams=3,\n",
    "                             num_return_sequences=2,\n",
    "                             early_stopping=True,\n",
    "                             no_repeat_ngram_size=2\n",
    "                            ) if frame > 0 else chat_history_ids\n",
    "    \n",
    "    reward -= jaccard_similarity(answer.numpy(), context.numpy())\n",
    "    return new_state, reward, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(token_ids):\n",
    "    return tokenizer.decode(token_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    size = len(r)\n",
    "    discounted_r = torch.zeros(size)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = answers[-1]\n",
    "# # torch.tensor(x.size())\n",
    "# y = model(x)[0]\n",
    "# w = y.sum(dim=1)\n",
    "# q = w/w\n",
    "# q.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.base_model.forward(answers[-2], output_hidden_states=True)['last_hidden_state'].max(dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(questions, answers):\n",
    "    model.train()\n",
    "    t1 = questions[-1].to(device)\n",
    "    t2 = answers[-1].to(device)\n",
    "#     print(t1)\n",
    "    A = model(t1)[0]\n",
    "#     print(t2, len(t2))\n",
    "    B = model(t2)[0]\n",
    "    lenA = list(A.size())[0]\n",
    "    lenB = list(B.size())[0]\n",
    "#     print(lenA, lenB)\n",
    "#     lenA = len(A.detach().numpy())\n",
    "#     lenB = len(B.detach().numpy())\n",
    "    max_len = np.max([lenA, lenB])\n",
    "#     extra = torch.zeros(size=(max_len-len(A.detach().numpy()),tokenizer.vocab_size))\n",
    "#     extra[:,-1] = 1\n",
    "    \n",
    "    if lenA > lenB:\n",
    "        extra = torch.zeros(size=(max_len-lenB,tokenizer.vocab_size)).to(device)\n",
    "        extra[:,-1] = 1\n",
    "        A , B = torch.softmax(A, dim=-1), torch.cat([torch.softmax(B, dim=-1), extra]), \n",
    "    \n",
    "    else:\n",
    "        extra = torch.zeros(size=(max_len-lenA,tokenizer.vocab_size)).to(device)\n",
    "        extra[:,-1] = 1\n",
    "        A , B = torch.cat([torch.softmax(A, dim=-1), extra]), torch.softmax(B, dim=-1)\n",
    "        \n",
    "    # loss = F.cosine_similarity(A,B, dim=-2)\n",
    "#     loss = -torch.mm(A, B.transpose(1, 0)).sum()\n",
    "\n",
    "    # loss = -F.cosine_similarity(torch.softmax(A, dim=-1),torch.softmax(B, dim=-1))\n",
    "    \n",
    "    def log_prob(tokens, debug=False):\n",
    "        # p = log(P(b|a)) / N\n",
    "        output_logits = model(tokens.to(device))[0]\n",
    "        p = 1\n",
    "        if debug: print('logits', output_logits, 'tokens', tokens)\n",
    "        for t, logit in zip(tokens, output_logits):\n",
    "            p *= torch.softmax(logit, dim=-1)[t]\n",
    "        if debug: print('p', p)\n",
    "        p_log = torch.log(p) / len(tokens) # (tokens/tokens).sum()# / len(tokens) # lenB # len(tokens_b)\n",
    "        if p == 0:\n",
    "            print('infinite')\n",
    "            return torch.log(p+10e-10)\n",
    "        else:\n",
    "            return p_log\n",
    "\n",
    "    def log_prob_r1(tokens, debug=False):\n",
    "        # p = log(P(b|a)) / N\n",
    "        output_logits = model(tokens.to(device))[0]\n",
    "        p = 0\n",
    "        if debug: print('logits', output_logits, 'tokens', tokens)\n",
    "        for t, logit in zip(tokens, output_logits):\n",
    "            p += torch.softmax(logit, dim=-1)[t]\n",
    "        if debug: print('p', p)\n",
    "        p_log = p / len(tokens) # (tokens/tokens).sum()# / len(tokens) # lenB # len(tokens_b)\n",
    "        if p == 0:\n",
    "            print('infinite')\n",
    "            return torch.log(p+10e-10)\n",
    "        else:\n",
    "            return p_log\n",
    "    # reward 1\n",
    "    x = [log_prob_r1(d) for d in dummy_responses[:30]]\n",
    "    r1 = torch.stack(x)\n",
    "    r1 = -torch.mean(r1) # if r1 else 0\n",
    "    \n",
    "    # reward 2\n",
    "#     model.transformer.wte.weight[text_index,:]\n",
    "#     if len(answers)<2:\n",
    "#         emb1 = model.get_input_embeddings()(t1).max(dim=0)[0]\n",
    "#     else:\n",
    "#         emb1 = model.get_input_embeddings()(answers[-2]).max(dim=0)[0]\n",
    "#     emb2 = model.get_input_embeddings()(t2).max(dim=0)[0]\n",
    "#     r2 = -torch.log(F.cosine_similarity(emb1,emb2,dim=-1))\n",
    "\n",
    "#     emb1 = model.get_input_embeddings()(t1).max(dim=0)[0]\n",
    "#     emb2 = model.get_input_embeddings()(t2).max(dim=0)[0]\n",
    "#     r2_2 = -torch.log(F.cosine_similarity(emb1,emb2,dim=-1))\n",
    "    \n",
    "    \n",
    "    if len(answers)<2:\n",
    "        emb1 = model.base_model.forward(t1, output_hidden_states=True)['last_hidden_state'].max(dim=0)[0]\n",
    "    else:\n",
    "        emb1 = model.base_model.forward(answers[-2], output_hidden_states=True)['last_hidden_state'].max(dim=0)[0]\n",
    "    emb2 = model.base_model.forward(t2, output_hidden_states=True)['last_hidden_state'].max(dim=0)[0]\n",
    "#     r2 = -torch.log(torch.clamp(F.cosine_similarity(emb1,emb2,dim=-1), min=1e-9))\n",
    "    r2 = -F.cosine_similarity(emb1,emb2,dim=-1)\n",
    "\n",
    "    emb1 = model.base_model.forward(t1, output_hidden_states=True)['last_hidden_state'].max(dim=0)[0]\n",
    "    emb2 = model.base_model.forward(t2, output_hidden_states=True)['last_hidden_state'].max(dim=0)[0]\n",
    "#     r2_2 = -torch.log(torch.clamp(F.cosine_similarity(emb1,emb2,dim=-1), min=1e-9))\n",
    "    r2_2 = -F.cosine_similarity(emb1,emb2,dim=-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # reward 3\n",
    "    r3 = log_prob_r1(t2, debug=False)\n",
    "#     print(t2, t2.size())\n",
    "    \n",
    "#     y = model(t2)[0]\n",
    "#     w = y.sum(dim=1)\n",
    "#     q = w/w\n",
    "#     r4 = q.sum()\n",
    "    print('r1:', r1, 'r2:', r2, 'r2_2:', r2_2, 'r3:', r3)\n",
    "    \n",
    "    R = 0.25*r1 + 0.25*r2 + 0.25*r2_2 #+ 0.5*r3 #* 0.01*r4\n",
    "#     R = 1*r2 + 1*r2_2 #+ 0.5*r3 #* 0.01*r4\n",
    "\n",
    "#     print(R)\n",
    "    return -R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.base_model.parameters():\n",
    "#     print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_responses = [torch.tensor([tokenizer.eos_token_id]),\n",
    "                   tokenizer.encode(\"1\" + tokenizer.eos_token, return_tensors='pt')[0],\n",
    "                   tokenizer.encode(\"I\" + tokenizer.eos_token, return_tensors='pt')[0],\n",
    "                   tokenizer.encode(\"I'm\" + tokenizer.eos_token, return_tensors='pt')[0],\n",
    "                   tokenizer.encode(\"I\", return_tensors='pt')[0],\n",
    "                   tokenizer.encode(\"..\" + tokenizer.eos_token, return_tensors='pt')[0],\n",
    "                   tokenizer.encode(\"!\" + tokenizer.eos_token, return_tensors='pt')[0],\n",
    "                   tokenizer.encode(\":D\" + tokenizer.eos_token, return_tensors='pt')[0],\n",
    "                   tokenizer.encode(\"I don't know what you're talking about\"+ tokenizer.eos_token, return_tensors='pt')[0],\n",
    "                   tokenizer.encode(\"I don't know\"+ tokenizer.eos_token, return_tensors='pt')[0],\n",
    "                   tokenizer.encode(\"I don't know, I don't know\"+ tokenizer.eos_token, return_tensors='pt')[0],\n",
    "                   tokenizer.encode(\"You don't know\"+ tokenizer.eos_token, return_tensors='pt')[0],\n",
    "                  ]\n",
    "# for i in range(256):\n",
    "#     s = tokenizer.encode(tokenizer.decode([i]) + tokenizer.eos_token, return_tensors='pt')[0]\n",
    "#     dummy_responses.append(s)\n",
    "# dummy_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(290-34):\n",
    "#     print(tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(chat_history_ids)[0].max(dim=1)[0][0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(chat_history_ids)[0].max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dummy_sentence(sent):\n",
    "    for d in dummy_responses:\n",
    "        if sent.size()[0] == d.to(device).size()[0] and all(sent == d.to(device)):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in list(model.base_model.parameters())[-5:]:\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'models/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True\n",
    "if USE_CUDA:\n",
    "    device ='cuda'\n",
    "    model  = model.cuda()\n",
    "    user   = user.cuda()\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    model  = model.cpu()\n",
    "#     user = user.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello\n",
      "DialoGPT: Hey buddieinoeusaurusesaurusesaurusesaurusaurusesaurusesauriaurusesaurusesauraaurusesaurusesaurosaurusesaurusesaunaaurusesaurusesaunosaurusaurusesaurusesauseaurusesaurusesaulosaurusaurusesaurulesaurusesaurusesausaurusaurusesaurulesaurusaurusesauruitsaurusesaurusesaultauseaurusesauruitsauseaurusesaurulesauseaurusesauruesaurusesaurusesaveauseaurusesaur\n",
      "r1: tensor(-9.6185e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7496, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7817, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.0095, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3828,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3828, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3828, grad_fn=<UnbindBackward>)\n",
      "Episode 0: 0.06360395330779915\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: Hey buddieinozzaaccaaccaaccaaccaaccaacioaccaaccaaccaaccaattaaccaaccaaccaaccaataaaaaccaaccaaccaaccaullaaccaaccaaccaaccaastaaccaaccaaccaaccaacaaccaaccaaccaaccaecaaccaaccaaccaaccaabbaaccaaccaaccaaccaaccualyssuperbiaaccaaccaaccaaccaacccccccccccomboaccaaccaaccaaccaccaaccaaccaaccaaccaacebookacebookacebookacebookacebookaccaacebookacebookacebookaccaaccaacebookacebookaccaacebookaccaacebookacebookaccaaccaaccaacebook\n",
      "r1: tensor(-0.0002, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7646, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8297, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3985,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3985, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3985, grad_fn=<UnbindBackward>)\n",
      "Episode 1: 0.06402156840106403\n",
      "User: Hello\n",
      "DialoGPT: Hey buddieinoeuseuseuseuseususeuseuseuseusauseuseuseuseusauhauseuseuseuseusatuseuseuseuseusuruseuseuseuseususeseuseuseuseusasuseuseuseuseususkeuseuseuseususseuseuseuseususaleuseuseuseusluseuseuseuseususaeuseuseuseusxuseuseuseuseuscuseuseuseuseusopuseuseuseuseusuxeuseuseuseusauxeuseuseuseusupitereuseus\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7194, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8595, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4771, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3947,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3947, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3947, grad_fn=<UnbindBackward>)\n",
      "Episode 2: 0.06443335431318262\n",
      "User: Hello\n",
      "DialoGPT: Hey buddieinoeuseuseuseuseususeuseuseuseusauseuseuseuseusauhauseuseuseuseusatuseuseuseuseusuruseuseuseuseususeseuseuseuseusasuseuseuseuseususkeuseuseuseususseuseuseuseususaleuseuseuseusluseuseuseuseususaeuseuseuseusxuseuseuseuseuscuseuseuseuseusopuseuseuseuseusuxeuseuseuseusauxeuseuseuseusupitereuseus\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8098, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8628, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5278, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4181,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4181, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4181, grad_fn=<UnbindBackward>)\n",
      "Episode 3: 0.06487325673392366\n",
      "User: Hello\n",
      "DialoGPT: Hey buddieinoiiiiiiiiiiiiiiiiiiiiiiiiiindaisyeahhhhhhhhhhhhhhhhhhhhhhhhhttttttttttttttyyyysssshhttttttttyysssshhttttttyysssshhthttttttyysshthttttttysssshhttttttysshttttttyysshttttttyysshhhhttttttyysshhhhhttttttyysshhhttttttyysssshttttttysshthttttyysshttttyysshtttttt\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7206, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7107, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.0695, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3578,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3578, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3578, grad_fn=<UnbindBackward>)\n",
      "Episode 4: 0.06523713386149048\n",
      "User: Hello\n",
      "DialoGPT: Hey buddieinozzaaccaaccaaccaaccaaccaacioaccaaccaaccaaccaattaaccaaccaaccaaccaataaaaaccaaccaaccaaccaullaaccaaccaaccaaccaastaaccaaccaaccaaccaacaaccaaccaaccaaccaecaaccaaccaaccaaccaabbaaccaaccaaccaaccaaccualyssuperbiaaccaaccaaccaaccaacccccccccccomboaccaaccaaccaaccaccaaccaaccaaccaaccaacebookacebookacebookacebookacebookaccaacebookacebookacebookaccaaccaacebookacebookaccaacebookaccaacebookacebookaccaaccaaccaacebook\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8270, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6736, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3002, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3751,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3751, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3751, grad_fn=<UnbindBackward>)\n",
      "Episode 5: 0.06562161750608486\n",
      "User: Hello\n",
      "DialoGPT: Hey buddieinozzaaccaaccaaccaaccaaccaacioaccaaccaaccaaccaattaaccaaccaaccaaccaataaaaaccaaccaaccaaccaullaaccaaccaaccaaccaastaaccaaccaaccaaccaacaaccaaccaaccaaccaecaaccaaccaaccaaccaabbaaccaaccaaccaaccaaccualyssuperbiaaccaaccaaccaaccaacccccccccccomboaccaaccaaccaaccaccaaccaaccaaccaaccaacebookacebookacebookacebookacebookaccaacebookacebookacebookaccaaccaacebookacebookaccaacebookaccaacebookacebookaccaaccaaccaacebook\n",
      "r1: tensor(-9.2376e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8392, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5829, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3273, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3555,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3555, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3555, grad_fn=<UnbindBackward>)\n",
      "Episode 6: 0.0659808457593631\n",
      "User: Hello\n",
      "DialoGPT: Hey buddieinozzaaccaaccaaccaaccaaccaacioaccaaccaaccaaccaattaaccaaccaaccaaccaataaaaaccaaccaaccaaccaullaaccaaccaaccaaccaastaaccaaccaaccaaccaacaaccaaccaaccaaccaecaaccaaccaaccaaccaabbaaccaaccaaccaaccaaccualyssuperbiaaccaaccaaccaaccaacccccccccccomboaccaaccaaccaaccaccaaccaaccaaccaaccaacebookacebookacebookacebookacebookaccaacebookacebookacebookaccaaccaacebookacebookaccaacebookaccaacebookacebookaccaaccaaccaacebook\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6754, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8479, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2944, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3808,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3808, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3808, grad_fn=<UnbindBackward>)\n",
      "Episode 7: 0.06637048029679676\n",
      "User: Hello\n",
      "DialoGPT: Hey buddieinozzaaccaaccaaccaaccaaccaacioaccaaccaaccaaccaattaaccaaccaaccaaccaataaaaaccaaccaaccaaccaullaaccaaccaaccaaccaastaaccaaccaaccaaccaacaaccaaccaaccaaccaecaaccaaccaaccaaccaabbaaccaaccaaccaaccaaccualyssuperbiaaccaaccaaccaaccaavoriteavoriteavoriteavoriteavoritefavoriteavoriteavoriteavoriteavoriteadelphiaavoriteavoriteavoriteavoriteavoravoriteavoriteavoriteavoriteavouravoriteavoriteavoriteavoriteavoredavoriteavoriteavoriteavoriteceryavoriteavoriteavoriteavoritepertyavoriteavorite\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7376, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8122, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4746, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3874,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3874, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3874, grad_fn=<UnbindBackward>)\n",
      "Episode 8: 0.06676733326132332\n",
      "User: Hello\n",
      "DialoGPT: Hey buddieinozzaaccaaccaaccaaccaaccaacioaccaaccaaccaaccaattaaccaaccaaccaaccaataaaaaccaaccaaccaaccaullaaccaaccaaccaaccaastaaccaaccaaccaaccaacaaccaaccaaccaaccaecaaccaaccaaccaaccaabbaaccaaccaaccaaccaaccualyssuperbiaaccaaccaaccaaccaavoriteavoriteavoriteavoriteavoritefavoriteavoriteavoriteavoriteavoriteadelphiaavoriteavoriteavoriteavoriteavoravoriteavoriteavoriteavoriteavouravoriteavoriteavoriteavoriteavoredavoriteavoriteavoriteavoriteceryavoriteavoriteavoriteavoritepertyavoriteavorite\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7764, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8201, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3647, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3991,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3991, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3991, grad_fn=<UnbindBackward>)\n",
      "Episode 9: 0.0671776060147188\n",
      "User: Hello\n",
      "DialoGPT: Hey buddieinoeuseuseuseuseususeuseuseuseusauseuseuseuseusauciuseuseuseuseusatuseuseuseuseusuruseuseuseuseususeseuseuseuseusasuseuseuseuseususkeuseuseuseususseuseuseuseususaleuseuseuseusluseuseuseuseusxuseuseuseuseuscuseuseuseuseususaeuseuseuseusopuseuseuseuseusupitereuseuseuseusauruseuseuseuseusetuseuseus\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8584, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6939, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6162, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3880,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3880, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3880, grad_fn=<UnbindBackward>)\n",
      "Episode 10: 0.06757324028119407\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: Hey buddieinoeuseuseuseuseususeuseuseuseusauseuseuseuseusauciuseuseuseuseusatuseuseuseuseusuruseuseuseuseususeseuseuseuseusasuseuseuseuseususkeuseuseuseususseuseuseuseususaleuseuseuseusluseuseuseuseusxuseuseuseuseuscuseuseuseuseususaeuseuseuseusopuseuseuseuseusupitereuseuseuseusauruseuseuseuseusetuseuseus\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8915, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8560, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5376, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4369,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4369, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4369, grad_fn=<UnbindBackward>)\n",
      "Episode 11: 0.06802802219048712\n",
      "User: Hello\n",
      "DialoGPT: Hey buddieinoeuseuseuseuseususeuseuseuseusauseuseuseuseusauciuseuseuseuseusatuseuseuseuseusuruseuseuseuseususeseuseuseuseusasuseuseuseuseususkeuseuseuseususseuseuseuseususaleuseuseuseusluseuseuseuseusxuseuseuseuseuscuseuseuseuseususaeuseuseuseusopuseuseuseuseusupitereuseuseuseusauruseuseuseuseusetuseuseus\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2605, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8528, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5813, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2783,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2783, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2783, grad_fn=<UnbindBackward>)\n",
      "Episode 12: 0.06828665130268806\n",
      "User: Hello\n",
      "DialoGPT: Hey buddieinozzaaccaaccaaccaaccaaccaacioaccaaccaaccaaccaattaaccaaccaaccaaccaataaaaaccaaccaaccaaccaullaaccaaccaaccaaccaastaaccaaccaaccaaccaavoriteavoriteavoriteavoriteavoritefavoriteavoriteavoriteavoriteavoriteavouravoriteavoriteavoriteavoriteavoravoriteavoriteavoriteavoriteadelphiaavoriteavoriteavoriteavoriteavoredavoriteavoriteavoriteavoriteracuseavoriteavoriteavoriteavoritevantageavoriteavoriteavoriteavoritepertyavoriteavoriteavoriteavoriteceryavoriteavoriteavoriteavoriteovieavoriteavoriteavoriteavoritelevisionavoriteavoriteavoriteavoriteoptionavorite\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6612, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8037, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3662,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3662, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3662, grad_fn=<UnbindBackward>)\n",
      "Episode 13: 0.06865263583781586\n",
      "User: Hello\n",
      "DialoGPT: Hey buddieinozzaaccaaccaaccaaccaaccaacioaccaaccaaccaaccaattaaccaaccaaccaaccaataaaaaccaaccaaccaaccaullaaccaaccaaccaaccaastaaccaaccaaccaaccaavoriteavoriteavoriteavoriteavoritefavoriteavoriteavoriteavoriteavoriteavouravoriteavoriteavoriteavoriteavoravoriteavoriteavoriteavoriteadelphiaavoriteavoriteavoriteavoriteavoredavoriteavoriteavoriteavoriteracuseavoriteavoriteavoriteavoritevantageavoriteavoriteavoriteavoritepertyavoriteavoriteavoriteavoriteceryavoriteavoriteavoriteavoriteovieavoriteavoriteavoriteavoritelevisionavoriteavoriteavoriteavoriteoptionavorite\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5849, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6842, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3173,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3173, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3173, grad_fn=<UnbindBackward>)\n",
      "Episode 14: 0.06895766826803608\n",
      "User: Hello\n",
      "DialoGPT: Hiya lovely lady lady lady lady lady woman lady lady lady lady human lady lady lady lady robot lady robot lady robot human lady robot human lady lady robot human lady human robot human lady robot lady robot robot human lady robot robot human robot human lady human lady robot human robot humanoid humanoid humanoid humanoid humanoid humanoid robot humanoid humanoid humanoid humanoidoid humanoidoid humanoidoidoidoidoidoidroidoidoidoidoidoidsoidoidoidoid humanoidoidoidroidoidroidoidoidroidoidoidsoidoidroid\n",
      "r1: tensor(-0.0004, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7799, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6390, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3546,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3546, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3546, grad_fn=<UnbindBackward>)\n",
      "Episode 15: 0.06930775485656626\n",
      "User: Hello\n",
      "DialoGPT: Hiya lovely lady lady lady lady lady woman lady lady lady lady human lady lady lady lady robot lady robot lady robot human lady robot human lady lady robot human lady human robot human lady robot lady robot robot human lady robot robot human robot human lady human lady robot humanoid robot humanoid robot humanoid robot humanoid robot robot humanoid robot Humanoid robot humanoid humanoid robot humanoid humanoid humanoid robot humanoid humanoid humanoid robot Humanoid robot Humanoid humanoid robot Humanoids Humanoids Humanoids Humans\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5941, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6107, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3012,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3012, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3012, grad_fn=<UnbindBackward>)\n",
      "Episode 16: 0.06959156327193541\n",
      "User: Hello\n",
      "DialoGPT: Hiya lovely lady lady lady lady lady woman lady lady lady lady human lady lady lady lady robot lady robot lady robot human lady robot human lady lady robot human lady machine human lady robot human robot human lady robot lady robot robot human lady robot robot human robot humanoid robot humanoid robot humanoid robot humanoid robot robot humanoid robot Humanoid robot humanoid humanoid robot humanoid humanoid humanoid robot humanoid humanoid humanoid robot Humanoid robot Humanoid humanoid robot Humanoids Humanoids Humanoids Humansoids\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8049, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5889, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.0802, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3484,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3484, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3484, grad_fn=<UnbindBackward>)\n",
      "Episode 17: 0.06993242710188924\n",
      "User: Hello\n",
      "DialoGPT: Hiya lovely ladymaidmaidmaidmaidmaid maidmaidmaidmaidmaidmothermaidmaidmaidmaidladmaidmaidmaidmaidlasslasslasslasslassladlasslasslasslassmaidlasslasslassladladlasslasslassladmaidlasslasslassmaidmaidlasslassmaidlassmaidlasslassmaidmaidmaidlassmaidlassmaidmaidlassmaidmaidmaidmaidlotlasslasslasslasslotlasslasslassmaidladlasslasslassmaidlotlasslassmaidlassladlasslassmaidlasslotlasslass\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7406, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6090, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4206, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3374,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3374, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3374, grad_fn=<UnbindBackward>)\n",
      "Episode 18: 0.07025895677950411\n",
      "User: Hello\n",
      "DialoGPT: Hiya lovely ladymaidmaidmaidmaidmaid maidmaidmaidmaidmaidmothermaidmaidmaidmaidladmaidmaidmaidmaidlasslasslasslasslassladlasslasslasslassmaidlasslasslassladladlasslasslassladmaidlasslasslassmaidmaidlasslassmaidlassmaidlasslassmaidmaidmaidlassmaidlassmaidmaidlassmaidmaidmaidmaidlotlasslasslasslasslotlasslasslassmaidladlasslasslassmaidlotlasslassmaidlassladlasslassmaidlasslotlasslass\n",
      "r1: tensor(-9.2623e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7066, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5178, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4333, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3061,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3061, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3061, grad_fn=<UnbindBackward>)\n",
      "Episode 19: 0.07054653482045978\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaoluluoluluoluluoluluohooluluoluluoluluoluluoyaoluluoluluoluluoluluolandoluluoluluoluluoluluokiaoluluoluluoluluoluluokaoluluoluluoluluoluluogooluluoluluoluluoluluolooluluoluluoluluoluluolioluluoluluoluluoluluopaoluluoluluoluluolululahomaoluluoluluoluluoluluneapolisoluluoluluoluluoluluinkioluluoluluoluluolulukokoluluoluluoluluoluluboaoluluoluluoluluoluluokooluluolulu\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6720, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5441, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6811, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3040,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3040, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3040, grad_fn=<UnbindBackward>)\n",
      "Episode 20: 0.07083086368098093\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaoluluoluluoluluoluluohooluluoluluoluluoluluoyaoluluoluluoluluoluluolandoluluoluluoluluoluluokiaoluluoluluoluluoluluokaoluluoluluoluluoluluogooluluoluluoluluoluluolooluluoluluoluluoluluolioluluoluluoluluoluluopaoluluoluluoluluolululahomaoluluoluluoluluoluluneapolisoluluoluluoluluoluluinkioluluoluluoluluolulukokoluluoluluoluluoluluboaoluluoluluoluluoluluokooluluolulu\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6753, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5172, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6809, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2981,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2981, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2981, grad_fn=<UnbindBackward>)\n",
      "Episode 21: 0.07110734688120367\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluneapolisoluluoluluoluluoluluucklandoluluoluluoluluoluluohaoluluoluluoluluoluluolandoluluoluluoluluoluluokiaoluluoluluoluluoluluinkioluluoluluoluluoluluoyaoluluoluluoluluoluluohooluluoluluoluluoluluogooluluoluluoluluoluluokaoluluoluluoluluoluluoaoluluoluluoluluolululahomaoluluoluluoluluoluluolooluluoluluoluluoluluolioluluoluluoluluolulukokoluluoluluoluluoluluokooluluoluluoluluoluluokioluluolulu\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6613, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7272, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6789, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3471,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3471, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3471, grad_fn=<UnbindBackward>)\n",
      "Episode 22: 0.0714426887094893\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluneapolisoluluoluluoluluoluluucklandoluluoluluoluluoluluohaoluluoluluoluluoluluolandoluluoluluoluluoluluokiaoluluoluluoluluoluluinkioluluoluluoluluoluluoyaoluluoluluoluluoluluohooluluoluluoluluoluluogooluluoluluoluluoluluokaoluluoluluoluluoluluoaoluluoluluoluluolululahomaoluluoluluoluluoluluolooluluoluluoluluoluluokooluluoluluoluluolulukokoluluoluluoluluoluluolioluluoluluoluluoluluokioluluolulu\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.4039, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6463, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6332, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2625,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2625, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2625, grad_fn=<UnbindBackward>)\n",
      "Episode 23: 0.07167458300901468\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemovieovieovieovieoviefilmovieovieovieoviemoviemoviemoviemoviemovie moviemoviemoviemoviemovieMovieMovieMovieMovieMoviemoviemoviemoviemovie MovieMovieMovieMovieMovie MovieMovieMovieMoviemovieMovieMovieMoviemovie MovieMovieMoviemoviemovieMovieMoviemoviemovie moviemovieMovieMovieMovie Movie MovieMovieMovieMovie Moviemoviemoviemoviemovie theater\n",
      "r1: tensor(-9.1963e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.4679, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6853, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2681, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2883,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2883, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2883, grad_fn=<UnbindBackward>)\n",
      "Episode 24: 0.07193712210960009\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen NguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyenginxguyenguyenguyenguyenngguyenguyenguyenguyenigonguyenguyenguyenguyengueguyenguyenguyenguyenNGguyenguyenguyenguyenUGEguyenguyenguyenguyenUGguyenguyenguyenguyenGUguyenguyenguyenguyenGVguyenguyenguyenguyenGCguyenguyenguyenguyenGiguyenguyenguyenguyenVGguyenguyen\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5323, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5823, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.7056, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2786,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2786, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2786, grad_fn=<UnbindBackward>)\n",
      "Episode 25: 0.0721873465477834\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemessageessageessageessageessageascriptessageascriptessageascriptascriptessageascriptessageessageascriptessageessageessageascriptascriptessageessageascriptascriptascriptessageascriptascriptascriptascriptessageessageessageessageTEXTascriptascriptessageascriptTEXTascriptTEXTascriptTEXTTEXTascriptTEXTascriptascriptTEXTascriptTEXTessageascriptTEXTascriptascriptascriptTEXTascriptascript TEXTascriptTEXTascriptTEXT TEXT\n",
      "r1: tensor(-9.9718e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6963, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6638, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3400,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3400, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3400, grad_fn=<UnbindBackward>)\n",
      "Episode 26: 0.07251116700026745\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemessageessageessageessageessageascriptessageascriptessageascriptascriptessageascriptessageessageascriptessageessageessageascriptascriptessageessageascriptascriptascriptessageascriptascriptascriptascriptessageessageessageessageTEXTascriptascriptessageascriptTEXTascriptTEXTascriptTEXTTEXTascriptTEXTascriptascriptTEXTascriptTEXTessageascriptTEXTascriptascriptascriptTEXTascriptascript TEXTascriptTEXTascriptTEXT TEXT\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6505, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6801, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1645, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3326,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3326, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3326, grad_fn=<UnbindBackward>)\n",
      "Episode 27: 0.07282530957245795\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemessageessageessageessageessageascriptessageascriptessageascriptascriptessageascriptessageessageascriptessageessageessageascriptascriptessageessageascriptascriptascriptessageascriptascriptascriptascriptessageessageessageessageTEXTascriptascriptessageascriptTEXTascriptTEXTascriptTEXTTEXTascriptTEXTascriptascriptTEXTascriptTEXTessageascriptTEXTascriptascriptascriptTEXTascriptascript TEXTascriptTEXTascriptTEXT TEXT\n",
      "r1: tensor(-9.7764e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5875, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6897, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3193,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3193, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3193, grad_fn=<UnbindBackward>)\n",
      "Episode 28: 0.07312259223566034\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemessageessageessageessageessageascriptessageascriptessageascriptascriptessageascriptessageessageascriptessageessageessageascriptascriptessageessageascriptascriptascriptessageascriptascriptascriptascriptessageessageessageessageTEXTascriptascriptessageascriptTEXTascriptTEXTascriptTEXTTEXTascriptTEXTascriptascriptTEXTascriptTEXTessageascriptTEXTascriptascriptascriptTEXTascriptascript TEXTascriptTEXTascriptTEXT TEXT\n",
      "r1: tensor(-9.1820e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6830, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6021, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3213,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3213, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3213, grad_fn=<UnbindBackward>)\n",
      "Episode 29: 0.07342155013760798\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemessageessageessageessageessageascriptessageascriptessageascriptascriptessageascriptessageessageascriptessageessageessageascriptascriptessageessageascriptascriptascriptessageascriptascriptascriptascriptessageessageessageessageTEXTascriptascriptessageascriptTEXTascriptTEXTascriptTEXTTEXTascriptTEXTascriptascriptTEXTascriptTEXTessageascriptTEXTascriptascriptascriptTEXTascriptascript TEXTascriptTEXTascriptTEXT TEXT\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6581, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6588, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1309, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3292,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3292, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3292, grad_fn=<UnbindBackward>)\n",
      "Episode 30: 0.0737293371757006\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemessageessageessageessageessageascriptessageascriptessageascriptascriptessageascriptessageessageascriptessageessageessageascriptascriptessageessageascriptascriptascriptessageascriptascriptascriptascriptessageessageessageessageTEXTascriptascriptessageascriptTEXTascriptTEXTascriptTEXTTEXTascriptTEXTascriptascriptTEXTascriptTEXTessageascriptTEXTascriptascriptascriptTEXTascriptascript TEXTascriptTEXTascriptTEXT TEXT\n",
      "r1: tensor(-8.5207e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6503, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.4019, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1759, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2630,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2630, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2630, grad_fn=<UnbindBackward>)\n",
      "Episode 31: 0.07395686210936848\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemovieovieovieovieoviefilmovieovieovieoviemoviemoviemoviemoviemovie moviemoviemoviemoviemovieMovieMovieMovieMovieMoviemoviemoviemoviemovie MovieMovieMovieMovieMovie MovieMovieMovieMoviemovieMovieMovieMoviemovie MovieMovieMoviemoviemovieMovieMoviemoviemovie moviemovieMovieMovieMovie Movie MovieMovieMovieMovie Moviemoviemoviemoviemovie theater\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7284, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7114, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3599,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3599, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3599, grad_fn=<UnbindBackward>)\n",
      "Episode 32: 0.0743001452401182\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemovieovieovieovieoviefilmovieovieovieoviemoviemoviemoviemoviemovie moviemoviemoviemoviemovieMovieMovieMovieMovieMoviemoviemoviemoviemovie MovieMovieMovieMovieMovie MovieMovieMovieMoviemovieMovieMovieMoviemovie MovieMovieMoviemoviemovieMovieMoviemoviemovie moviemovieMovieMovieMovie Movie MovieMovieMovieMovie Moviemoviemoviemoviemovie theater\n",
      "r1: tensor(-0.0005, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5530, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7051, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2270, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3144,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3144, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3144, grad_fn=<UnbindBackward>)\n",
      "Episode 33: 0.0745880508553058\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemovieovieovieovieoviefilmovieovieovieoviemoviemoviemoviemoviemovie moviemoviemoviemoviemovieMovieMovieMovieMovieMoviemoviemoviemoviemovie MovieMovieMovieMovieMovie MovieMovieMovieMoviemovieMovieMovieMoviemovie MovieMovieMoviemoviemovieMovieMoviemoviemovie moviemovieMovieMovieMovie Movie MovieMovieMovieMovie Moviemoviemoviemoviemovie theater\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6322, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7344, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2653, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3416,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3416, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3416, grad_fn=<UnbindBackward>)\n",
      "Episode 34: 0.07490783845894262\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemovieovieovieovieoviefilmovieovieovieoviemoviemoviemoviemoviemovie moviemoviemoviemoviemovieMovieMovieMovieMovieMoviemoviemoviemoviemovie MovieMovieMovieMovieMovie MovieMovieMovieMoviemovieMovieMovieMoviemovie MovieMovieMoviemoviemovieMovieMoviemoviemovie moviemovieMovieMovieMovie Movie MovieMovieMovieMovie Moviemoviemoviemoviemovie theater\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6249, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5529, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2944,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2944, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2944, grad_fn=<UnbindBackward>)\n",
      "Episode 35: 0.07517040697311866\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemovieovieovieovieoviefilmovieovieovieoviemoviemoviemoviemoviemovie moviemoviemoviemoviemovieMovieMovieMovieMovieMoviemoviemoviemoviemovie MovieMovieMovieMovieMovie MovieMovieMovieMoviemovieMovieMovieMoviemovie MovieMovieMoviemoviemovieMovieMoviemoviemovie moviemovieMovieMovieMovie Movie MovieMovieMovieMovie Moviemoviemoviemoviemovie theater\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7280, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7229, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2144, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3627,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3627, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3627, grad_fn=<UnbindBackward>)\n",
      "Episode 36: 0.07551391714663115\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemovieovieovieovieoviefilmovieovieovieoviemoviemoviemoviemoviemovie moviemoviemoviemoviemovieMovieMovieMovieMovieMoviemoviemoviemoviemovie MovieMovieMovieMovieMovie MovieMovieMovieMoviemovieMovieMovieMoviemovie MovieMovieMoviemoviemovieMovieMoviemoviemovie moviemovieMovieMovieMovie Movie MovieMovieMovieMovie Moviemoviemoviemoviemovie theater\n",
      "r1: tensor(-7.9254e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5222, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7427, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3162,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3162, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3162, grad_fn=<UnbindBackward>)\n",
      "Episode 37: 0.07580113099712371\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemovieovieovieovieoviefilmovieovieovieoviemoviemoviemoviemoviemovie moviemoviemoviemoviemovieMovieMovieMovieMovieMoviemoviemoviemoviemovie MovieMovieMovieMovieMovie MovieMovieMovieMoviemovieMovieMovieMoviemovie MovieMovieMoviemoviemovieMovieMoviemoviemovie moviemovieMovieMovieMovie Movie MovieMovieMovieMovie Moviemoviemoviemoviemovie theater\n",
      "r1: tensor(-9.6539e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7055, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6939, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2769, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3498,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3498, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3498, grad_fn=<UnbindBackward>)\n",
      "Episode 38: 0.07612774349666945\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemovieovieovieovieoviefilmovieovieovieoviemoviemoviemoviemoviemovie moviemoviemoviemoviemovieMovieMovieMovieMovieMoviemoviemoviemoviemovie MovieMovieMovieMovieMovie MovieMovieMovieMoviemovieMovieMovieMoviemovie MovieMovieMoviemoviemovieMovieMoviemoviemovie moviemovieMovieMovieMovie Movie MovieMovieMovieMovie Moviemoviemoviemoviemovie theater\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7157, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5846, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2369, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3250,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3250, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3250, grad_fn=<UnbindBackward>)\n",
      "Episode 39: 0.07642406354647219\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemovieovieovieovieoviefilmovieovieovieoviemoviemoviemoviemoviemovie moviemoviemoviemoviemovieMovieMovieMovieMovieMoviemoviemoviemoviemovie MovieMovieMovieMovieMovie MovieMovieMovieMoviemovieMovieMovieMoviemovie MovieMovieMoviemoviemovieMovieMoviemoviemovie moviemovieMovieMovieMovie Movie MovieMovieMovieMovie Moviemoviemoviemoviemovie theater\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6846, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.4296, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2665, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2785,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2785, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2785, grad_fn=<UnbindBackward>)\n",
      "Episode 40: 0.07666438164114155\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemovieovieovieovieoviefilmovieovieovieoviemoviemoviemoviemoviemovie moviemoviemoviemoviemovieMovieMovieMovieMovieMoviemoviemoviemoviemovie MovieMovieMovieMovieMovie MovieMovieMovieMoviemovieMovieMovieMoviemovie MovieMovieMoviemoviemovieMovieMoviemoviemovie moviemovieMovieMovieMovie Moviemoviemoviemoviemovie theater theater theater theater theater cinema\n",
      "r1: tensor(-9.2825e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7271, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8030, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2745, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3825,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3825, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3825, grad_fn=<UnbindBackward>)\n",
      "Episode 41: 0.07702761086759173\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemovieovieovieovieoviefilmovieovieovieoviemoviemoviemoviemoviemovie moviemoviemoviemoviemovieMovieMovieMovieMovieMoviemoviemoviemoviemovie MovieMovieMovieMovieMovie MovieMovieMovieMoviemovieMovieMovieMoviemovie MovieMovieMoviemoviemovieMovieMoviemoviemovie moviemovieMovieMovieMovie Moviemoviemoviemoviemovie theater theater theater theater theater cinema\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7621, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7695, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3829,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3829, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3829, grad_fn=<UnbindBackward>)\n",
      "Episode 42: 0.07739041374404147\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluneapolisoluluoluluoluluoluluicagoicagoicagoicagoicagooluluoluluoluluoluluadelphiaoluluoluluoluluoluluucklandoluluoluluoluluolulualoreoluluoluluoluluoluluminghamoluluoluluoluluoluluuayoluluoluluoluluoluluualaoluluoluluoluluoluluohaoluluoluluoluluolululahomaoluluoluluoluluoluluoyaoluluoluluoluluoluluolandoluluoluluoluluoluluiamioluluoluluoluluoluluboaoluluoluluoluluoluluinkioluluoluluoluluoluluogooluluoluluolulu\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7702, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5919, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6688, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3405,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3405, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3405, grad_fn=<UnbindBackward>)\n",
      "Episode 43: 0.07770214987183828\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluneapolisoluluoluluoluluoluluicagoicagoicagoicagoicagooluluoluluoluluoluluadelphiaoluluoluluoluluoluluucklandoluluoluluoluluolulualoreoluluoluluoluluoluluminghamoluluoluluoluluoluluuayoluluoluluoluluoluluualaoluluoluluoluluoluluohaoluluoluluoluluolululahomaoluluoluluoluluoluluoyaoluluoluluoluluoluluolandoluluoluluoluluoluluiamioluluoluluoluluoluluboaoluluoluluoluluoluluinkioluluoluluoluluoluluogooluluoluluolulu\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6015, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5536, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5293, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2887,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2887, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2887, grad_fn=<UnbindBackward>)\n",
      "Episode 44: 0.07795190520132698\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluadelphiaadelphiaadelphiaadelphiaadelphiaoluluoluluoluluoluluneapolisoluluoluluoluluoluluucklandoluluoluluoluluoluluicagooluluoluluoluluoluluuayuayuayuayuayoluluuayuayuayoluluoluluuayuayoluluuayoluluuayuayoluluoluluoluluuayoluluuayoluluoluluuayoluluoluluoluluoluluualaoluluuayuayuayualauayuayuayuayualaoluluuayuayoluluualauayuayuayoluluualaoluluuayoluluuayualauayuay\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2781, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7518, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2574,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2574, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2574, grad_fn=<UnbindBackward>)\n",
      "Episode 45: 0.07816407727308783\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluadelphiaadelphiaadelphiaadelphiaadelphiaoluluoluluoluluoluluneapolisoluluoluluoluluoluluucklandoluluoluluoluluoluluicagooluluoluluoluluoluluuayuayuayuayuayoluluuayuayuayoluluoluluuayuayoluluuayoluluuayuayoluluoluluoluluuayoluluuayoluluoluluuayoluluoluluoluluoluluualaoluluuayuayuayualauayuayuayuayualaoluluuayuayoluluualauayuayuayoluluualaoluluuayoluluuayualauayuay\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8051, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8195, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4061,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4061, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4061, grad_fn=<UnbindBackward>)\n",
      "Episode 46: 0.0785512969402515\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw havent seen anyone else use it yet havent seen anyone mention it yet either havent seen anyone use it yet hafthemovieovieovieovieoviefilmovieovieovieoviemoviemoviemoviemoviemovie moviemoviemoviemoviemovieMovieMovieMovieMovieMoviemoviemoviemoviemovie MovieMovieMovieMovieMovie MovieMovieMovieMoviemovieMovieMovieMoviemovie MovieMovieMoviemoviemovieMovieMoviemoviemovie moviemovieMovieMovieMovie Moviemoviemoviemoviemovie theater theater theater theater theater cinema\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6182, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7274, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3364,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3364, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3364, grad_fn=<UnbindBackward>)\n",
      "Episode 47: 0.07885535063061705\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw hahaahaahaahaahaahaahahahaahahahaahahahahahahahahahahahaahahahahahaahahahaahaahahahahahahahaahaahahahaahaahaahahahahahaahaahaahaahahaahahahahahahahahaahahahahahaahahaahahahaahahahahaahahahaahaahahaahaahahahahahahaahaahahahaahahaahaahaahahahahaahaahaahaohoohoohoohoohoahahohoohoohoohooboohoohoohoohoollahoho\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8193, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8173, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3305, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4091,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4091, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4091, grad_fn=<UnbindBackward>)\n",
      "Episode 48: 0.07924433642212911\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw hahaahaahaahaahaahaahahahaahahahaahahahahahahahahahahahaahahahahahaahahahaahaahahahahahahahaahaahahahaahaahaahahahahahaahaahaahaahahaahahahahahahahahaahahahahahaahahaahahahaahahahahaahahahaahaahahaahaahahahahahahaahaahahahaahahaahaahaahahahahaahaahaahaohoohoohoohoohoahahohoohoohoohooboohoohoohoohoollahoho\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6758, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7527, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3382, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3571,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3571, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3571, grad_fn=<UnbindBackward>)\n",
      "Episode 49: 0.07957123877316274\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw props for beating Duke UNC UNC UNC UNC UNC NC State UNC UNC UNC UNC Duke UNC UNC UNC Duke NC State UNC UNC Duke UNC Duke UNC UNC Duke UNC NC State UNC Duke UNC UNC NC State NC State UNC UNC NC State Chapel Hill UNC UNC UNC UNC Chapel Hill UNC UNC Chapel Hill Chapel Hill UNC UNC Duke UNC Chapel Hill UNC Chapel Hill UNC Duke UNC ChapelHill UNC UNC Chapel Hill NC State Chapel Hill Chapel Hill Chapel Hill NC State UNC Chapel Hill ChapelHill UNC Chapel\n",
      "r1: tensor(-8.3825e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7740, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6024, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3441,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3441, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3441, grad_fn=<UnbindBackward>)\n",
      "Episode 50: 0.0798820635576357\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw props for beating Duke UNC UNC UNC UNC UNC NC State UNC UNC UNC UNC Duke UNC UNC UNC Duke NC State UNC UNC Duke UNC Duke UNC UNC Duke UNC NC State UNC Duke UNC UNC NC State NC State UNC UNC NC State Chapel Hill UNC UNC UNC UNC Chapel Hill UNC UNC Chapel Hill Chapel Hill UNC UNC Duke UNC Chapel Hill UNC Chapel Hill UNC Duke UNC ChapelHill UNC UNC Chapel Hill NC State Chapel Hill Chapel Hill Chapel Hill NC State UNC Chapel Hill ChapelHill UNC Chapel\n",
      "r1: tensor(-8.7358e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6922, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6284, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1735, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3301,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3301, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3301, grad_fn=<UnbindBackward>)\n",
      "Episode 51: 0.08017577732827934\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw props for beating Duke UNC UNC UNC UNC UNC NC State UNC UNC UNC UNC Duke UNC UNC UNC Duke NC State UNC UNC Duke UNC Duke UNC UNC Duke UNC NC State UNC Duke UNC UNC NC State NC State UNC UNC NC State Chapel Hill UNC UNC UNC UNC Chapel Hill UNC UNC Chapel Hill Chapel Hill UNC UNC Duke UNC Chapel Hill UNC Chapel Hill UNC Duke UNC ChapelHill UNC UNC Chapel Hill NC State Chapel Hill Chapel Hill Chapel Hill NC State UNC Chapel Hill ChapelHill UNC Chapel\n",
      "r1: tensor(-9.6606e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7383, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6654, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1378, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3509,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3509, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3509, grad_fn=<UnbindBackward>)\n",
      "Episode 52: 0.08049314646356044\n",
      "User: Hello\n",
      "DialoGPT: Awesome username btw props for beating Duke UNC UNC UNC UNC UNC NC State UNC UNC UNC UNC Duke UNC UNC UNC Duke NC State UNC UNC Duke UNC Duke UNC UNC Duke UNC NC State UNC Duke UNC UNC NC State NC State UNC UNC NC State Chapel Hill UNC UNC UNC UNC Chapel Hill UNC UNC Chapel Hill Chapel Hill UNC UNC Duke UNC Chapel Hill UNC Chapel Hill UNC Duke UNC ChapelHill UNC UNC Chapel Hill NC State Chapel Hill Chapel Hill Chapel Hill NC State UNC Chapel Hill ChapelHill UNC Chapel\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7503, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6916, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1370, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3604,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3604, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3604, grad_fn=<UnbindBackward>)\n",
      "Episode 53: 0.08082094500593806\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen Nguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyenginxguyenguyenguyenguyenngguyenguyenguyenguyengueguyenguyenguyenguyenigonguyenguyenguyenguyenuggetsguyenguyenguyenguyenongyangguyenguyenguyenguyenorgetownguyenguyenguyenguyenraltarguyenguyenguyenguyenmongguyenguyenguyenguyenhengguyenguyenguyenguyenrenheitguyenguyenguyenguyenregorguyenguyen\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7242, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6542, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.7575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3446,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3446, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3446, grad_fn=<UnbindBackward>)\n",
      "Episode 54: 0.08112943282198401\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen Nguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyenginxguyenguyenguyenguyenngguyenguyenguyenguyengueguyenguyenguyenguyenigonguyenguyenguyenguyenuggetsguyenguyenguyenguyenongyangguyenguyenguyenguyenorgetownguyenguyenguyenguyenraltarguyenguyenguyenguyenrenheitguyenguyenguyenguyenhengguyenguyenguyenguyenregorguyenguyenguyenguyenmongguyenguyen\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7390, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7548, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.7478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3734,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3734, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3734, grad_fn=<UnbindBackward>)\n",
      "Episode 55: 0.08147090149414508\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea TeaTea Tea Tea Tea tea Tea Tea Tea teaTea Tea Tea tea Tea tea Tea Tea tea TeaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea Teepee tea tea tea tea coffee tea tea tea tea Earl Tea Tea Tea Tea Tee Tea Tea Tea Tea Earl Tea Tea Tea tea Tee Tea Tea Tea tea\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7075, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5311, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4065, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3096,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3096, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3096, grad_fn=<UnbindBackward>)\n",
      "Episode 56: 0.08173712383784015\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea TeaTea Tea Tea Tea tea Tea Tea Tea teaTea Tea Tea tea Tea tea Tea Tea tea TeaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea Teepee tea tea tea tea coffee tea tea tea tea Earl Tea Tea Tea Tea Tee Tea Tea Tea Tea Earl Tea Tea Tea tea Tee Tea Tea Tea tea\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6420, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5699, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5219, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3030,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3030, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3030, grad_fn=<UnbindBackward>)\n",
      "Episode 57: 0.0819949490514243\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea TeaTea Tea Tea TeaTeaTea Tea Tea Tea tea Tea Tea Tea teaTea Tea Tea tea Tea tea Tea Tea tea TeaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea Tee Tea Tea Tea Tea Tee Tea Tea Tea tea Tee Tea Tea tea Tea Tee Tea Tea tea tea Tee Tea tea Tea TeaTea\n",
      "r1: tensor(-9.8475e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7880, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7531, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3853,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3853, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3853, grad_fn=<UnbindBackward>)\n",
      "Episode 58: 0.08234798316643231\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea TeaTea Tea Tea TeaTeaTea Tea Tea Tea tea Tea Tea Tea teaTea Tea Tea tea Tea tea Tea Tea tea TeaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea Tee Tea Tea Tea Tea Tee Tea Tea Tea tea Tee Tea Tea tea Tea Tee Tea Tea tea tea Tee Tea tea Tea TeaTea\n",
      "r1: tensor(-9.2214e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8333, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1707, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4259, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2510,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2510, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2510, grad_fn=<UnbindBackward>)\n",
      "Episode 59: 0.08254406744949953\n",
      "User: Hello\n",
      "DialoGPT: raltarraltarraltarraltarraltar Gibraltarraltarraltarraltarraltaruaniaraltarraltarraltarraltaremaleraltarraltarraltarraltarlehemraltarraltarraltarraltarorkshireraltarraltarraltarraltarcastleraltarraltarraltarraltarmileraltarraltarraltarraltarmereraltarraltarraltarraltarmaxwellraltarraltarraltarraltarierrezraltarraltarraltarraltaranchesterraltarraltarraltarraltarmunitionraltarraltarraltarraltarmortgageraltarraltarraltarraltarlandishraltarraltarraltarraltarultonraltarraltarraltarraltaruphemraltarraltarraltarraltarschildraltarraltarraltarraltarxtapositionraltarraltarraltarraltarploy\n",
      "r1: tensor(-8.2381e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5497, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6566, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6645, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3015,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3015, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3015, grad_fn=<UnbindBackward>)\n",
      "Episode 60: 0.08279841755755878\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluadelphiaadelphiaadelphiaadelphiaadelphiaoluluoluluoluluoluluneapolisoluluoluluoluluoluluucklandoluluoluluoluluoluluicagooluluoluluoluluoluluuayuayuayuayuayoluluuayuayuayoluluoluluuayuayoluluuayoluluuayuayoluluoluluoluluuayoluluuayoluluoluluuayoluluoluluoluluoluluualaoluluuayuayuayualauayuayuayuayualaoluluuayuayoluluualauayuayuayoluluualaoluluuayoluluuayualauayuay\n",
      "r1: tensor(-9.5445e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7516, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8488, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4001,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4001, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4001, grad_fn=<UnbindBackward>)\n",
      "Episode 61: 0.08316647752270258\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluadelphiaadelphiaadelphiaadelphiaadelphiaoluluoluluoluluoluluneapolisoluluoluluoluluoluluucklandoluluoluluoluluoluluicagooluluoluluoluluoluluuayuayuayuayuayoluluuayuayuayoluluoluluuayuayoluluuayoluluuayuayoluluoluluoluluuayoluluuayoluluoluluuayoluluoluluoluluoluluualaoluluuayuayuayualauayuayuayuayualaoluluuayuayoluluualauayuayuayoluluualaoluluuayoluluuayualauayuay\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7948, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8837, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3701, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4196,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4196, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4196, grad_fn=<UnbindBackward>)\n",
      "Episode 62: 0.08355632698841108\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluadelphiaadelphiaadelphiaadelphiaadelphiaoluluoluluoluluoluluneapolisoluluoluluoluluoluluucklandoluluoluluoluluoluluicagooluluoluluoluluoluluuayuayuayuayuayoluluuayuayuayoluluoluluuayuayoluluuayoluluuayuayoluluoluluoluluuayoluluuayoluluoluluuayoluluoluluoluluoluluualaoluluuayuayuayualauayuayuayuayualaoluluuayuayoluluualauayuayuayoluluualaoluluuayoluluuayualauayuay\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7927, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7602, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3882,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3882, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3882, grad_fn=<UnbindBackward>)\n",
      "Episode 63: 0.08390890912064437\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluadelphiaadelphiaadelphiaadelphiaadelphiaoluluoluluoluluoluluneapolisoluluoluluoluluoluluucklandoluluoluluoluluoluluicagooluluoluluoluluoluluuayuayuayuayuayoluluuayuayuayoluluoluluuayuayoluluuayoluluuayuayoluluoluluoluluuayoluluuayoluluoluluuayoluluoluluoluluoluluualaoluluuayuayuayualauayuayuayuayualaoluluuayuayoluluualauayuayuayoluluualaoluluuayoluluuayualauayuay\n",
      "r1: tensor(-8.0905e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7455, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7973, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4075, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3857,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3857, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3857, grad_fn=<UnbindBackward>)\n",
      "Episode 64: 0.08425777924833873\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaoluluoluluoluluoluluohooluluoluluoluluoluluoyaoluluoluluoluluoluluokaoluluoluluoluluoluluokiaokiaokiaokiaokiaohookiaokiaokiaokiaoluluokiaokiaokiaohoohookiaokiaokiaohoopaokiaokiaokiaokiaohaokiaokiaokiaokiaopaokiaokiaokiaohoohaokiaokiaokiaohoolandokiaokiaokiaokiaolandokiaokiaokiaohoordokiaokiaokiaokiaoaokiaokia\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7582, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6632, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5780, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3553,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3553, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3553, grad_fn=<UnbindBackward>)\n",
      "Episode 65: 0.08457079658760124\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea Teepee tea tea tea tea tee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Te Tea Tea Tea Tea Coffee Tea Tea\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7521, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7553, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4865, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3768,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3768, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3768, grad_fn=<UnbindBackward>)\n",
      "Episode 66: 0.08490786508032934\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea Teepee tea tea tea tea tee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Te Tea Tea Tea Tea Coffee Tea Tea\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5750, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8005, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4704, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3439,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3439, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3439, grad_fn=<UnbindBackward>)\n",
      "Episode 67: 0.08520618614940238\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea Teepee tea tea tea tea tee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Te Tea Tea Tea Tea Coffee Tea Tea\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5156, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.3549, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4940, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2176,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2176, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2176, grad_fn=<UnbindBackward>)\n",
      "Episode 68: 0.08535855413458478\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea Teepee tea tea tea tea tee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Te Tea Tea Tea Tea Coffee Tea Tea\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7747, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6870, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5016, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3654,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3654, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3654, grad_fn=<UnbindBackward>)\n",
      "Episode 69: 0.08568043552885024\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen Nguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyenginxguyenguyenguyenguyenngguyenguyenguyenguyengueguyenguyenguyenguyenigonguyenguyenguyenguyeneguguyenguyenguyenguyenguguyenguyenguyenguyenghaguyenguyenguyenguyengenguyenguyenguyenguyenhengguyenguyenguyenguyenegalguyenguyenguyenguyengianguyenguyenguyenguyenmongguyenguyen\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6999, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8064, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.7497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3765,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3765, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3765, grad_fn=<UnbindBackward>)\n",
      "Episode 70: 0.0860143691364432\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen Nguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyenginxguyenguyenguyenguyenngguyenguyenguyenguyengueguyenguyenguyenguyenigonguyenguyenguyenguyeneguguyenguyenguyenguyenguguyenguyenguyenguyenghaguyenguyenguyenguyengenguyenguyenguyenguyenhengguyenguyenguyenguyenegalguyenguyenguyenguyengianguyenguyenguyenguyenmongguyenguyen\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7191, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6870, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.7085, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3515,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3515, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3515, grad_fn=<UnbindBackward>)\n",
      "Episode 71: 0.08631879308233209\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen Nguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyenginxguyenguyenguyenguyenngguyenguyenguyenguyengueguyenguyenguyenguyenigonguyenguyenguyenguyeneguguyenguyenguyenguyenguguyenguyenguyenguyenghaguyenguyenguyenguyengenguyenguyenguyenguyenhengguyenguyenguyenguyenegalguyenguyenguyenguyengianguyenguyenguyenguyenmongguyenguyen\n",
      "r1: tensor(-9.8486e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7622, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7906, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6631, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3882,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3882, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3882, grad_fn=<UnbindBackward>)\n",
      "Episode 72: 0.08666454829043044\n",
      "User: Hello\n",
      "DialoGPT: raltarraltarraltarraltarraltar Gibraltarraltarraltarraltarraltaruaniaraltarraltarraltarraltaremaleraltarraltarraltarraltarlehemraltarraltarraltarraltarorkshireraltarraltarraltarraltarcastleraltarraltarraltarraltarmileraltarraltarraltarraltarmereraltarraltarraltarraltarmaxwellraltarraltarraltarraltarierrezraltarraltarraltarraltaranchesterraltarraltarraltarraltarmunitionraltarraltarraltarraltarmortgageraltarraltarraltarraltarlandishraltarraltarraltarraltaruphemraltarraltarraltarraltarschildraltarraltarraltarraltarultonraltarraltarraltarraltarxtapositionraltarraltarraltarraltarploy\n",
      "r1: tensor(-7.9334e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7164, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7013, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6249, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3544,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3544, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3544, grad_fn=<UnbindBackward>)\n",
      "Episode 73: 0.08697088746390214\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea Teepee tea tea tea tea tee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Te Tea Tea Tea Tea Coffee Tea Tea\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8315, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5962, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4631, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3569,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3569, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3569, grad_fn=<UnbindBackward>)\n",
      "Episode 74: 0.0872793711077954\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea Teepee tea tea tea tea tee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Te Tea Tea Tea Tea Coffee Tea Tea\n",
      "r1: tensor(-8.0266e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7970, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7690, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4906, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3915,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3915, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3915, grad_fn=<UnbindBackward>)\n",
      "Episode 75: 0.08762663233648883\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaoaoaoaoaohaoaoaoaoaoluluoluluoluluoluluohoohaoaoaoaohaoluluoaoaoaohaohaoaoaoaohoohaoaoaohaoaohaoaoaohaohoohoohoohoohoohaoaoaohoohoohoohooaohoohoohooaoaohoohooaohooaohoohooaoaoaohoohoohaoaohoohoohoohaohoohooho\n",
      "r1: tensor(-9.5935e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8662, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7895, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4259, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4139,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4139, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4139, grad_fn=<UnbindBackward>)\n",
      "Episode 76: 0.08799867321733924\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaoaoaoaoaohaoaoaoaoaoluluoluluoluluoluluohoohaoaoaoaohaoluluoaoaoaohaohaoaoaoaohoohaoaoaohaoaohaoaoaohaohoohoohoohoohoohaoaoaohoohoohoohooaohoohoohooaoaohoohooaohooaohoohooaoaoaohoohoohaoaohoohoohoohaohoohooho\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8173, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8832, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3842, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4251,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4251, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4251, grad_fn=<UnbindBackward>)\n",
      "Episode 77: 0.08838260945957366\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaoaoaoaoaohaoaoaoaoaoluluoluluoluluoluluohoohaoaoaoaohaoluluoaoaoaohaohaoaoaoaohoohaoaoaohaoaohaoaoaohaohoohoohoohoohoohaoaoaohoohoohoohooaohoohoohooaoaohoohooaohooaohoohooaoaoaohoohoohaoaohoohoohoohaohoohooho\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8094, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8706, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4708, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4200,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4200, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4200, grad_fn=<UnbindBackward>)\n",
      "Episode 78: 0.08875983565212975\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaoaoaoaoaohaoaoaoaoaoluluoluluoluluoluluohoohaoaoaoaohaoluluoaoaoaohaohaoaoaoaohoohaoaoaohaoaohaoaoaohaohoohoohoohoohoohaoaoaohoohoohoohooaohoohoohooaoaohoohooaohooaohoohooaoaoaohoohoohaoaohoohoohoohaohoohooho\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7756, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8461, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3059, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4054,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4054, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4054, grad_fn=<UnbindBackward>)\n",
      "Episode 79: 0.08911966625548137\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea Teepee tea tea tea tea tee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Te Tea Tea Tea Tea Coffee Tea Tea\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7492, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6397, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3472,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3472, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3472, grad_fn=<UnbindBackward>)\n",
      "Episode 80: 0.08941259278221295\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen Nguyenguyenguyenguyenguyenugenguyenguyenguyenguyengenguyenguyenguyenguyenngguyenguyenguyenguyenginxguyenguyenguyenguyenjenguyenguyenguyenguyenwangguyenguyenguyenguyenenhguyenguyenguyenguyeningenguyenguyenguyenguyenhengguyenguyenguyenguyenmongguyenguyenguyenguyenennisguyenguyenguyenguyeneneguyenguyenguyenguyenenfguyenguyenguyenguyenenesguyenguyenguyenguyenennesguyenguyenguyenguyenrenheitguyenguyenguyenguyennesguyenguyenguyenguyenephguyenguyen\n",
      "r1: tensor(-9.0813e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7817, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7492, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.7215, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3827,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3827, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3827, grad_fn=<UnbindBackward>)\n",
      "Episode 81: 0.08974512889421632\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea Teepee tea tea tea tea tee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Te Tea Tea Tea Tea Coffee Tea Tea\n",
      "r1: tensor(-9.4477e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7890, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8005, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4706, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3973,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3973, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3973, grad_fn=<UnbindBackward>)\n",
      "Episode 82: 0.0900934908116112\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea Teepee tea tea tea tea tee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Te Tea Tea Tea Tea Coffee Tea Tea\n",
      "r1: tensor(-9.1688e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0762, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8657, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4827, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2355,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2355, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2355, grad_fn=<UnbindBackward>)\n",
      "Episode 83: 0.09025793670881034\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea Teepee tea tea tea tea tee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Te Tea Tea Tea Tea Coffee Tea Tea\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7341, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7097, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5135, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3609,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3609, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3609, grad_fn=<UnbindBackward>)\n",
      "Episode 84: 0.0905637821229964\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea Teepee tea tea tea tea tee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Te Tea Tea Tea Tea Coffee Tea Tea\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7063, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7807, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3717,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3717, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3717, grad_fn=<UnbindBackward>)\n",
      "Episode 85: 0.09088113083205612\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea Teepee tea tea tea tea tee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Te Tea Tea Tea Tea Coffee Tea Tea\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6700, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5495, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4884, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3048,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3048, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3048, grad_fn=<UnbindBackward>)\n",
      "Episode 86: 0.09112235188028506\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen Nguyenguyenguyenguyenguyenugenguyenguyenguyenguyengenguyenguyenguyenguyenngguyenguyenguyenguyenginxguyenguyenguyenguyenjenguyenguyenguyenguyenwangguyenguyenguyenguyenenhguyenguyenguyenguyeningenguyenguyenguyenguyenhengguyenguyenguyenguyenmongguyenguyenguyenguyenennisguyenguyenguyenguyenenfguyenguyenguyenguyenrenheitguyenguyenguyenguyenennesguyenguyenguyenguyenenesguyenguyenguyenguyeneneguyenguyenguyenguyennesguyenguyenguyenguyenenneguyenguyen\n",
      "r1: tensor(-0.0003, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7411, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8012, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.7023, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3855,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3855, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3855, grad_fn=<UnbindBackward>)\n",
      "Episode 87: 0.09145385875977022\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea Teepee tea tea tea tea tee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Te Tea Tea Tea Tea Coffee Tea Tea\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.4383, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8468, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4076, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3213,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3213, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3213, grad_fn=<UnbindBackward>)\n",
      "Episode 88: 0.09171234934430474\n",
      "User: Hello\n",
      "DialoGPT: raltarraltarraltarraltarraltar Gibraltarraltarraltarraltarraltarlehemlehemlehemlehemlehemraltarlehemlehemlehemraltarraltarlehemlehemraltarlehemraltarlehemlehemraltarraltarraltarlehemraltarlehemraltarraltarlehemraltarraltarraltarraltaradelphialehemlehemlehemlehemsburghlehemlehemlehemlehemadelphialehemlehemlehemsburghsburghlehemlehemlehemsburghadelphialehemlehemlehemadelphiasburghlehemlehemlehemadelphiaadelphialehemlehemlehemphialehemlehemlehemlehemphiasburghlehemlehemlehemphiaadelphialehemlehemlehemsburglehemlehemlehemlehembuquerquelehemlehem\n",
      "r1: tensor(-8.7055e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5730, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7489, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6010, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3304,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3304, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3304, grad_fn=<UnbindBackward>)\n",
      "Episode 89: 0.09198058994246248\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea beverage tea tea tea tea soda tea tea tea tea liquor tea tea tea tea booze tea tea tea tea drink tea tea tea tea drinks tea tea tea tea beverages tea tea tea tea coff\n",
      "r1: tensor(-0.0005, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7755, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8337, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4024, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4022,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4022, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4022, grad_fn=<UnbindBackward>)\n",
      "Episode 90: 0.09232873931959873\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluicagoicagoicagoicagoicagooluluoluluoluluoluluneapolisoluluoluluoluluoluluadelphiaoluluoluluoluluoluluucklandoluluoluluoluluoluluminghamoluluoluluoluluolulualoreoluluoluluoluluoluluipegoluluoluluoluluoluluuayuayuayuayuayoluluoluluoluluoluluualaoluluoluluoluluoluluohaoluluoluluoluluoluluoyaoluluoluluoluluolululahomaoluluoluluoluluoluluboaoluluoluluoluluoluluolandoluluoluluoluluoluluinkioluluoluluoluluolulu\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.9240, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2429, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4726, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2917,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2917, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2917, grad_fn=<UnbindBackward>)\n",
      "Episode 91: 0.09255222618661303\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluicagoicagoicagoicagoicagooluluoluluoluluoluluneapolisoluluoluluoluluoluluadelphiaoluluoluluoluluoluluucklandoluluoluluoluluoluluminghamoluluoluluoluluolulualoreoluluoluluoluluoluluipegoluluoluluoluluoluluuayuayuayuayuayoluluoluluoluluoluluualaoluluoluluoluluoluluohaoluluoluluoluluoluluoyaoluluoluluoluluolululahomaoluluoluluoluluoluluboaoluluoluluoluluoluluolandoluluoluluoluluoluluinkioluluoluluoluluolulu\n",
      "r1: tensor(-9.6293e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8396, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8178, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5290, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4143,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4143, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4143, grad_fn=<UnbindBackward>)\n",
      "Episode 92: 0.09291254890006356\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluicagoicagoicagoicagoicagooluluoluluoluluoluluneapolisoluluoluluoluluoluluadelphiaoluluoluluoluluoluluucklandoluluoluluoluluoluluminghamoluluoluluoluluolulualoreoluluoluluoluluoluluipegoluluoluluoluluoluluuayuayuayuayuayoluluoluluoluluoluluualaoluluoluluoluluoluluohaoluluoluluoluluoluluoyaoluluoluluoluluolululahomaoluluoluluoluluoluluboaoluluoluluoluluoluluolandoluluoluluoluluoluluinkioluluoluluoluluolulu\n",
      "r1: tensor(-9.5658e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8523, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8786, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4672, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4327,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4327, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4327, grad_fn=<UnbindBackward>)\n",
      "Episode 93: 0.09329263339187925\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluicagoicagoicagoicagoicagooluluoluluoluluoluluneapolisoluluoluluoluluoluluadelphiaoluluoluluoluluoluluucklandoluluoluluoluluoluluminghamoluluoluluoluluolulualoreoluluoluluoluluoluluipegoluluoluluoluluoluluuayuayuayuayuayoluluoluluoluluoluluualaoluluoluluoluluoluluohaoluluoluluoluluoluluoyaoluluoluluoluluolululahomaoluluoluluoluluoluluboaoluluoluluoluluoluluolandoluluoluluoluluoluluinkioluluoluluoluluolulu\n",
      "r1: tensor(-9.5410e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8547, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8434, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4245,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4245, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4245, grad_fn=<UnbindBackward>)\n",
      "Episode 94: 0.0936627059337272\n",
      "User: Hello\n",
      "DialoGPT: hovahhovahhovahhovahhovah Jehovahhovahhovahhovahhovah Witnesses Witnesses Witnesses Witnesses Witnesses Witness Witnesses Witnesses Witnesses Witnesses witnesses Witnesses Witnesses Witnesses Witnesses Jehovah Witnesses Witnesses Witnesses Witnesseshovah Witnesses Witnesses Witnesses Witnessors Witnesses Witnesses Witnesses Witnesses congregants Witnesses Witnesses Witnesses Witnesses members Witnesses Witnesses Witnesses Witnesses followers Witnesses Witnesses Witnesses Witnesses disciples Witnesses Witnesses Witnesses Witnesses pastors Witnesses Witnesses Witnesses Witnesses elders Witnesses Witnesses Witnesses Witnesses cult members Witnesses Witnesses Witnesses cultmembers Witnesses Witnesses Witnesses Witnesses Cultmembers Witnesses Witnesses Witnesses cultmates Witnesses Witnesses Witnesses Witnesses Friends Witnesses Witnesses Witnesses Witnesses\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7126, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6795, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4322, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3480,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3480, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3480, grad_fn=<UnbindBackward>)\n",
      "Episode 95: 0.09394657435508894\n",
      "User: Hello\n",
      "DialoGPT: hovahhovahhovahhovahhovah Jehovahhovahhovahhovahhovah Witnesses Witnesses Witnesses Witnesses Witnesses Witness Witnesses Witnesses Witnesses Witnesses witnesses Witnesses Witnesses Witnesses Witnesses Jehovah Witnesses Witnesses Witnesses Witnesseshovah Witnesses Witnesses Witnesses Witnessors Witnesses Witnesses Witnesses Witnesses congregants Witnesses Witnesses Witnesses Witnesses members Witnesses Witnesses Witnesses Witnesses followers Witnesses Witnesses Witnesses Witnesses disciples Witnesses Witnesses Witnesses Witnesses pastors Witnesses Witnesses Witnesses Witnesses elders Witnesses Witnesses Witnesses Witnesses cult members Witnesses Witnesses Witnesses cultmembers Witnesses Witnesses Witnesses Witnesses Cultmembers Witnesses Witnesses Witnesses cultmates Witnesses Witnesses Witnesses Witnesses Friends Witnesses Witnesses Witnesses Witnesses\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8384, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8708, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4273,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4273, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4273, grad_fn=<UnbindBackward>)\n",
      "Episode 96: 0.09431818409987965\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea beverage tea tea tea tea soda tea tea tea tea liquor tea tea tea tea booze tea tea tea tea whiskey tea tea tea tea whisky tea tea tea tea wine tea tea tea tea champagne\n",
      "r1: tensor(-8.9448e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8584, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8271, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4214,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4214, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4214, grad_fn=<UnbindBackward>)\n",
      "Episode 97: 0.09468237660723616\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen Nguyenguyenguyenguyenguyenugenguyenguyenguyenguyengenguyenguyenguyenguyenngguyenguyenguyenguyenginxguyenguyenguyenguyenenhguyenguyenguyenguyenenegguyenguyenguyenguyeneneguyenguyenguyenguyenenesguyenguyenguyenguyenennesguyenguyenguyenguyenennisguyenguyenguyenguyenenfguyenguyenguyenguyenrenheitguyenguyenguyenguyennesguyenguyenguyenguyennenguyenguyenguyenguyenenneguyenguyenguyenguyenetonguyenguyenguyenguyennecguyenguyenguyenguyenynesguyenguyen\n",
      "r1: tensor(-8.0001e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6168, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8130, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.7293, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3574,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3574, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3574, grad_fn=<UnbindBackward>)\n",
      "Episode 98: 0.09497464816961375\n",
      "User: Hello\n",
      "DialoGPT: orkshire accent intensifies intensify intensify intensify intensify intensify intensifies intensify intensify intensifies intensifies intensify intensify intensified intensify intensify intensify intensify intensified intensify intensifies intensify intensifies intensify intensified intensify intensify intensifies intensified intensify intensify intensify intensities intensify intensities intensify intensify intensities intensities intensify intensifies intensify peaks peaks peaks peaks peaks Peaks peaks peaks peaks peaks peak peaks peaks peaks peaks Peak peaks peaks peaks peakspeak peaks peaks peaks peakspe peaks peaks peaks peakspes peaks peaks peaks peakspots peaks peaks peaks peakspees peaks\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7723, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7540, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3207, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3816,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3816, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3816, grad_fn=<UnbindBackward>)\n",
      "Episode 99: 0.0952930740841354\n",
      "User: Hello\n",
      "DialoGPT: orkshire accent intensifies intensify intensify intensify intensify intensify intensifies intensify intensify intensifies intensifies intensify intensify intensified intensify intensify intensify intensify intensified intensify intensifies intensify intensifies intensify intensified intensify intensify intensifies intensified intensify intensify intensify intensities intensify intensities intensify intensify intensities intensities intensify intensifies intensify peaks peaks peaks peaks peaks Peaks peaks peaks peaks peaks peak peaks peaks peaks peaks Peak peaks peaks peaks peakspeak peaks peaks peaks peakspe peaks peaks peaks peakspes peaks peaks peaks peakspots peaks peaks peaks peakspees peaks\n",
      "r1: tensor(-9.3838e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6235, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7401, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3409,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3409, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3409, grad_fn=<UnbindBackward>)\n",
      "Episode 100: 0.09556562667629877\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: orkshire accent intensifies intensify intensify intensify intensify intensify intensifies intensify intensify intensifies intensifies intensify intensify intensified intensify intensify intensify intensify intensified intensify intensifies intensify intensifies intensify intensified intensify intensify intensifies intensified intensify intensify intensify intensities intensify intensities intensify intensify intensities intensities intensify intensifies intensify peaks peaks peaks peaks peaks Peaks peaks peaks peaks peaks peak peaks peaks peaks peaks Peak peaks peaks peaks peakspeak peaks peaks peaks peakspe peaks peaks peaks peakspes peaks peaks peaks peakspots peaks peaks peaks peakspees peaks\n",
      "r1: tensor(-8.7361e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7826, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7681, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3877,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3877, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3877, grad_fn=<UnbindBackward>)\n",
      "Episode 101: 0.09588944885472947\n",
      "User: Hello\n",
      "DialoGPT: orkshire accent intensifies intensify intensify intensify intensify intensify intensifies intensify intensify intensifies intensifies intensify intensify intensified intensify intensify intensify intensify intensified intensify intensifies intensify intensifies intensify intensified intensify intensify intensifies intensified intensify intensify intensify intensities intensify intensities intensify intensify intensities intensities intensify intensifies intensify peaks peaks peaks peaks peaks Peaks peaks peaks peaks peaks peak peaks peaks peaks peaks Peak peaks peaks peaks peakspeak peaks peaks peaks peakspe peaks peaks peaks peakspes peaks peaks peaks peakspots peaks peaks peaks peakspees peaks\n",
      "r1: tensor(-7.6762e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7806, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7872, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3919,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3919, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3919, grad_fn=<UnbindBackward>)\n",
      "Episode 102: 0.09621729433550968\n",
      "User: Hello\n",
      "DialoGPT: orkshire accent intensifies intensify intensify intensify intensify intensify intensifies intensify intensify intensifies intensifies intensify intensify intensified intensify intensify intensify intensify intensified intensify intensifies intensify intensifies intensify intensified intensify intensify intensifies intensified intensify intensify intensify intensities intensify intensities intensify intensify intensities intensities intensify intensifies intensify peaks peaks peaks peaks peaks Peaks peaks peaks peaks peaks peak peaks peaks peaks peaks Peak peaks peaks peaks peakspeak peaks peaks peaks peakspe peaks peaks peaks peakspes peaks peaks peaks peakspots peaks peaks peaks peakspees peaks\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7998, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6835, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3708,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3708, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3708, grad_fn=<UnbindBackward>)\n",
      "Episode 103: 0.09652101053944913\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen NguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyenginxguyenguyenguyenguyenngguyenguyenguyenguyengueguyenguyenguyenguyenuggetsguyenguyenguyenguyenUGEguyenguyenguyenguyenuggestguyenguyenguyenguyenorgetownguyenguyenguyenguyenmunitionguyenguyenguyenguyenliviousguyenguyenguyenguyenullivanguyenguyenguyenguyenraltarguyenguyenguyenguyenutoniumguyenguyen\n",
      "r1: tensor(-9.6478e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8098, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5993, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6846, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3522,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3522, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3522, grad_fn=<UnbindBackward>)\n",
      "Episode 104: 0.09680357397539098\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaoaoaoaoaohaoaoaoaoaohoohaoaoaoaohaohaoaoaoaohoohoohaoaoaohaoaohaoaoaohaohaohaoaoaohaohoohaoaoaohoohoohoohoohoohaoaohoohoohoohaohoohoohoohooaohoohoohooaoaohoohooaohooaohoohooaoaoaohooaohooaoaohooa\n",
      "r1: tensor(-0.0003, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8733, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5404, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3823, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3533,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3533, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3533, grad_fn=<UnbindBackward>)\n",
      "Episode 105: 0.0970867288357498\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea beverage tea tea tea tea soda tea tea tea tea liquor tea tea tea tea booze tea tea tea tea Earl Grey tea tea tea tea drink tea tea tea tea water tea tea tea tea\n",
      "r1: tensor(-9.8706e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8346, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8593, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4235,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4235, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4235, grad_fn=<UnbindBackward>)\n",
      "Episode 106: 0.09744656450641902\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaoaoaoaoaohaoaoaoaoaohoohaoaoaoaohaohaoaoaoaohoohoohaoaoaohaoaohaoaoaohaohaohaoaoaohaohoohaoaoaohoohoohoohoohoohaoaohoohoohoohaohoohoohoohooaohoohoohooaoaohoohooaohooaohoohooaoaoaohooaohooaoaohooa\n",
      "r1: tensor(-7.8491e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8902, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6309, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3803,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3803, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3803, grad_fn=<UnbindBackward>)\n",
      "Episode 107: 0.09775802476633143\n",
      "User: Hello\n",
      "DialoGPT: raltarraltarraltarraltarraltar Gibraltarraltarraltarraltarraltarlehemlehemlehemlehemlehemraltarlehemlehemlehemraltarraltarlehemlehemraltarlehemraltarlehemlehemraltarraltarraltarlehemraltarlehemraltarraltarlehemraltarraltarraltarraltaradelphialehemlehemlehemlehemsburghlehemlehemlehemlehemadelphialehemlehemlehemsburghsburghlehemlehemlehemsburghadelphialehemlehemlehemadelphiasburghlehemlehemlehemadelphiaadelphialehemlehemlehemphialehemlehemlehemlehemphiasburghlehemlehemlehemphiaadelphialehemlehemlehemsburglehemlehemlehemlehembuquerquelehemlehem\n",
      "r1: tensor(-9.6671e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7319, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8187, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6060, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3876,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3876, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3876, grad_fn=<UnbindBackward>)\n",
      "Episode 108: 0.09807691663222919\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaoaoaoaoaohaoaoaoaoaohoohaoaoaoaohaohaoaoaoaohoohoohaoaoaohaoaohaoaoaohaohaohaoaoaohaohoohaoaoaohoohoohoohoohoohaoaohoohoohoohaohoohoohoohooaohoohoohooaoaohoohooaohooaohoohooaoaoaohooaohooaoaohooa\n",
      "r1: tensor(-8.6013e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8832, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8838, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3038, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4417,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4417, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4417, grad_fn=<UnbindBackward>)\n",
      "Episode 109: 0.09845457325873712\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea party member checking in confirming suspicions suspicions suspicions suspicions suspicions suspicion suspicions suspicions suspicions suspicions doubts suspicions suspicions suspicions suspicions accusations suspicions suspicions suspicions suspicions allegations allegations allegations allegations allegations accusations allegations allegations allegations allegations suspicions allegations allegations allegations accusations accusations allegations allegations allegations accusations suspicions suspicions suspicions allegations accusations allegations accusations accusations accusations accusations accusations allegations accusations accusations allegations accusations allegations accusations allegations allegations accusations accusations accusations allegations allegations accusations allegations accusations suspicions accusations accusations accusations accusations suspicions accusations accusations suspicions accusations suspicions suspicions accusations accusations accusations suspicions suspicions suspicions accusations accusations suspicions suspicions accusations\n",
      "r1: tensor(-8.3553e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7592, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7623, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3860, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3804,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3804, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3804, grad_fn=<UnbindBackward>)\n",
      "Episode 110: 0.09876402499334136\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea party member checking in confirming suspicions suspicions suspicions suspicions suspicions suspicion suspicions suspicions suspicions suspicions doubts suspicions suspicions suspicions suspicions accusations suspicions suspicions suspicions suspicions allegations allegations allegations allegations allegations accusations allegations allegations allegations allegations suspicions allegations allegations allegations accusations accusations allegations allegations allegations accusations suspicions suspicions suspicions allegations accusations allegations accusations accusations accusations accusations accusations allegations accusations accusations allegations accusations allegations accusations allegations allegations accusations accusations accusations allegations allegations accusations allegations accusations suspicions accusations accusations accusations accusations suspicions accusations accusations suspicions accusations suspicions suspicions accusations accusations accusations suspicions suspicions suspicions accusations accusations suspicions suspicions accusations\n",
      "r1: tensor(-9.1498e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8422, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7884, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4076,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4076, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4076, grad_fn=<UnbindBackward>)\n",
      "Episode 111: 0.0991026792147303\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen NguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyenginxguyenguyenguyenguyenngguyenguyenguyenguyengueguyenguyenguyenguyenuggetsguyenguyenguyenguyenUGEguyenguyenguyenguyenuggestguyenguyenguyenguyenorgetownguyenguyenguyenguyenmunitionguyenguyenguyenguyenliviousguyenguyenguyenguyenullivanguyenguyenguyenguyenraltarguyenguyenguyenguyenutoniumguyenguyen\n",
      "r1: tensor(-9.9641e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7528, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7968, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.7268, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3874,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3874, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3874, grad_fn=<UnbindBackward>)\n",
      "Episode 112: 0.09941844216130698\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaoaoaoaoaohaoaoaoaoaohoohaoaoaoaohaohaoaoaoaohoohoohaoaoaohaoaohaoaoaohaohaohoohoohoohoohoohaoaohaoaohaohoohoohoohaohoohoohooaoaoaoaoluluoluluoluluoluluohoohoohoohooluluohoohoohoohaoluluohoohoohooaohoohoohoohooaohaoho\n",
      "r1: tensor(-7.9061e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8393, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8268, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4165,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4165, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4165, grad_fn=<UnbindBackward>)\n",
      "Episode 113: 0.09976538236424147\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluohaohaohaohaohaoluluoluluoluluoluluoaohaohaohaohaoaohaohaohaoluluohaohaohaoaoaohaohaohaoaoluluoluluoluluoluluohoohaohaohaohaohoohaohaohaoaohoohaohaohaohoohoohoohoohoohaohoohoohoohaohaohoohoohaohoohaohoohoohaohaohaoluluohoohoohoohooaohoohoohoohooboohoohoohoohooluluohoohoohoohaoaohoohoohoohaobo\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8111, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8752, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3252, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4215,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4215, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4215, grad_fn=<UnbindBackward>)\n",
      "Episode 114: 0.10011705853849079\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea party member checking in confirming suspicions suspicions suspicions suspicions suspicions suspicion suspicions suspicions suspicions suspicions doubts suspicions suspicions suspicions suspicions accusations suspicions suspicions suspicions suspicions allegations allegations allegations allegations allegations accusations allegations allegations allegations allegations suspicions allegations allegations allegations accusations accusations allegations allegations allegations accusations suspicions suspicions suspicions allegations accusations allegations accusations accusations accusations accusations accusations allegations accusations accusations allegations accusations allegations accusations allegations allegations accusations accusations accusations allegations allegations accusations allegations accusations suspicions accusations accusations accusations accusations suspicions accusations accusations suspicions accusations suspicions suspicions accusations accusations accusations suspicions suspicions suspicions accusations accusations suspicions suspicions accusations\n",
      "r1: tensor(-8.7603e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8283, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8047, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3897, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4082,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4082, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4082, grad_fn=<UnbindBackward>)\n",
      "Episode 115: 0.10045343728514396\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea party member checking in confirming suspicions suspicions suspicions suspicions suspicions suspicion suspicions suspicions suspicions suspicions doubts suspicions suspicions suspicions suspicions accusations suspicions suspicions suspicions suspicions allegations allegations allegations allegations allegations accusations allegations allegations allegations allegations suspicions allegations allegations allegations accusations accusations allegations allegations allegations accusations suspicions suspicions suspicions allegations accusations allegations accusations accusations accusations accusations accusations allegations accusations accusations allegations accusations allegations accusations allegations allegations accusations accusations accusations allegations allegations accusations allegations accusations suspicions accusations accusations accusations accusations suspicions accusations accusations suspicions accusations suspicions suspicions accusations accusations accusations suspicions suspicions suspicions accusations accusations suspicions suspicions accusations\n",
      "r1: tensor(-7.7133e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7726, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6590, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3674, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3579,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3579, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3579, grad_fn=<UnbindBackward>)\n",
      "Episode 116: 0.1007341748360451\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea party member checking in confirming suspicions suspicions suspicions suspicions suspicions suspicion suspicions suspicions suspicions suspicions doubts suspicions suspicions suspicions suspicions theories suspicions suspicions suspicions suspicions accusations suspicions suspicions suspicions suspicions allegations accusations accusations accusations accusations accusations allegations accusations accusations accusations allegations allegations accusations accusations accusations suspicions accusations accusations accusations accusations suspicions suspicions accusations accusations accusations allegations suspicions accusations accusations accusations suspicions allegations accusations accusations suspicions accusations suspicions suspicions accusations suspicions accusations accusations suspicions suspicions suspicions accusations accusations suspicions accusations allegations accusations accusations suspicions suspicions allegations accusations suspicions suspicions suspicions allegations suspicions suspicions suspicions suspicions rumors suspicions\n",
      "r1: tensor(-9.0124e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7747, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7568, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3783, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3829,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3829, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3829, grad_fn=<UnbindBackward>)\n",
      "Episode 117: 0.10104149950045305\n",
      "User: Hello\n",
      "DialoGPT: raltarraltarraltarraltarraltar Gibraltarraltarraltarraltarraltarlehemlehemlehemlehemlehemraltarlehemlehemlehemraltarraltarlehemlehemraltarlehemraltarlehemlehemraltarraltarraltarlehemraltarlehemraltarraltarlehemraltarraltarraltarraltaradelphialehemlehemlehemlehemsburghlehemlehemlehemlehemadelphialehemlehemlehemsburghsburghlehemlehemlehemsburghadelphialehemlehemlehemadelphiasburghlehemlehemlehemadelphiaadelphialehemlehemlehemphialehemlehemlehemlehemphiasburghlehemlehemlehemphiaadelphialehemlehemlehemsburglehemlehemlehemlehembuquerquelehemlehem\n",
      "r1: tensor(-9.3370e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6365, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7754, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5368, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3529,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3529, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3529, grad_fn=<UnbindBackward>)\n",
      "Episode 118: 0.10131560682680892\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluohaohaohaohaohaoluluoluluoluluoluluoaohaohaohaohaoaohaohaohaoluluohaohaohaoaoaohaohaohaoaoluluoluluoluluoluluohoohaohaohaohaohoohaohaohaoaohoohaohaohaohoohoohoohoohoohaohoohoohoohaohaohoohoohaohoohaohoohoohaohaohaoluluohoohoohoohooboohaohoohoohooboohoohoohoohooaohoohoohoohooluluohoohoohooboobo\n",
      "r1: tensor(-8.5603e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8177, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8136, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3373, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4078,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4078, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4078, grad_fn=<UnbindBackward>)\n",
      "Episode 119: 0.10164872430538273\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluohaohaohaohaohaoluluoluluoluluoluluoaohaohaohaohaoaohaohaohaoluluohaohaohaoaoaohaohaohaoaoluluoluluoluluoluluohoohaohaohaohaohoohaohaohaoaohoohaohaohaohoohoohoohoohoohaohoohoohoohaohaohoohoohaohoohaohoohoohaohaohaoluluohoohoohoohooboohaohoohoohooboohoohoohoohooaohoohoohoohooluluohoohoohooboobo\n",
      "r1: tensor(-8.1639e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7391, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7807, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2922, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3799,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3799, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3799, grad_fn=<UnbindBackward>)\n",
      "Episode 120: 0.10195087878454182\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen NguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyenginxguyenguyenguyenguyenngguyenguyenguyenguyengueguyenguyenguyenguyenuggetsguyenguyenguyenguyenUGEguyenguyenguyenguyenuggestguyenguyenguyenguyenorgetownguyenguyenguyenguyenmunitionguyenguyenguyenguyenutoniumguyenguyenguyenguyenavoriteguyenguyenguyenguyenuclearguyenguyenguyenguyenperatureguyenguyen\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.4459, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8252, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.7090, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3177,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3177, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3177, grad_fn=<UnbindBackward>)\n",
      "Episode 121: 0.1021849203389138\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluohaohaohaohaohaoluluoluluoluluoluluoaohaohaohaohaoaohaohaohaoluluohaohaohaoaoaohaohaohaoaoluluoluluoluluoluluohoohaohaohaohaohoohaohaohaoaohoohaohaohaohoohoohoohoohoohaohoohoohoohaohaohoohoohaohoohaohoohoohaohaohaoluluohoohoohoohooboohaohoohoohooboohoohoohoohooaohoohoohoohooluluohoohoohooboobo\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8389, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8277, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3118, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4166,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4166, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4166, grad_fn=<UnbindBackward>)\n",
      "Episode 122: 0.1025255884917681\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluohaohaohaohaohaoluluoluluoluluoluluoaohaohaohaohaoaohaohaohaoluluohaohaohaoaoaohaohaohaoaoluluoluluoluluoluluohoohaohaohaohaohoohaohaohaoaohoohaohaohaohoohoohoohoohoohaohoohoohoohaohaohoohoohaohoohaohoohoohaohaohaoluluohoohoohoohooboohaohoohoohooboohoohoohoohooaohoohoohoohooluluohoohoohooboobo\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8851, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7539, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2920, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4097,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4097, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4097, grad_fn=<UnbindBackward>)\n",
      "Episode 123: 0.10285805359238848\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea beverage tea tea tea tea soda tea tea tea tea liquor tea tea tea tea booze tea tea tea tea Earl Grey tea tea tea tea drink tea tea tea tea water tea tea tea tea\n",
      "r1: tensor(-9.6441e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7804, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8567, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2766, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4092,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4092, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4092, grad_fn=<UnbindBackward>)\n",
      "Episode 124: 0.10318927681073546\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea beverage tea tea tea tea soda tea tea tea tea liquor tea tea tea tea booze tea tea tea tea Earl Grey tea tea tea tea drink tea tea tea tea water tea tea tea tea\n",
      "r1: tensor(-0.0006, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8439, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.4156, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3921, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3147,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3147, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3147, grad_fn=<UnbindBackward>)\n",
      "Episode 125: 0.10341770625089901\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checking inorkshiremanorkshiremanorkshiremanimbabweimbabweimbabweimbabweimbabweorkshiremanimbabweimbabweorkshiremanorkshiremananchesterorkshiremanimbabweimbabweanchesterorkshiremanimbabweanchesterorkshiremananchesterorkshireanchesterorkshireanchesterorkshiremananchesterimbabweanchesterorkshiremanchesteranchesterorkshireanchesterorkshireMANchesteranchesterorkshireanchesteranchesterorkshireanchesterorkshireManchesteranchesterorkshireanchesterorkshirecastleanchesterorkshireanchesterorkshireorkshireanchesterorkshireanchesteranchesteranchesterorkshireanchesteranchesterimbabweanchesterorkshireanchesterorkshiresburghorkshireanchesterorkshireanchesterimbabweanchester\n",
      "r1: tensor(-9.0175e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7844, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7862, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1087, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3926,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3926, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3926, grad_fn=<UnbindBackward>)\n",
      "Episode 126: 0.10372969103040738\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checking inorkshiremanorkshiremanorkshiremanimbabweimbabweimbabweimbabweimbabweorkshiremanimbabweimbabweorkshiremanorkshiremananchesterorkshiremanimbabweimbabweanchesterorkshiremanimbabweanchesterorkshiremananchesterorkshireanchesterorkshireanchesterorkshiremananchesterimbabweanchesterorkshiremanchesteranchesterorkshireanchesterorkshireMANchesteranchesterorkshireanchesteranchesterorkshireanchesterorkshireManchesteranchesterorkshireanchesterorkshirecastleanchesterorkshireanchesterorkshireorkshireanchesterorkshireanchesteranchesteranchesterorkshireanchesteranchesterimbabweanchesterorkshireanchesterorkshiresburghorkshireanchesterorkshireanchesterimbabweanchester\n",
      "r1: tensor(-9.8572e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8818, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6609, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1300, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3857,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3857, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3857, grad_fn=<UnbindBackward>)\n",
      "Episode 127: 0.10403348971108087\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checking inorkshiremanorkshiremanorkshiremanimbabweimbabweimbabweimbabweimbabweorkshiremanimbabweimbabweorkshiremanorkshiremananchesterorkshiremanimbabweimbabweanchesterorkshiremanimbabweanchesterorkshiremananchesterorkshireanchesterorkshireanchesterorkshiremananchesterimbabweanchesterorkshiremanchesteranchesterorkshireanchesterorkshireMANchesteranchesterorkshireanchesteranchesterorkshireanchesterorkshireManchesteranchesterorkshireanchesterorkshirecastleanchesterorkshireanchesterorkshireorkshireanchesterorkshireanchesteranchesteranchesterorkshireanchesteranchesterimbabweanchesterorkshireanchesterorkshiresburghorkshireanchesterorkshireanchesterimbabweanchester\n",
      "r1: tensor(-8.3433e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8505, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7752, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1314, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4064,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4064, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4064, grad_fn=<UnbindBackward>)\n",
      "Episode 128: 0.10435896271346508\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checking inorkshiremanorkshiremanorkshiremanimbabweimbabweimbabweimbabweimbabweorkshiremanimbabweimbabweorkshiremanorkshiremananchesterorkshiremanimbabweimbabweanchesterorkshiremanimbabweanchesterorkshiremananchesterorkshireanchesterorkshireanchesterorkshiremananchesterimbabweanchesterorkshiremanchesteranchesterorkshireanchesterorkshireMANchesteranchesterorkshireanchesteranchesterorkshireanchesterorkshireManchesteranchesterorkshireanchesterorkshirecastleanchesterorkshireanchesterorkshireorkshireanchesterorkshireanchesteranchesteranchesterorkshireanchesteranchesterimbabweanchesterorkshireanchesterorkshiresburghorkshireanchesterorkshireanchesterimbabweanchester\n",
      "r1: tensor(-8.0166e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5200, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5361, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2640,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2640, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2640, grad_fn=<UnbindBackward>)\n",
      "Episode 129: 0.10453063236719738\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen NguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyenginxguyenguyenguyenguyenngguyenguyenguyenguyengueguyenguyenguyenguyenuggetsguyenguyenguyenguyenUGEguyenguyenguyenguyenuggestguyenguyenguyenguyenorgetownguyenguyenguyenguyenmunitionguyenguyenguyenguyenutoniumguyenguyenguyenguyenavoriteguyenguyenguyenguyenuclearguyenguyenguyenguyenperatureguyenguyen\n",
      "r1: tensor(-8.8753e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8721, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8673, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6971, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4348,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4348, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4348, grad_fn=<UnbindBackward>)\n",
      "Episode 130: 0.10488541742734747\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen NguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyenginxguyenguyenguyenguyenngguyenguyenguyenguyengueguyenguyenguyenguyenuggetsguyenguyenguyenguyenUGEguyenguyenguyenguyenuggestguyenguyenguyenguyenorgetownguyenguyenguyenguyenmunitionguyenguyenguyenguyenutoniumguyenguyenguyenguyenavoriteguyenguyenguyenguyenuclearguyenguyenguyenguyenperatureguyenguyen\n",
      "r1: tensor(-9.8785e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8635, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8525, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.7150, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4290,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4290, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4290, grad_fn=<UnbindBackward>)\n",
      "Episode 131: 0.1052331559578868\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checking inorkshiremanorkshiremanorkshiremanimbabweimbabweimbabweimbabweimbabweorkshiremanimbabweimbabweorkshiremanorkshiremananchesterorkshiremanimbabweimbabweanchesterorkshiremanimbabweanchesterorkshiremananchesterorkshireanchesterorkshireanchesterorkshiremananchesterimbabweanchesterorkshiremanchesteranchesterorkshireanchesterorkshireMANchesteranchesterorkshireanchesteranchesterorkshireanchesterorkshireManchesteranchesterorkshireanchesterorkshirecastleanchesterorkshireanchesterorkshireorkshireanchesterorkshireanchesteranchesteranchesterorkshireanchesteranchesterimbabweanchesterorkshireanchesterorkshiresburghorkshireanchesterorkshireanchesterimbabweanchester\n",
      "r1: tensor(-9.7671e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8740, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8471, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1002, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4302,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4302, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4302, grad_fn=<UnbindBackward>)\n",
      "Episode 132: 0.10558150138452425\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu HonoluluoluluoluluoluluoluluaiioluluoluluoluluoluluicagoicagoicagoicagoicagoistaniicagoicagoicagoicagoistanicagoicagoicagoicagoiscicagoicagoicagoicagoiscoicagoicagoicagoicagoiconeicagoicagoicagoicagoillacicagoicagoicagoicagoISCicagoicagoicagoicagochnologyicagoicagoicagoicagoierrezicagoicagoicagoicagoolinaicagoicagoicagoicagoisleicagoicagoicagoicagoiseumicagoicagoicagoicagoisoicagoicagoicagoicagoioxideicagoicagoicagoicagoundaiicagoicagoicagoicagoenzaicagoicago\n",
      "r1: tensor(-0.0002, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7652, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7023, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4338, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3668,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3668, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3668, grad_fn=<UnbindBackward>)\n",
      "Episode 133: 0.10586119990355118\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu HonoluluoluluoluluoluluoluluaiioluluoluluoluluoluluicagoicagoicagoicagoicagoistaniicagoicagoicagoicagoistanicagoicagoicagoicagoiscicagoicagoicagoicagoiscoicagoicagoicagoicagoiconeicagoicagoicagoicagoillacicagoicagoicagoicagoISCicagoicagoicagoicagochnologyicagoicagoicagoicagoierrezicagoicagoicagoicagoolinaicagoicagoicagoicagoisleicagoicagoicagoicagoiseumicagoicagoicagoicagoisoicagoicagoicagoicagoioxideicagoicagoicagoicagoundaiicagoicagoicagoicagoenzaicagoicago\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8385, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8608, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3188, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4248,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4248, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4248, grad_fn=<UnbindBackward>)\n",
      "Episode 134: 0.10620230222718521\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluolulu Honoluluoluluoluluoluluoluluaiioluluoluluoluluoluluadelphiaadelphiaadelphiaadelphiaadelphialehemlehemlehemlehemlehemadelphialehemlehemlehemadelphiaadelphialehemlehemadelphialehemadelphialehemlehemadelphiaadelphiaadelphialehemadelphialehemadelphiaadelphialehemadelphiaadelphiaadelphiaadelphiasburghlehemlehemlehemlehemsburghlehemlehemlehemadelphiasburghlehemlehemadelphialehemsburghlehemlehemadelphiaadelphiasburghlehemadelphialehemlehemsburghlehemadelphialehemadelphiasburghlehemadelphiaadelphialehemsburghlehemadelphiaadelphiaadelphiasburghsburghlehemlehemlehemsburghsburgh\n",
      "r1: tensor(-9.7031e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6633, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8294, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3732,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3732, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3732, grad_fn=<UnbindBackward>)\n",
      "Episode 135: 0.10648751443472858\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluohaohaohaohaohaoluluoluluoluluoluluoaohaohaohaohaoaohaohaohaoluluohaohaohaoaoaohaohaohaoaoluluoluluoluluoluluohoohaohaohaohaohoohaohaohaoaohoohaohaoaohaoaohaoaohoohaoaohaoaoaohaoaohaohoohoohoohoohoohaoaohoohoohoohooaohoohoohoohaohoohoohoohaohaohoohoohooaohaohoohoohaohoohaohoohooha\n",
      "r1: tensor(-7.8999e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.4323, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8970, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3323,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3323, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3323, grad_fn=<UnbindBackward>)\n",
      "Episode 136: 0.10672852056589983\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluohaohaohaohaohaoluluoluluoluluoluluoaohaohaohaohaoaohaohaohaoluluohaohaohaoaoaohaohaohaoaoluluoluluoluluoluluohoohaohaohaohaohoohaohaohaoaohoohaohaoaohaoaohaoaohoohaoaohaoaoaohaoaohaohoohoohoohoohoohaoaohoohoohoohooaohoohoohoohaohoohoohoohaohaohoohoohooaohaohoohoohaohoohaohoohooha\n",
      "r1: tensor(-0.0003, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8618, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8692, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2808, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4327,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4327, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4327, grad_fn=<UnbindBackward>)\n",
      "Episode 137: 0.1070760056085543\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluohaohaohaohaohaoluluoluluoluluoluluoaohaohaohaohaoaohaohaohaoluluohaohaohaoaoaohaohaohaoaoluluoluluoluluoluluohoohaohaohaohaohoohaohaohaoaohoohaohaoaohaoaohaoaohoohaoaohaoaoaohaoaohaohoohoohoohoohoohaoaohoohoohoohooaohoohoohoohaohoohoohoohaohaohoohoohooaohaohoohoohaohoohaohoohooha\n",
      "r1: tensor(-8.9806e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8595, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6074, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2659, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3667,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3667, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3667, grad_fn=<UnbindBackward>)\n",
      "Episode 138: 0.10735248741708672\n",
      "User: Hello\n",
      "DialoGPT: raltarraltarraltarraltarraltar GibraltarraltarraltarraltarraltaruaniaraltarraltarraltarraltaremaleraltarraltarraltarraltarlehemraltarraltarraltarraltarorkshireraltarraltarraltarraltarcastleraltarraltarraltarraltarLagoraltarraltarraltarraltarmaxwellraltarraltarraltarraltarmileraltarraltarraltarraltarmereraltarraltarraltarraltaranchesterraltarraltarraltarraltarmunitionraltarraltarraltarraltaruphemraltarraltarraltarraltarultonraltarraltarraltarraltarvantageraltarraltarraltarraltarlandishraltarraltarraltarraltarployraltarraltarraltarraltarxtapositionraltarraltarraltarraltarmonaryraltar\n",
      "r1: tensor(-9.0334e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7923, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7613, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6679, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3884,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3884, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3884, grad_fn=<UnbindBackward>)\n",
      "Episode 139: 0.10765144929557326\n",
      "User: Hello\n",
      "DialoGPT: raltarraltarraltarraltarraltar GibraltarraltarraltarraltarraltaruaniaraltarraltarraltarraltaremaleraltarraltarraltarraltarlehemraltarraltarraltarraltarorkshireraltarraltarraltarraltarcastleraltarraltarraltarraltarLagoraltarraltarraltarraltarmaxwellraltarraltarraltarraltarmileraltarraltarraltarraltarmereraltarraltarraltarraltaranchesterraltarraltarraltarraltarmunitionraltarraltarraltarraltaruphemraltarraltarraltarraltarultonraltarraltarraltarraltarvantageraltarraltarraltarraltarlandishraltarraltarraltarraltarployraltarraltarraltarraltarxtapositionraltarraltarraltarraltarmonaryraltar\n",
      "r1: tensor(-8.4701e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8183, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8298, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6618, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4120,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4120, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4120, grad_fn=<UnbindBackward>)\n",
      "Episode 140: 0.10797487340192533\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: hovahhovahhovahhovahhovah Jehovahhovahhovahhovahhovah Witnesses Witnesses Witnesses Witnesses Witnesses Witness Witnesses Witnesses Witnesses Witnesses Jehovah Witnesses Witnesses Witnesses Witnesseshovah Witnesses Witnesses Witnesses Witnessors Witnesses Witnesses Witnesses Witnesses witnesses Witnesses Witnesses Witnesses Witnesses congregants Witnesses Witnesses Witnesses Witnesses followers Witnesses Witnesses Witnesses Witnesses members Witnesses Witnesses Witnesses Witnesses Friends Witnesses Witnesses Witnesses Witnesses friends Witnesses Witnesses Witnesses Witnesses wives Witnesses Witnesses Witnesses Witnesses spouses Witnesses Witnesses Witnesses Witnesses husbands wives wives husbands husbands husbands husbands husbands wives husbands husbands husbands wives wives husbands wives husbands husbands wives husbands wives husbands\n",
      "r1: tensor(-8.9593e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7496, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8370, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4354, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3966,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3966, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3966, grad_fn=<UnbindBackward>)\n",
      "Episode 141: 0.10828129020163635\n",
      "User: Hello\n",
      "DialoGPT: hovahhovahhovahhovahhovah Jehovahhovahhovahhovahhovah Witnesses Witnesses Witnesses Witnesses Witnesses Witness Witnesses Witnesses Witnesses Witnesses Jehovah Witnesses Witnesses Witnesses Witnesseshovah Witnesses Witnesses Witnesses Witnessors Witnesses Witnesses Witnesses Witnesses witnesses Witnesses Witnesses Witnesses Witnesses congregants Witnesses Witnesses Witnesses Witnesses followers Witnesses Witnesses Witnesses Witnesses members Witnesses Witnesses Witnesses Witnesses Friends Witnesses Witnesses Witnesses Witnesses Members Witnesses Witnesses Witnesses Witnesses Children Witnesses Witnesses Witnesses Witnesses Sons Witnesses Witnesses Witnesses Witnesses Parents Witnesses Witnesses Witnesses Witnesses Families Witnesses Witnesses Witnesses Witnesses Fathers Witnesses Witnesses Witnesses Witnesses Mothers Witnesses Witnesses Witnesses Witnesses Family Witnesses\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7640, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8094, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4270, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3933,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3933, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3933, grad_fn=<UnbindBackward>)\n",
      "Episode 142: 0.10858353940781881\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluohaohaohaohaohaoluluoluluoluluoluluohoohaohaohaohaohoohaohaohaohoohoohoohoohoohaohoohoohoohaohaohoohoohaohoohaohoohoohaohaohaoaohoohoohoohooboobooboobooboohooboobooboohoohoobooboohooboohoobooboohoohoohooboohooboohoohooboohoohoohoohoboaohoohoohoohoaboobooboobooboboaohooboobooboboaoboobo\n",
      "r1: tensor(-9.8502e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8968, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8813, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3948, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4445,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4445, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4445, grad_fn=<UnbindBackward>)\n",
      "Episode 143: 0.10893938245234017\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluohaohaohaohaohaoluluoluluoluluoluluohoohaohaohaohaohoohaohaohaohoohoohoohoohoohaohoohoohoohaohaohoohoohaohoohaohoohoohaohaohaoaohoohoohoohooboobooboobooboohooboobooboohoohoobooboohooboohoobooboohoohoohooboohooboohoohooboohoohoohoohoboaohoohoohoohoaboobooboobooboboaohooboobooboboaoboobo\n",
      "r1: tensor(-8.3655e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8674, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8933, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3985, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4401,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4401, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4401, grad_fn=<UnbindBackward>)\n",
      "Episode 144: 0.10928986939754357\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea party member checking in confirming suspicions suspicions suspicions suspicions suspicions suspicion suspicions suspicions suspicions suspicions doubts suspicions suspicions suspicions suspicions theories suspicions suspicions suspicions suspicions rumours suspicions suspicions suspicions suspicions accusations suspicions suspicions suspicions suspicions allegations allegations allegations allegations allegations accusations allegations allegations allegations allegations suspicions allegations allegations allegations accusations accusations allegations allegations allegations accusations suspicions allegations allegations accusations accusations accusations accusations accusations allegations accusations accusations accusations allegations allegations accusations accusations suspicions accusations accusations accusations accusations suspicions accusations suspicions suspicions accusations accusations accusations suspicions suspicions suspicions accusations accusations suspicions suspicions accusations suspicions accusations suspicions accusations accusations\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7344, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7553, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3654, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3724,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3724, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3724, grad_fn=<UnbindBackward>)\n",
      "Episode 145: 0.109567992057923\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea beverage tea tea tea tea Earl Grey tea tea tea tea soda tea tea tea tea liquor tea tea tea tea booze tea tea tea tea wine tea tea tea tea cider cider cider cider cider\n",
      "r1: tensor(-8.2910e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8781, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8418, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2967, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4300,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4300, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4300, grad_fn=<UnbindBackward>)\n",
      "Episode 146: 0.10990631753389961\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea coffee tea tea tea tea beverage tea tea tea tea beverages tea tea tea tea drinks tea tea tea tea coffees tea tea tea tea cups tea tea tea tea Cups tea tea tea tea cakes tea tea tea tea\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2294, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8612, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3635, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2726,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2726, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2726, grad_fn=<UnbindBackward>)\n",
      "Episode 147: 0.11007796844049016\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea party member checking in confirming suspicions suspicions suspicions suspicions suspicions suspicion suspicions suspicions suspicions suspicions doubts suspicions suspicions suspicions suspicions theories suspicions suspicions suspicions suspicions rumours suspicions suspicions suspicions suspicions accusations suspicions suspicions suspicions suspicions allegations allegations allegations allegations allegations accusations allegations allegations allegations allegations suspicions allegations allegations allegations accusations accusations allegations allegations allegations accusations suspicions allegations allegations allegations suspicions suspicions suspicions suspicions rumors allegations accusations accusations accusations accusations accusations suspicions suspicions suspicions accusations accusations accusations accusations allegations accusations accusations accusations suspicions accusations accusations accusations suspicions allegations accusations accusations accusations allegations suspicions suspicions suspicions accusations allegations\n",
      "r1: tensor(-8.8244e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8292, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8201, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3303, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4123,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4123, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4123, grad_fn=<UnbindBackward>)\n",
      "Episode 148: 0.11039643346020918\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea party member checking in confirming suspicions suspicions suspicions suspicions suspicions suspicion suspicions suspicions suspicions suspicions doubts suspicions suspicions suspicions suspicions theories suspicions suspicions suspicions suspicions rumours suspicions suspicions suspicions suspicions accusations suspicions suspicions suspicions suspicions allegations allegations allegations allegations allegations accusations allegations allegations allegations allegations suspicions allegations allegations allegations accusations accusations allegations allegations allegations accusations suspicions allegations allegations allegations suspicions suspicions suspicions suspicions rumors allegations accusations accusations accusations accusations accusations suspicions suspicions suspicions accusations accusations accusations accusations allegations accusations accusations accusations suspicions accusations accusations accusations suspicions allegations accusations accusations accusations allegations suspicions suspicions suspicions accusations allegations\n",
      "r1: tensor(-8.7094e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8500, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.3441, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.2985,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.2985, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.2985, grad_fn=<UnbindBackward>)\n",
      "Episode 149: 0.11059443260445015\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluohaohaohaohaohaoluluoluluoluluoluluohoohaohaohaohaohoohaohaohaohoohoohoohoohoohaohoohoohoohaohaohoohoohaohoohaohoohoohaohaohaoaohoohoohoohooboobooboobooboohooboobooboohoohoobooboohooboohoobooboohoohoohooboohooboohoohooboohoohoohoohoboaohoohoohoohoaboobooboobooboboaohooboobooboboaoboobo\n",
      "r1: tensor(-9.0633e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.9230, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8762, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3654, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4498,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4498, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4498, grad_fn=<UnbindBackward>)\n",
      "Episode 150: 0.11095110055100325\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen NguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyengueguyenguyenguyenguyenginxguyenguyenguyenguyenUGEguyenguyenguyenguyenuggetsguyenguyenguyenguyenuggestguyenguyenguyenguyenugsguyenguyenguyenguyenngguyenguyenguyenguyenGGGGGGGGguyenguyenguyenguyenUGguyenguyenguyenguyenNGguyenguyenguyenguyenGCguyenguyenguyenguyenGGGGguyenguyen\n",
      "r1: tensor(-8.3742e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8067, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6483, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6837, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3637,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3637, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3637, grad_fn=<UnbindBackward>)\n",
      "Episode 151: 0.11121662358602728\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluohaohaohaohaohaoluluoluluoluluoluluohoohaohaohaohaohoohaohaohaohoohoohoohoohoohaohoohoohoohaohaohoohoohaohoohaohoohoohaohaohaoaohoohoohoohooboobooboobooboohooboobooboohoohoobooboohooboohoobooboohoohoohooboohooboohoohooboohoohoohoohoboaohoohoohoohoaboobooboobooboboaohooboobooboboaoboobo\n",
      "r1: tensor(-7.9674e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7513, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7075, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4143, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3647,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3647, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3647, grad_fn=<UnbindBackward>)\n",
      "Episode 152: 0.11148258593326273\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen NguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyengueguyenguyenguyenguyenginxguyenguyenguyenguyenUGEguyenguyenguyenguyenuggetsguyenguyenguyenguyenuggestguyenguyenguyenguyenugsguyenguyenguyenguyenngguyenguyenguyenguyenGGGGGGGGguyenguyenguyenguyenUGguyenguyenguyenguyenNGguyenguyenguyenguyenGCguyenguyenguyenguyenGGGGguyenguyen\n",
      "r1: tensor(-8.3156e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8988, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8808, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6768, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4449,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4449, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4449, grad_fn=<UnbindBackward>)\n",
      "Episode 153: 0.11183207802253871\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaohaoluluoluluoluluohaohaoluluoluluoluluoaoaohaoluluoluluohaoluluohaoluluoluluohaohaohaoluluoluluohaoaohaoluluoluluoaohaohaoluluoluluoaoaoaohaoluluoaohaoluluoaoaohaohaoluluoaohaohaoaohaoluluoaoluluoaohaoluluohaoluluoaohaoaohaoluluohaoaohaoaohaoaoluluoaohaoaoluluohaoaohaohaoluluoha\n",
      "r1: tensor(-8.7878e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8768, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8851, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2017, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4404,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4404, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4404, grad_fn=<UnbindBackward>)\n",
      "Episode 154: 0.11217617239578778\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checking inorkshiremanorkshiremanorkshiremanimbabweimbabweimbabweimbabweimbabweorkshiremanimbabweimbabweorkshiremanorkshiremananchesterorkshiremanimbabweimbabweanchesterorkshiremanimbabweanchesterorkshiremananchesterorkshireanchesterorkshireanchesterorkshiremananchesterimbabweanchesterorkshiremanchesteranchesterorkshireanchesterorkshireMANchesteranchesterorkshireanchesteranchesterorkshireanchesterorkshireManchesteranchesterorkshireanchesterorkshirecastleanchesterorkshireanchesterorkshireorkshireanchesterorkshireanchesteranchesteranchesterorkshireanchesteranchesterimbabweanchesterorkshireanchesterorkshiresburghorkshireanchesterorkshireanchesterimbabweanchester\n",
      "r1: tensor(-7.9851e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8833, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8621, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1326, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4363,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4363, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4363, grad_fn=<UnbindBackward>)\n",
      "Episode 155: 0.11251522962220216\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checking inorkshiremanorkshiremanorkshiremanimbabweimbabweimbabweimbabweimbabweorkshiremanimbabweimbabweorkshiremanorkshiremananchesterorkshiremanimbabweimbabweanchesterorkshiremanimbabweanchesterorkshiremananchesterorkshireanchesterorkshireanchesterorkshiremananchesterimbabweanchesterorkshiremanchesteranchesterorkshireanchesterorkshireMANchesteranchesterorkshireanchesteranchesterorkshireanchesterorkshireManchesteranchesterorkshireanchesterorkshirecastleanchesterorkshireanchesterorkshireorkshireanchesterorkshireanchesteranchesteranchesterorkshireanchesteranchesterimbabweanchesterorkshireanchesterorkshiresburghorkshireanchesterorkshireanchesterimbabweanchester\n",
      "r1: tensor(-9.6707e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8614, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7862, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4119,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4119, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4119, grad_fn=<UnbindBackward>)\n",
      "Episode 156: 0.11282804586497391\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checking inorkshiremanorkshiremanorkshiremanimbabweimbabweimbabweimbabweimbabweorkshiremanimbabweimbabweorkshiremanorkshiremananchesterorkshiremanimbabweimbabweanchesterorkshiremanimbabweanchesterorkshiremananchesterorkshireanchesterorkshireanchesterorkshiremananchesterimbabweanchesterorkshiremanchesteranchesterorkshireanchesterorkshireMANchesteranchesterorkshireanchesteranchesterorkshireanchesterorkshireManchesteranchesterorkshireanchesterorkshirecastleanchesterorkshireanchesterorkshireorkshireanchesterorkshireanchesteranchesteranchesterorkshireanchesteranchesterimbabweanchesterorkshireanchesterorkshiresburghorkshireanchesterorkshireanchesterimbabweanchester\n",
      "r1: tensor(-8.7413e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.9031, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8747, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1077, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4444,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4444, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4444, grad_fn=<UnbindBackward>)\n",
      "Episode 157: 0.11317417859235906\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checking inorkshiremanorkshiremanorkshiremanimbabweimbabweimbabweimbabweimbabweorkshiremanimbabweimbabweorkshiremanorkshiremananchesterorkshiremanimbabweimbabweanchesterorkshiremanimbabweanchesterorkshiremananchesterorkshireanchesterorkshireanchesterorkshiremananchesterimbabweanchesterorkshiremanchesteranchesterorkshireanchesterorkshireMANchesteranchesterorkshireanchesteranchesterorkshireanchesterorkshireManchesteranchesterorkshireanchesterorkshirecastleanchesterorkshireanchesterorkshireorkshireanchesterorkshireanchesteranchesteranchesterorkshireanchesteranchesterimbabweanchesterorkshireanchesterorkshiresburghorkshireanchesterorkshireanchesterimbabweanchester\n",
      "r1: tensor(-9.5171e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7110, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8781, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1054, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3973,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3973, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3973, grad_fn=<UnbindBackward>)\n",
      "Episode 158: 0.11347040288466902\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaohaoluluoluluoluluohaohaoluluoluluoluluoaoluluoluluoluluoluluohoohaoluluoluluoluluohooluluoluluoluluoluluoyaoaohaoluluoluluohoohoohoohoohoohaohoohoohoohooaohoohoohoohooboobooboobooboohoohoohoohoboaboaboaboaboaobooboobooboboaboaboaboaobaobooboobooboaboobooboobooboobaoboobooboboaobo\n",
      "r1: tensor(-8.8104e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8328, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8755, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4271,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4271, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4271, grad_fn=<UnbindBackward>)\n",
      "Episode 159: 0.11379705139697763\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaohaoluluoluluoluluohaohaoluluoluluoluluoaoluluoluluoluluoluluohoohaoluluoluluoluluohooluluoluluoluluoluluoyaoaohaoluluoluluohoohoohoohoohoohaohoohoohoohooaohoohoohoohooboobooboobooboohoohoohoohoboaboaboaboaboaobooboobooboboaboaboaboaobaobooboobooboaboobooboobooboobaoboobooboboaobo\n",
      "r1: tensor(-8.2205e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7799, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8281, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4269, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4020,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4020, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4020, grad_fn=<UnbindBackward>)\n",
      "Episode 160: 0.1140969354076879\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaohaoluluoluluoluluohaohaoluluoluluoluluoaoluluoluluoluluoluluohoohaoluluoluluoluluohoohoohoohoohoohaohoohoohoohooaohoohoohoohooboobooboobooboohoohoohoohoboaboaboaboaboaobooboobooboboaboaboaboaobaobooboobooboaboobooboobooboobaoboobooboboaoboobooboboaobaobooboboaboaoboobo\n",
      "r1: tensor(-7.6627e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.9215, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8444, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3194, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4415,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4415, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4415, grad_fn=<UnbindBackward>)\n",
      "Episode 161: 0.11443722759191481\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen NguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyengueguyenguyenguyenguyenginxguyenguyenguyenguyenUGEguyenguyenguyenguyenuggetsguyenguyenguyenguyenuggestguyenguyenguyenguyenugsguyenguyenguyenguyenngguyenguyenguyenguyenGGGGGGGGguyenguyenguyenguyenUGguyenguyenguyenguyenNGguyenguyenguyenguyenGCguyenguyenguyenguyenVIDIAguyenguyen\n",
      "r1: tensor(-8.3333e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7166, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6600, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6956, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3441,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3441, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3441, grad_fn=<UnbindBackward>)\n",
      "Episode 162: 0.11467573845078192\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen NguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyengueguyenguyenguyenguyenginxguyenguyenguyenguyenUGEguyenguyenguyenguyenuggetsguyenguyenguyenguyenuggestguyenguyenguyenguyenugsguyenguyenguyenguyenngguyenguyenguyenguyenGGGGGGGGguyenguyenguyenguyenUGguyenguyenguyenguyenNGguyenguyenguyenguyenGCguyenguyenguyenguyenVIDIAguyenguyen\n",
      "r1: tensor(-9.1529e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8080, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8404, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6291, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4121,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4121, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4121, grad_fn=<UnbindBackward>)\n",
      "Episode 163: 0.11498425025985619\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea Teepee tea tea tea tea coffee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Coffee Tea Tea Tea Tea Te Tea Tea Tea Tea CoffeesTea\n",
      "r1: tensor(-7.9724e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8758, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.4444, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3300,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3300, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3300, grad_fn=<UnbindBackward>)\n",
      "Episode 164: 0.11520711010553598\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea Teepee tea tea tea tea coffee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Coffee Tea Tea Tea Tea Te Tea Tea Tea Tea CoffeesTea\n",
      "r1: tensor(-8.0622e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8935, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6257, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3815, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3798,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3798, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3798, grad_fn=<UnbindBackward>)\n",
      "Episode 165: 0.11548099520242162\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea tea tea tea tea Tea Tea Tea Tea TeaTea Tea Tea Tea Tea tea Tea Tea Tea tea tea tea teaTea Tea Tea Tea teaTea Tea Tea tea tea Tea Tea tea teaTea Tea tea tea tea Tea tea tea teaTea tea tea tea tea te tea tea tea tea Teepee tea tea tea tea coffee tea tea tea tea Earl Grey Tea Tea Tea Tea Teepee Tea Tea Tea Tea Earl Tea Tea Tea Tea Coffee Tea Tea Tea Tea Te Tea Tea Tea Tea CoffeesTea\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8330, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8764, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3764, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4273,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4273, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4273, grad_fn=<UnbindBackward>)\n",
      "Episode 166: 0.1158034637211143\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaohaoluluoluluoluluohaohaoluluoluluoluluoaoaohaoluluoluluohaoluluohaoluluoluluohaohaohaoluluoluluohaoaohaoluluoluluoaohaohaoluluoluluoaoaoaohaoluluoaohaoluluoaoaohaohaoluluoaohaohaoaohaoluluoaoluluoaohaoluluohaoluluoaohaoaohaoluluohaoaohaoaohaohaoluluohaoaoluluoaohaohaohaoluluoaolulu\n",
      "r1: tensor(-0.0003, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8798, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.9220, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4504,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4504, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4504, grad_fn=<UnbindBackward>)\n",
      "Episode 167: 0.11614910214475534\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaohaoluluoluluoluluohaohaoluluoluluoluluoaoaohaoluluoluluohaoluluohaoluluoluluohaohaohaoluluoluluohaoaohaoluluoluluoaohaohaoluluoluluoaoaoaohaoluluoaohaoluluoaoaohaohaoluluoaohaohaoaohaoluluoaoluluoaohaoluluohaoluluoaohaoaohaoluluohaoaohaoaohaohaoluluohaoaoluluoaohaohaohaoluluoaolulu\n",
      "r1: tensor(-8.3001e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.9036, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8951, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4496,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4496, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4496, grad_fn=<UnbindBackward>)\n",
      "Episode 168: 0.11649327021586191\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checking inorkshiremanorkshiremanorkshiremanimbabweimbabweimbabweimbabweimbabweorkshiremanimbabweimbabweorkshiremanorkshiremananchesterorkshiremanimbabweimbabweanchesterorkshiremanimbabweanchesterorkshiremananchesterorkshireanchesterorkshireanchesterorkshiremananchesterimbabweanchesterorkshiremanchesteranchesterorkshireanchesterorkshireMANchesteranchesterorkshireanchesteranchesterorkshireanchesterorkshireManchesteranchesterorkshireanchesterorkshirecastleanchesterorkshireanchesterorkshireorkshireanchesterorkshireanchesteranchesteranchesterorkshireanchesteranchesterimbabweanchesterorkshireanchesterorkshiresburghorkshireanchesterorkshireanchesterimbabweanchester\n",
      "r1: tensor(-6.4012e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8699, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6741, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3860,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3860, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3860, grad_fn=<UnbindBackward>)\n",
      "Episode 169: 0.11677108821747152\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checking inorkshiremanorkshiremanorkshiremanimbabweimbabweimbabweimbabweimbabweorkshiremanimbabweimbabweorkshiremanorkshiremananchesterorkshiremanimbabweimbabweanchesterorkshiremanimbabweanchesterorkshiremananchesterorkshireanchesterorkshireanchesterorkshiremananchesterimbabweanchesterorkshiremanchesteranchesterorkshireanchesterorkshireMANchesteranchesterorkshireanchesteranchesterorkshireanchesterorkshireManchesteranchesterorkshireanchesterorkshirecastleanchesterorkshireanchesterorkshireorkshireanchesterorkshireanchesteranchesteranchesterorkshireanchesteranchesterimbabweanchesterorkshireanchesterorkshiresburghorkshireanchesterorkshireanchesterimbabweanchester\n",
      "r1: tensor(-8.1761e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8104, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8885, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1207, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4247,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4247, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4247, grad_fn=<UnbindBackward>)\n",
      "Episode 170: 0.11708823312889213\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checking inorkshiremanorkshiremanorkshiremanimbabweimbabweimbabweimbabweimbabweorkshiremanimbabweimbabweorkshiremanorkshiremananchesterorkshiremanimbabweimbabweanchesterorkshiremanimbabweanchesterorkshiremananchesterorkshireanchesterorkshireanchesterorkshiremananchesterimbabweanchesterorkshiremanchesteranchesterorkshireanchesterorkshireMANchesteranchesterorkshireanchesteranchesterorkshireanchesterorkshireManchesteranchesterorkshireanchesterorkshirecastleanchesterorkshireanchesterorkshireorkshireanchesterorkshireanchesteranchesteranchesterorkshireanchesteranchesterimbabweanchesterorkshireanchesterorkshiresburghorkshireanchesterorkshireanchesterimbabweanchester\n",
      "r1: tensor(-9.1753e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7099, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8519, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.0851, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3904,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3904, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3904, grad_fn=<UnbindBackward>)\n",
      "Episode 171: 0.11736944167531735\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaohaoluluoluluoluluohaohaoluluoluluoluluoaoluluoluluoluluoluluohoohaoluluoluluoluluohoohoohoohoohoohaohoohoohoohooaohoohoohoohooboobooboobooboohoohoohoohoboaboaboaboaboaobooboobooboboaboaboaboaobaobooboobooboaboobooboobooboobaoboobooboboaoboobooboboaobaobooboboaboaoboobo\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.9298, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7927, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3979, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4306,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4306, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4306, grad_fn=<UnbindBackward>)\n",
      "Episode 172: 0.11769136783350087\n",
      "User: Hello\n",
      "DialoGPT: raltarraltarraltarraltarraltar GibraltarraltarraltarraltarraltaruaniaraltarraltarraltarraltaremaleraltarraltarraltarraltarlehemraltarraltarraltarraltarorkshireraltarraltarraltarraltarcastleraltarraltarraltarraltarLagoraltarraltarraltarraltarmaxwellraltarraltarraltarraltarmileraltarraltarraltarraltarmereraltarraltarraltarraltarierrezraltarraltarraltarraltaranchesterraltarraltarraltarraltarmunitionraltarraltarraltarraltarlandishraltarraltarraltarraltaruphemraltarraltarraltarraltarschildraltarraltarraltarraltarultonraltarraltarraltarraltarployraltarraltarraltarraltarxtapositionraltar\n",
      "r1: tensor(-8.2857e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8591, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8443, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4258,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4258, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4258, grad_fn=<UnbindBackward>)\n",
      "Episode 173: 0.11800772331041155\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaohaoluluoluluoluluohaohaoluluoluluoluluoaoluluoluluoluluoluluohoohaoluluoluluoluluohoohoohoohoohoohaohoohoohoohooaohoohoohoohooboobooboobooboohoohoohoohoboaboaboaboaboaobooboobooboboaboaboaboaobaobooboobooboaboobooboobooboobaoboobooboboaoboobooboboaobaobooboboaboaoboobo\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.9055, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8886, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4485,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4485, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4485, grad_fn=<UnbindBackward>)\n",
      "Episode 174: 0.11834668819673168\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaohaoluluoluluoluluohaohaoluluoluluoluluoaoluluoluluoluluoluluohoohaoluluoluluoluluohoohoohoohoohoohaohoohoohoohooaohoohoohoohooboobooboobooboohoohoohoohoboaboaboaboaboaobooboobooboboaboaboaboaobaobooboobooboaboobooboobooboobaoboobooboboaoboobooboboaobaobooboboaboaoboobo\n",
      "r1: tensor(-0.0002, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8304, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8198, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2988, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4125,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4125, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4125, grad_fn=<UnbindBackward>)\n",
      "Episode 175: 0.11864808549843041\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaohaoluluoluluoluluohaohaoluluoluluoluluoaoluluoluluoluluoluluohoohaoluluoluluoluluohoohoohoohoohoohaohoohoohoohooaohoohoohoohooboobooboobooboohoohoohoohoboaboaboaboaboaobooboobooboboaboaboaboaobaobooboobooboaboobooboobooboobaoboobooboboaoboobooboboaobaobooboboaboaoboobo\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8571, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8956, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3936, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4381,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4381, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4381, grad_fn=<UnbindBackward>)\n",
      "Episode 176: 0.1189750961901214\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen NguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyengueguyenguyenguyenguyenginxguyenguyenguyenguyenUGEguyenguyenguyenguyenuggetsguyenguyenguyenguyenuggestguyenguyenguyenguyenugsguyenguyenguyenguyenngguyenguyenguyenguyenGGGGGGGGguyenguyenguyenguyenUGguyenguyenguyenguyenNGguyenguyenguyenguyenGCguyenguyenguyenguyenVIDIAguyenguyen\n",
      "r1: tensor(-8.4332e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8421, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.9142, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.7198, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4391,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4391, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4391, grad_fn=<UnbindBackward>)\n",
      "Episode 177: 0.11930237865839191\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen NguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyengueguyenguyenguyenguyenginxguyenguyenguyenguyenUGEguyenguyenguyenguyenuggetsguyenguyenguyenguyenuggestguyenguyenguyenguyenugsguyenguyenguyenguyenngguyenguyenguyenguyenGGGGGGGGguyenguyenguyenguyenUGguyenguyenguyenguyenNGguyenguyenguyenguyenGCguyenguyenguyenguyenVIDIAguyenguyen\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8659, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7661, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4080,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4080, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4080, grad_fn=<UnbindBackward>)\n",
      "Episode 178: 0.11959723901408853\n",
      "User: Hello\n",
      "DialoGPT: raltarraltarraltarraltarraltar GibraltarraltarraltarraltarraltaruaniaraltarraltarraltarraltaremaleraltarraltarraltarraltarlehemraltarraltarraltarraltarorkshireraltarraltarraltarraltarcastleraltarraltarraltarraltarLagoraltarraltarraltarraltarmaxwellraltarraltarraltarraltarmileraltarraltarraltarraltarmereraltarraltarraltarraltarierrezraltarraltarraltarraltaranchesterraltarraltarraltarraltarmunitionraltarraltarraltarraltaruphemraltarraltarraltarraltarschildraltarraltarraltarraltarlandishraltarraltarraltarraltarultonraltarraltarraltarraltarployraltarraltarraltarraltarxtapositionraltar\n",
      "r1: tensor(-8.6705e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7471, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7584, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5971, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3764,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3764, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3764, grad_fn=<UnbindBackward>)\n",
      "Episode 179: 0.11985923295070855\n",
      "User: Hello\n",
      "DialoGPT: DESU76561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561\n",
      "r1: tensor(-9.5442e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8796, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7763, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.0004, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4139,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4139, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4139, grad_fn=<UnbindBackward>)\n",
      "Episode 180: 0.12015901100332872\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea party member checking in confirming suspicions suspicions suspicions suspicions suspicions suspicion suspicions suspicions suspicions suspicions doubts suspicions suspicions suspicions suspicions theories suspicions suspicions suspicions suspicions rumours suspicions suspicions suspicions suspicions accusations suspicions suspicions suspicions suspicions allegations allegations allegations allegations allegations accusations allegations allegations allegations allegations suspicions allegations allegations allegations accusations accusations allegations allegations allegations accusations suspicions allegations allegations accusations accusations accusations accusations accusations allegations accusations accusations accusations allegations allegations accusations accusations suspicions accusations accusations accusations accusations suspicions accusations suspicions suspicions accusations accusations accusations suspicions suspicions suspicions accusations accusations suspicions suspicions accusations suspicions accusations suspicions accusations accusations\n",
      "r1: tensor(-8.6529e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7920, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8085, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4001,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4001, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4001, grad_fn=<UnbindBackward>)\n",
      "Episode 181: 0.12044407827865006\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea party member checking in confirming suspicions suspicions suspicions suspicions suspicions suspicion suspicions suspicions suspicions suspicions doubts suspicions suspicions suspicions suspicions theories suspicions suspicions suspicions suspicions rumours suspicions suspicions suspicions suspicions accusations suspicions suspicions suspicions suspicions allegations allegations allegations allegations allegations accusations allegations allegations allegations allegations suspicions allegations allegations allegations accusations accusations allegations allegations allegations accusations suspicions allegations allegations accusations accusations accusations accusations accusations allegations accusations accusations accusations allegations allegations accusations accusations suspicions accusations accusations accusations accusations suspicions accusations suspicions suspicions accusations accusations accusations suspicions suspicions suspicions accusations accusations suspicions suspicions accusations suspicions accusations suspicions accusations accusations\n",
      "r1: tensor(-8.7900e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7451, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8296, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3936,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3936, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3936, grad_fn=<UnbindBackward>)\n",
      "Episode 182: 0.12072199388972786\n",
      "User: Hello\n",
      "DialoGPT: orkshire tea party member checking in confirming suspicions suspicions suspicions suspicions suspicions suspicion suspicions suspicions suspicions suspicions doubts suspicions suspicions suspicions suspicions theories suspicions suspicions suspicions suspicions rumours suspicions suspicions suspicions suspicions accusations suspicions suspicions suspicions suspicions allegations allegations allegations allegations allegations accusations allegations allegations allegations allegations suspicions allegations allegations allegations accusations accusations allegations allegations allegations accusations suspicions allegations allegations accusations accusations accusations accusations accusations allegations accusations accusations accusations allegations allegations accusations accusations suspicions accusations accusations accusations accusations suspicions accusations suspicions suspicions accusations accusations accusations suspicions suspicions suspicions accusations accusations suspicions suspicions accusations suspicions accusations suspicions accusations accusations\n",
      "r1: tensor(-7.6299e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8511, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7018, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4311, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3882,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3882, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3882, grad_fn=<UnbindBackward>)\n",
      "Episode 183: 0.12099384068183949\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaohaoluluoluluoluluohaohaoluluoluluoluluoaoaohaoluluoluluohaoluluohaoluluoluluohaohaohaoluluoluluohaoaohaoluluoluluoaohaohaoluluohaoluluohaohaoluluohaohaohaohaoluluohaoaohaohaoluluoluluoaoluluohaoluluohaoaoluluohaoluluoaoluluohaohaoluluoaoluluoaoluluohaoaoluluoaoluluoaohaoluluohaoluluoaohaoluluoaolulu\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8337, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.9099, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2334, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4359,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4359, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4359, grad_fn=<UnbindBackward>)\n",
      "Episode 184: 0.12131351156840998\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaohaoluluoluluoluluohaohaoluluoluluoluluoaoaohaoluluoluluohaoluluohaoluluoluluohaohaohaoluluoluluohaoaohaoluluoluluoaohaohaoluluohaoluluohaohaoluluohaohaohaohaoluluohaoaohaohaoluluoluluoaoluluohaoluluohaoaoluluohaoluluoaoluluohaohaoluluoaoluluoaoluluohaoaoluluoaoluluoaohaoluluohaoluluoaohaoluluoaolulu\n",
      "r1: tensor(-9.6302e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8997, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.9056, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.1624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4513,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4513, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4513, grad_fn=<UnbindBackward>)\n",
      "Episode 185: 0.12164818729712244\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluaiioluluoluluoluluoluluohaoluluoluluoluluoluluoaohaoluluoluluoluluohaohaoluluoluluoluluoaoaohaoluluoluluohaoluluohaoluluoluluohaohaohaoluluoluluohaoaohaoluluoluluoaohaohaoluluohaoluluohaohaoluluohaohaohaohaoluluohaoaohaohaoluluoluluoaoluluohaoluluohaoaoluluohaoluluoaoluluohaohaoluluoaoluluoaoluluohaoaoluluoaoluluoaohaoluluohaoluluoaohaoluluoaolulu\n",
      "r1: tensor(-8.5548e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.9185, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.9099, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2016, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4571,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4571, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4571, grad_fn=<UnbindBackward>)\n",
      "Episode 186: 0.12198803323937209\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checkingimbabweimbabweimbabweimbabweimbabweorkshireimbabweimbabweimbabweimbabwe Zimbabweimbabweimbabweimbabweimbabwewanaimbabweimbabweimbabweimbabweabweimbabweimbabweimbabweimbabweminghamimbabweimbabweimbabweimbabweierrezimbabweimbabweimbabweimbabwehabiimbabweimbabweimbabweimbabweibiaimbabweimbabweimbabweimbabweanmarimbabweimbabweimbabweimbabwelehemimbabweimbabweimbabweimbabwebeltimbabweimbabweimbabweimbabweimbimbabweimbabweimbabweimbabweibandimbabweimbabweimbabweimbabwechevroletimbabweimbabweimbabweimbabweillacimbabweimbabweimbabweimbabwebiltimbabweimbabweimbabweimbabwebourgimbabweimbabwe\n",
      "r1: tensor(-9.4365e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8614, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.9226, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4936, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4460,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4460, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4460, grad_fn=<UnbindBackward>)\n",
      "Episode 187: 0.12231594870038527\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checkingimbabweimbabweimbabweimbabweimbabweorkshireimbabweimbabweimbabweimbabwe Zimbabweimbabweimbabweimbabweimbabwewanaimbabweimbabweimbabweimbabweabweimbabweimbabweimbabweimbabweminghamimbabweimbabweimbabweimbabweierrezimbabweimbabweimbabweimbabwehabiimbabweimbabweimbabweimbabweibiaimbabweimbabweimbabweimbabweanmarimbabweimbabweimbabweimbabwelehemimbabweimbabweimbabweimbabwebeltimbabweimbabweimbabweimbabweimbimbabweimbabweimbabweimbabweibandimbabweimbabweimbabweimbabwechevroletimbabweimbabweimbabweimbabweillacimbabweimbabweimbabweimbabwebiltimbabweimbabweimbabweimbabwebourgimbabweimbabwe\n",
      "r1: tensor(-8.1101e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8834, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8836, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5816, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4417,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4417, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4417, grad_fn=<UnbindBackward>)\n",
      "Episode 188: 0.12263891655298212\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checkingimbabweimbabweimbabweimbabweimbabweorkshireimbabweimbabweimbabweimbabwe Zimbabweimbabweimbabweimbabweimbabwewanaimbabweimbabweimbabweimbabweabweimbabweimbabweimbabweimbabweminghamimbabweimbabweimbabweimbabweierrezimbabweimbabweimbabweimbabwehabiimbabweimbabweimbabweimbabweibiaimbabweimbabweimbabweimbabweanmarimbabweimbabweimbabweimbabwelehemimbabweimbabweimbabweimbabwebeltimbabweimbabweimbabweimbabwebiltimbabweimbabweimbabweimbabweibandimbabweimbabweimbabweimbabwebourgimbabweimbabweimbabweimbabwebardimbabweimbabweimbabweimbabwebachimbabweimbabweimbabweimbabwebahimbabweimbabweimbabwe\n",
      "r1: tensor(-8.2806e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5435, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.9034, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5211, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3617,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3617, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3617, grad_fn=<UnbindBackward>)\n",
      "Episode 189: 0.12288038962370142\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen NguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyengueguyenguyenguyenguyenginxguyenguyenguyenguyenUGEguyenguyenguyenguyenuggetsguyenguyenguyenguyenugsguyenguyenguyenguyenuggestguyenguyenguyenguyenorgetownguyenguyenguyenguyenmunitionguyenguyenguyenguyenrenheitguyenguyenguyenguyenperatureguyenguyenguyenguyenutoniumguyenguyenguyenguyenuclearguyenguyen\n",
      "r1: tensor(-8.2067e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7922, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8414, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.6678, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4084,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4084, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4084, grad_fn=<UnbindBackward>)\n",
      "Episode 190: 0.12316847006873981\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "User: Hello\n",
      "DialoGPT: guyenguyenguyenguyenguyen NguyenguyenguyenguyenguyenugenguyenguyenguyenguyenuguguyenguyenguyenguyenugguyenguyenguyenguyenugiguyenguyenguyenguyenugaguyenguyenguyenguyenugeguyenguyenguyenguyengueguyenguyenguyenguyenginxguyenguyenguyenguyenUGEguyenguyenguyenguyenuggetsguyenguyenguyenguyenugsguyenguyenguyenguyenuggestguyenguyenguyenguyenorgetownguyenguyenguyenguyenmunitionguyenguyenguyenguyenrenheitguyenguyenguyenguyenperatureguyenguyenguyenguyenutoniumguyenguyenguyenguyenuclearguyenguyen\n",
      "r1: tensor(-7.2097e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5820, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7954, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.7153, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3443,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3443, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3443, grad_fn=<UnbindBackward>)\n",
      "Episode 191: 0.12339140291489105\n",
      "User: Hello\n",
      "DialoGPT: VIDIADES76561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561cery765617656176561\n",
      "r1: tensor(-0.0001, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8797, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8349, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4286,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4286, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4286, grad_fn=<UnbindBackward>)\n",
      "Episode 192: 0.12369877271014683\n",
      "User: Hello\n",
      "DialoGPT: VIDIADES76561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561765617656176561cery765617656176561\n",
      "r1: tensor(-7.6620e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8727, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8604, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4333,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4333, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4333, grad_fn=<UnbindBackward>)\n",
      "Episode 193: 0.12401020056118353\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checkingimbabweimbabweimbabweimbabweimbabweorkshireimbabweimbabweimbabweimbabwe Zimbabweimbabweimbabweimbabweimbabwewanaimbabweimbabweimbabweimbabweabweimbabweimbabweimbabweimbabweminghamimbabweimbabweimbabweimbabweierrezimbabweimbabweimbabweimbabwehabiimbabweimbabweimbabweimbabweibiaimbabweimbabweimbabweimbabweanmarimbabweimbabweimbabweimbabwelehemimbabweimbabweimbabweimbabwebeltimbabweimbabweimbabweimbabweimbimbabweimbabweimbabweimbabweibandimbabweimbabweimbabweimbabwechevroletimbabweimbabweimbabweimbabweillacimbabweimbabweimbabweimbabwebiltimbabweimbabweimbabweimbabwebourgimbabweimbabwe\n",
      "r1: tensor(-8.2474e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8702, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8853, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4388,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4388, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4388, grad_fn=<UnbindBackward>)\n",
      "Episode 194: 0.12432660502358223\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluohaohaohaohaohaoluluoluluoluluoluluohoohaohaohaohaohoohaohaohaohoohoohoohoohoohaohoohoohoohaohaohoohoohaohoohaohoohoohaohaohaoaohoohoohoohooboobooboobooboohooboobooboohoohoobooboohooboohoobooboohoohoohooboohooboohoohooboohoohoohoohoboaohoohoohoohoaboobooboobooboboaohooboobooboboaoboobo\n",
      "r1: tensor(-8.8386e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.9183, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6490, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3210, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3918,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3918, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3918, grad_fn=<UnbindBackward>)\n",
      "Episode 195: 0.12459516300806724\n",
      "User: Hello\n",
      "DialoGPT: oluluoluluoluluoluluoluluohaohaohaohaohaoluluoluluoluluoluluohoohaohaohaohaohoohaohaohaohoohoohoohoohoohaohoohoohoohaohaohoohoohaohoohaohoohoohaohaohaoaohoohoohoohooboobooboobooboohooboobooboohoohoobooboohooboohoobooboohoohoohooboohooboohoohooboohoohoohoohoboaohoohoohoohoaboobooboobooboboaohooboobooboboaoboobo\n",
      "r1: tensor(-0.0002, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8800, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8150, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.3461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4237,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4237, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4237, grad_fn=<UnbindBackward>)\n",
      "Episode 196: 0.1248951668182244\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checkingimbabweimbabweimbabweimbabweimbabweorkshireimbabweimbabweimbabweimbabwe Zimbabweimbabweimbabweimbabweimbabwewanaimbabweimbabweimbabweimbabweabweimbabweimbabweimbabweimbabweminghamimbabweimbabweimbabweimbabweierrezimbabweimbabweimbabweimbabwehabiimbabweimbabweimbabweimbabweibiaimbabweimbabweimbabweimbabweanmarimbabweimbabweimbabweimbabwelehemimbabweimbabweimbabweimbabwebeltimbabweimbabweimbabweimbabweimbimbabweimbabweimbabweimbabweibandimbabweimbabweimbabweimbabwechevroletimbabweimbabweimbabweimbabweillacimbabweimbabweimbabweimbabwebiltimbabweimbabweimbabweimbabwebourgimbabweimbabwe\n",
      "r1: tensor(-8.2824e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8740, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8069, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.5231, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4202,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4202, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4202, grad_fn=<UnbindBackward>)\n",
      "Episode 197: 0.1251910607473279\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checkingimbabweimbabweimbabweimbabweimbabweorkshireimbabweimbabweimbabweimbabwe Zimbabweimbabweimbabweimbabweimbabwewanaimbabweimbabweimbabweimbabweabweimbabweimbabweimbabweimbabweminghamimbabweimbabweimbabweimbabweierrezimbabweimbabweimbabweimbabwehabiimbabweimbabweimbabweimbabweibiaimbabweimbabweimbabweimbabweanmarimbabweimbabweimbabweimbabwelehemimbabweimbabweimbabweimbabwebeltimbabweimbabweimbabweimbabweimbimbabweimbabweimbabweimbabweibandimbabweimbabweimbabweimbabwechevroletimbabweimbabweimbabweimbabweillacimbabweimbabweimbabweimbabwebiltimbabweimbabweimbabweimbabwebourgimbabweimbabwe\n",
      "r1: tensor(-7.3520e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.9001, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6336, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4957, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.3834,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.3834, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.3834, grad_fn=<UnbindBackward>)\n",
      "Episode 198: 0.12544952397066\n",
      "User: Hello\n",
      "DialoGPT: orkshireman checking in checking inorkshireman checkingimbabweimbabweimbabweimbabweimbabweorkshireimbabweimbabweimbabweimbabwe Zimbabweimbabweimbabweimbabweimbabwewanaimbabweimbabweimbabweimbabweabweimbabweimbabweimbabweimbabweminghamimbabweimbabweimbabweimbabweierrezimbabweimbabweimbabweimbabwehabiimbabweimbabweimbabweimbabweibiaimbabweimbabweimbabweimbabweanmarimbabweimbabweimbabweimbabwelehemimbabweimbabweimbabweimbabwebeltimbabweimbabweimbabweimbabweimbimbabweimbabweimbabweimbabweibandimbabweimbabweimbabweimbabwechevroletimbabweimbabweimbabweimbabweillacimbabweimbabweimbabweimbabwebiltimbabweimbabweimbabweimbabwebourgimbabweimbabwe\n",
      "r1: tensor(-6.9130e-05, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8795, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8994, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(0.4828, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-0.4447,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-0.4447, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-0.4447, grad_fn=<UnbindBackward>)\n",
      "Episode 199: 0.12576877248240634\n",
      "imbabweimbimbabweimbabweimbabweimbabweibandimbabweimbabweimbabweimbabwechevroletimbabweimbabweimbabweimbabweillacimbabweimbabweimbabweimbabwebiltimbabweimbabweimbabweimbabwebourgimbabweimbabwe\n"
     ]
    }
   ],
   "source": [
    "chat_history_ids = tokenizer.encode(tokenizer.bos_token + \"Hello\" + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "reward = 0\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00025)\n",
    "# replay_buffer = ReplayBuffer(10000)\n",
    "batch_size = 64\n",
    "gamma      = 0.99  # 0.99\n",
    "# ep_rewards = []\n",
    "\n",
    "for episode in range(200):\n",
    "    chat_history_ids = tokenizer.encode(\"Hello\" + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "    questions = []\n",
    "    answers   = []\n",
    "    turns     = []\n",
    "    rewards   = []\n",
    "    model.eval()\n",
    "    max_length = 1000\n",
    "    for frame in range(20):\n",
    "        epsilon = epsilon_by_frame(frame)\n",
    "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    #     new_user_input_ids = tokenizer.encode(tokenizer.eos_token, return_tensors='pt')\n",
    "        input_ids = chat_history_ids[-(max_length-100):].to(device)\n",
    "        chat_history_ids = user.generate(input_ids, max_length=100,#max_length, \n",
    "                                         pad_token_id=tokenizer.eos_token_id, \n",
    "#                                          repetition_penalty=1.75,\n",
    "#                                          do_sample=True,\n",
    "#                                          temperature=0.98,\n",
    "#                                          top_k=0,\n",
    "#                                          num_beams=3,\n",
    "    # #                              num_return_sequences=1,\n",
    "    #                              early_stopping=True,\n",
    "                                         no_repeat_ngram_size=5\n",
    "                                ) if frame > 0 else input_ids\n",
    "        question = chat_history_ids[:, input_ids.shape[-1]:][0] if frame > 0 else input_ids[0]\n",
    "        questions.append(question.to(device))\n",
    "        turns.append(question)\n",
    "        print(\"User: {}\".format(decode(question)))\n",
    "\n",
    "        # append the new user input tokens to the chat history\n",
    "        input_ids = chat_history_ids[-(max_length-100):].to(device) # if step > 0 else new_user_input_ids\n",
    "        # generated a response while limiting the total chat history to 1000 tokens, \n",
    "        chat_history_ids = model.generate(input_ids, \n",
    "                                          pad_token_id=tokenizer.eos_token_id,\n",
    "                                          max_length=100,#max_length, \n",
    "#                                           repetition_penalty=1.25,\n",
    "#                                           min_length=2,\n",
    "#                                           do_sample=True,\n",
    "#                                           temperature=0.99,\n",
    "#                                           top_k=40,\n",
    "#                                           num_beams=3,\n",
    "#                                           early_stopping=True,\n",
    "    #                                       num_return_sequences=3,\n",
    "                                          no_repeat_ngram_size=5\n",
    "                                         )\n",
    "\n",
    "        # pretty print last output tokens from bot\n",
    "        answer = chat_history_ids[:, input_ids.shape[-1]:][0]\n",
    "        answers.append(answer)\n",
    "        turns.append(answer)\n",
    "        print(\"DialoGPT: {}\".format(decode(answer)))\n",
    "        \n",
    "#         if len(question) == 0: questions[-1] = torch.tensor([tokenizer.eos_token_id])\n",
    "#         if len(answer) == 0: answers[-1] = torch.tensor([tokenizer.eos_token_id])\n",
    "        \n",
    "        if is_dummy_sentence(answer) or len(answer) == 0:\n",
    "            print('dummy')\n",
    "            print(len(answer) > 0, len(answer))\n",
    "            \n",
    "            reward = compute_reward(questions, answers) if len(answer) > 0 else torch.tensor(0.0)\n",
    "#             reward += torch.tensor(0.0 - 1*len(answers), requires_grad=True)\n",
    "            rewards.append(reward)\n",
    "            break\n",
    "        else:\n",
    "            reward = compute_reward(questions, answers)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            \n",
    "    # Train\n",
    "    model.train()\n",
    "    r = discount_rewards(rewards)\n",
    "    print(r)\n",
    "    print(rewards)\n",
    "#     model = model.cuda()\n",
    "    loss = rewards[0]#.to('cuda')\n",
    "    \n",
    "#     loss = r.mean()\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "    if loss.item() != np.NINF or True:\n",
    "        ep_rewards.append(rewards[0].item())\n",
    "        for l in r[:1]:\n",
    "            if l.item() != np.NINF and l.item() != np.nan:\n",
    "                print('----- Loss:', l)\n",
    "                optimizer.zero_grad()\n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "#         if loss.item() != np.NINF\n",
    "#             loss.backward()\n",
    "#                 for param in model.parameters():\n",
    "#                         param.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "#                 optimizer.step()\n",
    "\n",
    "#         optimizer.step()\n",
    "    print(f'Episode {episode}:', -np.mean(ep_rewards))\n",
    "    \n",
    "    # Limit chat_history_ids to 50 tokens\n",
    "    chat_history_ids = chat_history_ids[:,-30:]\n",
    "    \n",
    "    if episode%10 == 0:\n",
    "        print('----------------------------------------')\n",
    "#         print_test_dialogue()\n",
    "        print('----------------------------------------')\n",
    "    \n",
    "#         torch.save(model.state_dict(), f'models/checkpoint_{episode}')\n",
    "    \n",
    "#     model = model.cpu()\n",
    "#     reward -= jaccard_similarity(answer.numpy(), context.numpy())\n",
    "    \n",
    "#     state = torch.cat([question, answer_ids], dim=-1)  # add separation token?\n",
    "#     action = act(model, state, epsilon)\n",
    "    # next_state, reward, done, _ = next_step(action)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(reward)\n",
    "#     print(answer)\n",
    "#     print(context)\n",
    "#     print(chat_history_ids)\n",
    "print(tokenizer.decode(chat_history_ids[0], skip_special_tokens=True))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(-1*np.array(ep_rewards), label='Mean episode reward')\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episodes')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('1000----rewards.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model_trained.load_state_dict(torch.load('models/checkpoint_60'))\n",
    "model_trained.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the trace\n",
    "# traced_model = torch.jit.trace(model)\n",
    "# torch.jit.save(traced_model, \"traced_bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Did you see Titanic?\n",
      "DialoGPT: I did, but I didn't think it was that good.\n",
      "User: I saw it twelve times.\n",
      "DialoGPT: I saw it once and I thought it was pretty bad.\n",
      "User: I have the DVD.\n",
      "DialoGPT: I have the Blu ray.\n",
      "User: Let's go to your home.\n",
      "DialoGPT: You're not my supervisor!\n",
      "User: And then we can go to my home.\n",
      "DialoGPT: No, no, no, no...\n",
      "User: I always cry at the end.\n",
      "DialoGPT: It was a good movie.\n"
     ]
    }
   ],
   "source": [
    "def print_test_dialogue():\n",
    "    model.eval()\n",
    "    # Let's chat for 5 lines\n",
    "    sentences = [\"Did you see Titanic?\",\n",
    "                \"I saw it twelve times.\",\n",
    "                \"I have the DVD.\",\n",
    "                \"Let's go to your home.\",\n",
    "                \"And then we can go to my home.\",\n",
    "                \"I always cry at the end.\"]\n",
    "    for step in range(len(sentences)):\n",
    "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    #     new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "        new_user_input_ids = tokenizer.encode(sentences[step] + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "        print(\"User:\", sentences[step])\n",
    "\n",
    "        # append the new user input tokens to the chat history\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "        # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    #     model.eval()\n",
    "        chat_history_ids = model.generate(bot_input_ids, max_length=500, \n",
    "                                          pad_token_id=tokenizer.eos_token_id, \n",
    "                                          no_repeat_ngram_size=5,\n",
    "                                          repetition_penalty=1.15,\n",
    "                                         )\n",
    "\n",
    "\n",
    "        # pretty print last ouput tokens from bot\n",
    "        print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "        chat_history_ids = chat_history_ids[:,-50:]\n",
    "\n",
    "print_test_dialogue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  262, 12490,    13, 50256,  5756,   338,   467,   284,   534,  1363,\n",
       "            13, 50256,  1870,   788,   356,   460,   467,   284,   616,  1363,\n",
       "            13, 50256,    40,  1464,  3960,   379,   262,   886,    13, 50256]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Hi! What is your name?Hi there! Mine is Alexei Alexandre Alexandrovitchovitchovitchovitchovitchovitchovitchovitchovitchovitchovitchovitchovitchovitchovitchov'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = tokenizer.encode('Hello Hi! What is your name?' + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "output = tokenizer.decode(model.generate(input_test, max_length=50, pad_token_id=tokenizer.eos_token_id,)[0], skip_special_tokens=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17.1766, 14.1621], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rews = discount_rewards(rewards)\n",
    "rews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "rewards[0].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-222-590a0fe59134>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrews\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "for r in rews:\n",
    "    r.backward()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "logits = model(v1)[0]\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "prev_input = torch.multinomial(probs, num_samples=1)\n",
    "decode(prev_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello!Hello! :DHow are you?I'm good! How are you?Pretty good!That's good!Can you recommend me a movie?I can!Tell meI will!\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(chat_history_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 15496, 50256,  ..., 50256, 50256, 50256],\n",
       "        [50256, 15496, 50256,  ..., 50256, 50256, 50256],\n",
       "        [50256, 15496, 50256,  ..., 50256, 50256, 50256],\n",
       "        ...,\n",
       "        [50256, 15496, 50256,  ..., 50256, 50256, 50256],\n",
       "        [50256, 15496, 50256,  ..., 50256, 50256, 50256],\n",
       "        [50256, 15496, 50256,  ...,   393,  1223, 50256]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ -1.0835, -18.4458, -17.6662,  ..., -14.8533, -13.5491,   0.9567],\n",
       "          [ -6.1334, -16.7606, -15.5976,  ..., -10.8078, -10.3935,   1.5186],\n",
       "          [  6.9852, -10.1889,  -8.7772,  ...,  -6.3700,  -5.0316,   5.8794],\n",
       "          ...,\n",
       "          [  5.4960, -10.9813,  -9.6111,  ...,  -6.2588,  -3.6435,   8.3031],\n",
       "          [ -2.2493, -12.7885, -12.5046,  ...,  -8.5337,  -5.7225,  11.4771],\n",
       "          [  5.3813, -11.0729,  -9.6908,  ...,  -6.2201,  -3.6808,   8.4822]]],\n",
       "        grad_fn=<UnsafeViewBackward>),\n",
       " (tensor([[[[[ 3.5647e-01,  2.9394e-02,  1.2568e-01,  ..., -5.8864e-01,\n",
       "              -1.5299e-02, -1.5551e-01],\n",
       "             [-2.4501e-01, -1.0144e-01,  9.3549e-02,  ..., -3.0358e-01,\n",
       "              -1.4868e-01,  2.2968e-02],\n",
       "             [-4.9309e-01, -4.7411e-02,  1.2059e-01,  ..., -2.4241e-01,\n",
       "              -3.4761e-01, -3.3903e-01],\n",
       "             ...,\n",
       "             [-8.7066e-01, -2.2038e-01, -2.6833e-01,  ..., -1.0996e-01,\n",
       "              -7.4337e-01, -1.9390e-01],\n",
       "             [-6.2056e-01, -3.1638e-01, -1.9239e-01,  ...,  4.4419e-03,\n",
       "              -4.6287e-01,  1.8755e-01],\n",
       "             [-8.7530e-01, -2.0415e-01, -2.5814e-01,  ..., -1.4883e-01,\n",
       "              -7.3084e-01, -2.2438e-01]],\n",
       "  \n",
       "            [[-6.1324e-01, -6.9942e-01, -1.6361e+00,  ...,  7.0650e-03,\n",
       "               3.5510e-01,  1.7768e-02],\n",
       "             [ 3.2067e-01, -1.6376e-01,  1.1313e+00,  ...,  1.0370e-01,\n",
       "              -1.0782e-01,  5.0387e-01],\n",
       "             [-5.4835e-01, -7.1475e-01, -1.2811e+00,  ..., -2.1157e-01,\n",
       "               3.4687e-01,  1.4071e+00],\n",
       "             ...,\n",
       "             [-6.0077e-01, -8.4454e-01, -1.4336e+00,  ..., -7.2469e-02,\n",
       "               5.2511e-01,  1.4458e+00],\n",
       "             [ 4.1453e-01, -2.4197e-01,  1.4339e+00,  ...,  2.2245e-01,\n",
       "              -1.6637e-01,  4.9240e-01],\n",
       "             [-6.4262e-01, -9.0989e-01, -1.4511e+00,  ..., -2.2200e-02,\n",
       "               5.2657e-01,  1.4381e+00]],\n",
       "  \n",
       "            [[-2.7557e-01, -2.0155e-01, -6.3953e-01,  ...,  1.0910e+00,\n",
       "              -4.3234e-03, -1.7263e-01],\n",
       "             [ 3.1270e-01,  1.7569e-01, -1.0122e-01,  ..., -3.8638e-01,\n",
       "               1.8320e-01,  9.4447e-02],\n",
       "             [ 7.5713e-01,  2.5464e-01, -1.0865e+00,  ...,  1.0854e+00,\n",
       "              -5.6328e-01,  3.9266e-01],\n",
       "             ...,\n",
       "             [ 4.8354e-01,  2.1161e-01, -1.0193e+00,  ...,  1.2306e+00,\n",
       "              -7.2439e-01,  3.9395e-01],\n",
       "             [ 3.9067e-02,  9.4769e-02,  1.1326e-01,  ..., -6.3839e-01,\n",
       "               1.7478e-01,  1.9321e-01],\n",
       "             [ 4.9378e-01,  2.1582e-01, -1.0807e+00,  ...,  1.2124e+00,\n",
       "              -7.0893e-01,  3.7200e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-2.7213e-01, -2.0335e-01, -1.8826e-02,  ..., -3.3091e-01,\n",
       "               3.3576e-01, -4.3095e-01],\n",
       "             [ 3.6834e-01,  1.4400e-01, -2.7347e-02,  ...,  4.5087e-01,\n",
       "              -2.3431e-01,  1.3409e-01],\n",
       "             [ 1.7375e-01,  2.0847e-01,  2.7629e-01,  ..., -7.6812e-01,\n",
       "               2.8449e-01,  3.5646e-01],\n",
       "             ...,\n",
       "             [ 3.7828e-01,  3.1220e-01,  1.9310e-01,  ..., -1.3968e+00,\n",
       "               5.8986e-01, -2.2694e-01],\n",
       "             [ 3.8820e-01,  8.5326e-02, -1.0958e-01,  ...,  5.6934e-02,\n",
       "              -7.2865e-02, -3.3456e-01],\n",
       "             [ 3.5121e-01,  2.5736e-01,  1.9702e-01,  ..., -1.3461e+00,\n",
       "               5.8669e-01, -2.5786e-01]],\n",
       "  \n",
       "            [[ 3.4766e-01, -3.1442e-01, -2.2133e-01,  ..., -1.0254e+00,\n",
       "              -8.3510e-01,  2.3008e-01],\n",
       "             [ 2.1702e-01, -4.7279e-01, -4.1508e-01,  ...,  7.1609e-01,\n",
       "              -4.1796e-02,  8.3373e-02],\n",
       "             [ 2.9578e-01, -1.3560e-01, -5.1633e-01,  ..., -4.2625e-01,\n",
       "              -1.1206e+00,  4.5758e-01],\n",
       "             ...,\n",
       "             [ 4.5645e-01, -5.9489e-02, -3.3366e-01,  ..., -6.8987e-01,\n",
       "              -1.5137e+00,  2.3145e-01],\n",
       "             [ 1.6369e-01, -3.9806e-01, -5.1567e-01,  ...,  8.8409e-01,\n",
       "              -2.5165e-02,  8.8157e-03],\n",
       "             [ 4.7713e-01, -6.7030e-02, -3.6515e-01,  ..., -6.5550e-01,\n",
       "              -1.5063e+00,  2.5451e-01]],\n",
       "  \n",
       "            [[ 2.9870e-01,  2.9359e-01, -1.7060e-02,  ...,  9.1998e-01,\n",
       "              -1.2900e+00, -4.4113e-01],\n",
       "             [ 1.3649e-02, -3.3173e-01,  1.1074e-01,  ..., -3.2771e-01,\n",
       "               5.3511e-01,  1.1888e-01],\n",
       "             [ 8.2113e-02,  3.5760e-01,  5.7908e-01,  ...,  1.3913e+00,\n",
       "              -5.6062e-01, -6.8358e-01],\n",
       "             ...,\n",
       "             [ 1.9669e-01,  5.1821e-01,  5.1821e-01,  ...,  1.4756e+00,\n",
       "              -1.0319e+00, -8.4158e-01],\n",
       "             [ 7.3062e-02, -4.5276e-01,  6.6683e-02,  ..., -6.3871e-01,\n",
       "               3.9133e-01,  2.6854e-01],\n",
       "             [ 2.1427e-01,  5.1307e-01,  5.4620e-01,  ...,  1.5034e+00,\n",
       "              -1.0510e+00, -8.5909e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-1.5427e-02,  1.4064e-02, -1.4428e-02,  ..., -2.6810e-02,\n",
       "              -1.0428e-02, -7.6356e-03],\n",
       "             [-3.7155e-02, -5.5862e-02, -3.3441e-02,  ...,  4.6452e-02,\n",
       "              -6.9445e-03,  7.6375e-03],\n",
       "             [-2.7414e-02,  2.5784e-02,  4.0973e-03,  ..., -4.4848e-02,\n",
       "              -4.5324e-02, -5.4945e-03],\n",
       "             ...,\n",
       "             [-3.0233e-02,  1.5706e-02, -1.8014e-02,  ..., -6.0905e-02,\n",
       "              -4.2522e-02,  1.8541e-03],\n",
       "             [-7.4506e-02, -7.2123e-02, -7.0580e-02,  ...,  7.2690e-02,\n",
       "               3.5773e-02,  1.4194e-02],\n",
       "             [-3.5731e-02,  2.1805e-02, -1.6192e-02,  ..., -6.3145e-02,\n",
       "              -3.5476e-02,  3.1879e-03]],\n",
       "  \n",
       "            [[ 8.3074e-03, -2.8373e-02, -2.6207e-02,  ..., -1.1710e-02,\n",
       "               1.6359e-02, -7.5925e-04],\n",
       "             [ 5.2416e-03,  3.3944e-02,  3.5075e-02,  ..., -4.5050e-03,\n",
       "              -2.3753e-03, -2.3858e-02],\n",
       "             [ 1.3297e-02, -2.9829e-02,  2.1122e-02,  ...,  1.2053e-03,\n",
       "               9.6025e-03,  4.8546e-03],\n",
       "             ...,\n",
       "             [ 2.4855e-02, -4.8428e-02,  7.3654e-03,  ..., -8.4170e-03,\n",
       "               1.0500e-02,  9.3064e-03],\n",
       "             [-6.5294e-04,  3.7523e-02,  4.9824e-03,  ..., -3.2325e-02,\n",
       "              -3.0355e-03, -3.4064e-02],\n",
       "             [ 2.3316e-02, -5.0718e-02,  1.1400e-03,  ..., -4.6231e-03,\n",
       "               1.1423e-02, -1.5631e-03]],\n",
       "  \n",
       "            [[ 7.5115e-03, -7.2156e-03, -2.1719e-03,  ...,  5.1252e-03,\n",
       "               1.3564e-02, -1.3940e-02],\n",
       "             [-1.2027e-02,  1.9729e-02,  2.6076e-03,  ..., -5.5551e-03,\n",
       "              -4.5144e-02,  3.6382e-02],\n",
       "             [ 7.1255e-03, -1.3193e-02,  4.3888e-03,  ...,  1.3248e-02,\n",
       "              -1.0062e-02, -9.6777e-03],\n",
       "             ...,\n",
       "             [ 1.3177e-02, -9.6703e-03,  6.9762e-03,  ...,  1.6377e-02,\n",
       "               3.3508e-03, -1.3634e-02],\n",
       "             [-2.9910e-02,  1.3213e-02,  1.1513e-02,  ..., -1.1051e-02,\n",
       "              -5.7365e-02,  4.4392e-02],\n",
       "             [ 9.6987e-03, -5.6534e-03, -5.4152e-03,  ...,  1.1793e-02,\n",
       "               7.7697e-03, -1.3555e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 2.8884e-03, -3.1777e-04,  1.2526e-02,  ...,  1.0357e-01,\n",
       "              -1.3195e-03,  2.7323e-02],\n",
       "             [ 1.4252e-02, -8.5170e-03,  1.0223e-02,  ..., -1.5446e-02,\n",
       "              -2.5261e-02, -2.9132e-02],\n",
       "             [ 6.4918e-04,  2.2602e-02,  2.3441e-02,  ...,  2.5597e-01,\n",
       "              -7.0115e-03,  2.0920e-02],\n",
       "             ...,\n",
       "             [ 5.2084e-03,  8.6208e-03,  2.6251e-02,  ...,  1.5231e-01,\n",
       "              -2.1594e-02,  1.9266e-02],\n",
       "             [ 1.1481e-02, -3.9369e-02,  3.1823e-02,  ..., -5.2330e-02,\n",
       "              -8.8497e-03, -3.6166e-02],\n",
       "             [ 3.0384e-03,  9.7844e-03,  1.9395e-02,  ...,  1.6410e-01,\n",
       "              -2.1198e-02,  2.2550e-02]],\n",
       "  \n",
       "            [[-3.8660e-02, -1.9084e-02, -4.5845e-03,  ..., -4.2062e-03,\n",
       "               1.2585e-02,  4.7612e-03],\n",
       "             [ 6.1396e-02,  4.5043e-02, -1.5445e-02,  ...,  3.3933e-02,\n",
       "              -2.8724e-02, -1.4793e-02],\n",
       "             [ 9.2317e-03, -1.5910e-02,  5.7461e-03,  ...,  4.7116e-02,\n",
       "              -6.7413e-03,  3.8151e-02],\n",
       "             ...,\n",
       "             [-6.6746e-03, -5.7703e-03,  4.2490e-03,  ...,  2.6396e-02,\n",
       "              -9.8569e-03,  3.3953e-02],\n",
       "             [ 5.5540e-02,  2.8859e-02, -2.8222e-02,  ...,  3.0147e-02,\n",
       "              -3.4354e-02, -2.3551e-02],\n",
       "             [-6.1915e-03, -1.4374e-02,  7.6163e-03,  ...,  2.7397e-02,\n",
       "              -1.7698e-02,  3.3610e-02]],\n",
       "  \n",
       "            [[ 1.0614e-01, -6.2515e-03,  8.8402e-03,  ...,  1.5647e-03,\n",
       "              -7.9393e-03, -3.1664e-04],\n",
       "             [-1.0708e-01, -3.1211e-03,  3.6196e-03,  ..., -7.2911e-03,\n",
       "               1.2847e-02, -4.6337e-02],\n",
       "             [ 1.8343e-01, -9.3105e-03,  1.3973e-02,  ..., -2.7889e-02,\n",
       "              -1.3634e-02,  1.4880e-03],\n",
       "             ...,\n",
       "             [ 1.6527e-01, -8.2843e-03,  5.8837e-03,  ..., -1.3642e-02,\n",
       "              -1.5869e-02,  1.7887e-03],\n",
       "             [-1.3140e-01, -1.1849e-02,  2.6250e-03,  ..., -5.2022e-03,\n",
       "               2.2451e-02, -6.0746e-02],\n",
       "             [ 1.5053e-01, -8.6654e-03,  6.2439e-03,  ..., -1.6795e-02,\n",
       "              -2.0212e-02,  5.8278e-03]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 1.3732e-03, -3.1152e-02, -2.1452e-01,  ..., -2.7631e-01,\n",
       "              -1.2909e+00,  7.3533e-01],\n",
       "             [ 5.5356e-01,  5.9802e-02,  7.7254e-01,  ...,  3.8367e-01,\n",
       "               6.7550e-01, -4.6682e-01],\n",
       "             [ 1.0180e-01,  6.6716e-02, -1.2580e-01,  ..., -5.1118e-01,\n",
       "              -6.9886e-01,  9.8184e-01],\n",
       "             ...,\n",
       "             [ 2.1958e-01,  1.2745e-01, -3.7242e-01,  ..., -5.6320e-01,\n",
       "              -7.5294e-01,  1.1791e+00],\n",
       "             [ 8.4037e-01, -3.2573e-02,  4.0821e-01,  ...,  2.3323e-01,\n",
       "               1.2273e-01,  4.4319e-02],\n",
       "             [ 2.1496e-01,  1.1876e-01, -4.4507e-01,  ..., -5.5489e-01,\n",
       "              -8.4894e-01,  1.1979e+00]],\n",
       "  \n",
       "            [[-1.1132e+00, -1.2448e-01,  1.0983e+00,  ..., -7.3489e-03,\n",
       "               6.9657e-02, -1.6341e-01],\n",
       "             [-2.6266e-01, -3.5057e-01,  1.0390e+00,  ...,  1.2706e+00,\n",
       "               1.6491e-02, -8.8436e-01],\n",
       "             [-1.7826e-01, -2.5279e-01,  2.9990e-01,  ...,  4.5925e-01,\n",
       "               6.1034e-01, -1.4961e-01],\n",
       "             ...,\n",
       "             [-4.8965e-02, -6.2697e-01, -1.8539e+00,  ..., -2.2442e-01,\n",
       "              -1.4169e-01,  1.9390e-01],\n",
       "             [-1.7084e-01, -6.4219e-01, -1.0467e+00,  ...,  4.3814e-01,\n",
       "              -6.7040e-01, -3.0298e-01],\n",
       "             [-4.8870e-02, -6.4088e-01, -2.0001e+00,  ..., -2.8450e-01,\n",
       "              -2.0127e-01,  2.0114e-01]],\n",
       "  \n",
       "            [[ 1.3171e+00,  6.7066e-01,  1.0201e+00,  ...,  1.4822e+00,\n",
       "              -6.5354e-04, -1.2782e+00],\n",
       "             [-4.9257e-01,  7.0507e-01,  2.8585e-01,  ..., -1.0026e+00,\n",
       "               3.3309e-01,  1.0703e+00],\n",
       "             [ 8.1321e-01, -1.3244e-01,  2.2641e-01,  ..., -2.8384e-01,\n",
       "               1.8885e-01, -1.3899e+00],\n",
       "             ...,\n",
       "             [-1.6844e-01, -4.6495e-01, -3.3195e-01,  ...,  3.2054e-01,\n",
       "               1.3388e+00, -2.3499e+00],\n",
       "             [-9.5213e-01,  4.4838e-01, -2.2329e-01,  ...,  1.0145e-01,\n",
       "               1.4386e+00, -5.5737e-01],\n",
       "             [-2.6512e-01, -4.4866e-01, -3.9866e-01,  ...,  3.9018e-01,\n",
       "               1.4138e+00, -2.3916e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-1.9582e-01,  9.0748e-02,  4.6948e-01,  ...,  2.0914e-01,\n",
       "              -1.9690e-01,  1.7281e-01],\n",
       "             [ 3.9708e-01,  4.9278e-01,  3.4358e-01,  ..., -5.7817e-01,\n",
       "              -1.1119e+00, -3.5681e-01],\n",
       "             [-5.3029e-01,  1.0232e+00,  2.6085e-01,  ...,  6.2739e-02,\n",
       "              -4.8667e-01, -5.2726e-01],\n",
       "             ...,\n",
       "             [ 4.9742e-02,  1.6388e-01,  5.5877e-01,  ...,  1.0970e+00,\n",
       "               3.0226e-01, -4.3555e-01],\n",
       "             [ 7.5735e-01, -5.1173e-01,  2.7250e-01,  ...,  1.0058e+00,\n",
       "               1.2645e-02, -4.5818e-01],\n",
       "             [ 1.0384e-01,  8.5821e-02,  6.2418e-01,  ...,  1.1076e+00,\n",
       "               3.4843e-01, -4.3180e-01]],\n",
       "  \n",
       "            [[-2.1836e-02, -1.6478e-02,  3.3250e-01,  ...,  4.6523e-01,\n",
       "              -3.0216e-01, -5.4917e-01],\n",
       "             [-7.5696e-01, -9.0302e-01,  3.9400e-02,  ...,  4.5685e-02,\n",
       "              -1.3434e-01, -1.3074e+00],\n",
       "             [ 4.3539e-02,  4.6468e-01,  1.2926e-02,  ..., -7.1159e-01,\n",
       "              -4.3663e-01, -2.5163e-01],\n",
       "             ...,\n",
       "             [ 1.8335e+00,  1.3168e+00,  1.2520e+00,  ..., -1.0992e+00,\n",
       "              -1.6982e+00, -2.4647e+00],\n",
       "             [ 1.6050e+00,  2.8709e-01,  7.1577e-01,  ..., -6.1914e-01,\n",
       "              -1.6414e+00, -2.9707e+00],\n",
       "             [ 1.9613e+00,  1.3047e+00,  1.2215e+00,  ..., -9.9180e-01,\n",
       "              -1.9706e+00, -2.5699e+00]],\n",
       "  \n",
       "            [[ 2.2117e-01, -2.2352e-01, -2.7293e-01,  ...,  1.2287e-01,\n",
       "               1.7066e+00,  4.3141e-02],\n",
       "             [-3.4082e-01, -2.3486e-01, -7.1640e-02,  ..., -5.6265e-01,\n",
       "              -8.6947e-01, -1.7060e-01],\n",
       "             [-6.1464e-02,  1.2267e-01, -1.9044e-01,  ...,  1.3128e-01,\n",
       "               1.2613e-01,  1.2517e-02],\n",
       "             ...,\n",
       "             [ 7.2369e-02,  5.6791e-02, -2.0905e-01,  ..., -1.4716e-01,\n",
       "               3.8101e-01,  1.8978e-02],\n",
       "             [-7.8229e-02, -1.7727e-01, -2.1226e-01,  ..., -3.4204e-01,\n",
       "               4.7143e-02, -2.3802e-01],\n",
       "             [ 9.2552e-02,  6.0483e-02, -2.2751e-01,  ..., -1.6088e-01,\n",
       "               3.3967e-01,  1.0776e-02]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 1.1288e-01, -7.3756e-02,  4.1562e-01,  ...,  1.0891e-01,\n",
       "              -2.2000e-01,  2.7641e-01],\n",
       "             [ 8.4596e-02, -3.1537e-01, -3.6905e-01,  ..., -8.2744e-03,\n",
       "               1.9836e-02, -8.6060e-04],\n",
       "             [ 2.1927e-01,  1.0642e-01,  7.3121e-04,  ...,  6.8869e-02,\n",
       "              -1.7593e-01,  2.3871e-01],\n",
       "             ...,\n",
       "             [ 5.9715e-01,  5.4864e-01,  4.9133e-01,  ..., -3.1126e-01,\n",
       "              -3.1610e-01,  3.2728e-01],\n",
       "             [-7.5342e-02, -1.2938e-01,  4.5588e-01,  ..., -4.2842e-01,\n",
       "              -1.6973e-01,  1.6444e-01],\n",
       "             [ 5.6355e-01,  5.7384e-01,  5.4991e-01,  ..., -3.3279e-01,\n",
       "              -3.2332e-01,  3.0293e-01]],\n",
       "  \n",
       "            [[ 1.7196e-01, -1.1144e-01,  1.4478e-03,  ...,  4.6665e-03,\n",
       "               1.4802e-01, -1.1413e-01],\n",
       "             [-8.7649e-01, -7.2703e-02,  4.7985e-01,  ..., -3.3779e-01,\n",
       "               3.2150e-01,  2.1270e-01],\n",
       "             [ 4.3443e-01,  1.9312e-01,  1.4849e-01,  ...,  6.1159e-02,\n",
       "              -9.9687e-02, -2.0709e-01],\n",
       "             ...,\n",
       "             [ 3.7974e-01,  1.0961e+00,  2.9356e-01,  ..., -1.1159e-01,\n",
       "              -9.1351e-01, -1.2655e-01],\n",
       "             [-9.5393e-01,  1.1897e+00,  3.8126e-01,  ..., -5.4444e-01,\n",
       "              -5.6157e-01,  2.5204e-01],\n",
       "             [ 3.9494e-01,  1.2013e+00,  2.6462e-01,  ..., -1.0423e-01,\n",
       "              -9.5901e-01, -1.1632e-01]],\n",
       "  \n",
       "            [[ 8.0207e-02, -6.9696e-03, -2.6643e-01,  ..., -2.5784e-02,\n",
       "               2.9343e-01, -1.4394e-02],\n",
       "             [ 7.5280e-01,  4.0011e-02, -6.5069e-02,  ...,  8.8561e-01,\n",
       "              -5.1950e-01,  1.6396e-01],\n",
       "             [ 1.4365e-01, -2.5928e-01, -1.1437e-01,  ..., -1.7281e-02,\n",
       "               7.2640e-02,  1.6000e-02],\n",
       "             ...,\n",
       "             [ 2.9043e-01, -2.3284e-01,  3.1093e-01,  ...,  8.2343e-02,\n",
       "               7.9434e-02, -2.0749e-01],\n",
       "             [ 6.4281e-01,  2.2744e-01,  3.1879e-01,  ...,  7.3648e-01,\n",
       "              -3.9788e-01, -1.0599e-01],\n",
       "             [ 3.0557e-01, -2.6229e-01,  3.0262e-01,  ...,  1.0792e-01,\n",
       "               5.6345e-02, -2.1503e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.9911e-01,  1.4074e-01, -7.1719e-02,  ..., -1.5074e-01,\n",
       "               3.8447e-02, -3.9748e-01],\n",
       "             [ 9.7150e-02, -4.8043e-01, -1.1247e-01,  ...,  1.7712e-01,\n",
       "               1.8198e-01,  5.7814e-01],\n",
       "             [ 1.6426e-01,  7.7734e-02,  1.6053e-01,  ..., -2.7912e-01,\n",
       "               6.3470e-02, -3.0118e-01],\n",
       "             ...,\n",
       "             [ 1.9972e-01, -3.9443e-02, -8.0196e-02,  ..., -5.4356e-01,\n",
       "               7.7398e-02,  1.7048e-01],\n",
       "             [-1.6864e-02, -4.0871e-01, -2.1529e-02,  ..., -8.4738e-02,\n",
       "               2.1150e-01,  7.0404e-01],\n",
       "             [ 1.8359e-01, -5.5178e-02, -6.8768e-02,  ..., -5.4374e-01,\n",
       "               7.6699e-02,  1.9276e-01]],\n",
       "  \n",
       "            [[ 4.5376e-01, -9.5503e-02, -2.1041e-01,  ...,  3.5804e-01,\n",
       "              -1.3702e-01, -4.7131e-01],\n",
       "             [ 6.1372e-02, -2.5000e-01,  4.7806e-01,  ...,  1.5363e-01,\n",
       "              -3.8773e-01, -5.7048e-02],\n",
       "             [ 3.0118e-01, -6.9409e-02, -2.2144e-01,  ...,  2.6856e-01,\n",
       "              -1.4035e-01, -3.5713e-01],\n",
       "             ...,\n",
       "             [ 6.7400e-02, -2.8573e-01, -8.7119e-02,  ..., -9.8326e-02,\n",
       "              -9.4396e-02, -1.4109e-01],\n",
       "             [-7.3351e-02, -1.8733e-01,  4.8566e-01,  ...,  9.9767e-02,\n",
       "              -2.3151e-01,  2.6291e-01],\n",
       "             [ 8.1810e-02, -2.6022e-01, -5.2111e-02,  ..., -1.2480e-01,\n",
       "              -7.0141e-02, -1.4968e-01]],\n",
       "  \n",
       "            [[ 5.9382e-02, -3.5986e-01, -5.5193e-02,  ...,  2.0033e-01,\n",
       "              -1.6054e-01, -4.1664e-03],\n",
       "             [-2.6004e-01,  5.2948e-01,  3.0210e-01,  ...,  1.3766e-01,\n",
       "              -6.0209e-01, -2.2138e-01],\n",
       "             [-1.4559e-02, -7.8340e-02, -5.4997e-03,  ...,  5.8090e-02,\n",
       "              -3.5198e-01, -1.0430e-01],\n",
       "             ...,\n",
       "             [-3.6104e-02, -4.7305e-01,  1.0073e-02,  ...,  5.2401e-01,\n",
       "              -2.3104e+00,  1.7670e-01],\n",
       "             [-2.4976e-01,  5.3703e-02,  9.7487e-02,  ...,  4.0168e-01,\n",
       "              -2.7093e+00,  1.2379e-01],\n",
       "             [-3.3083e-02, -5.4230e-01,  3.8988e-03,  ...,  5.4442e-01,\n",
       "              -2.3365e+00,  2.0276e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 1.9054e-01, -1.0591e-01, -4.6353e-01,  ..., -9.0642e-01,\n",
       "               3.1063e-01,  5.7120e-01],\n",
       "             [-1.1171e-01, -3.0458e-01, -2.0842e-01,  ..., -5.6840e-01,\n",
       "              -1.2728e-01, -1.4252e-02],\n",
       "             [ 2.4394e-02, -1.2025e-01, -6.0032e-01,  ..., -2.2887e-01,\n",
       "              -8.0994e-02,  3.8038e-01],\n",
       "             ...,\n",
       "             [ 3.7199e-01, -1.4595e-01, -1.6711e+00,  ...,  6.6729e-01,\n",
       "              -1.8593e-01,  1.6283e-01],\n",
       "             [ 3.0604e-01, -3.8023e-01, -1.5408e+00,  ...,  6.8192e-01,\n",
       "               2.2499e-02,  9.7974e-02],\n",
       "             [ 3.7747e-01, -1.3802e-01, -1.7531e+00,  ...,  5.8435e-01,\n",
       "              -1.8760e-01,  1.3263e-01]],\n",
       "  \n",
       "            [[ 1.4559e-01, -1.5670e-01, -4.9808e-01,  ...,  2.3234e-01,\n",
       "              -5.5468e-01, -2.4019e-01],\n",
       "             [ 2.2048e-01, -1.8740e-01,  1.5918e-01,  ...,  3.0774e-01,\n",
       "              -1.0063e+00,  2.5774e-01],\n",
       "             [ 4.2587e-02, -1.8807e-01, -1.2804e-01,  ..., -4.9654e-02,\n",
       "               8.0500e-01, -2.3177e-01],\n",
       "             ...,\n",
       "             [ 5.5299e-02, -3.4687e-03,  9.7126e-02,  ..., -1.0889e-01,\n",
       "               1.6101e+00, -1.3396e-01],\n",
       "             [ 2.1951e-01,  1.0742e-01,  3.2421e-01,  ...,  2.6101e-01,\n",
       "               6.4505e-01,  9.2614e-02],\n",
       "             [ 4.8023e-02,  1.6621e-02,  1.0357e-01,  ..., -8.9844e-02,\n",
       "               1.5801e+00, -1.3042e-01]],\n",
       "  \n",
       "            [[-3.5510e-01, -1.4094e-01, -5.2819e-01,  ...,  4.5299e-01,\n",
       "              -6.5512e-01, -7.8750e-01],\n",
       "             [ 7.9614e-01, -1.6137e-02, -1.3484e+00,  ...,  1.0741e+00,\n",
       "               1.1378e+00, -6.0276e-01],\n",
       "             [ 2.4603e-02,  1.4563e-01, -2.1560e-01,  ..., -2.8729e-01,\n",
       "               2.2414e-01, -1.7944e-01],\n",
       "             ...,\n",
       "             [-7.0532e-01, -1.3299e+00,  7.0482e-02,  ..., -1.6577e+00,\n",
       "               1.0023e+00,  2.7428e+00],\n",
       "             [-2.6476e-01, -1.5248e+00, -8.9148e-01,  ..., -7.3778e-01,\n",
       "               1.8105e+00,  2.0857e+00],\n",
       "             [-8.7015e-01, -1.4125e+00,  5.2351e-02,  ..., -1.5908e+00,\n",
       "               9.7487e-01,  2.8582e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-6.4292e-01,  3.7202e-02, -2.9920e-01,  ...,  3.4284e-01,\n",
       "               2.2034e-01,  5.4896e-01],\n",
       "             [-2.6560e-01,  4.2992e-01, -2.9731e-01,  ...,  3.0372e-03,\n",
       "              -7.4193e-02,  4.2945e-01],\n",
       "             [-3.9789e-01, -9.7989e-02, -2.5294e-01,  ...,  4.2359e-01,\n",
       "               3.9511e-02,  2.8348e-01],\n",
       "             ...,\n",
       "             [ 1.4876e+00,  2.6357e-01,  8.4809e-01,  ..., -1.0977e+00,\n",
       "              -9.3810e-02, -1.5059e+00],\n",
       "             [ 1.6310e+00,  6.9003e-01,  9.0988e-01,  ..., -1.4293e+00,\n",
       "               1.4535e-02, -1.0727e+00],\n",
       "             [ 1.5956e+00,  3.1386e-01,  9.2419e-01,  ..., -1.1977e+00,\n",
       "              -7.1769e-02, -1.5956e+00]],\n",
       "  \n",
       "            [[ 4.7137e-01, -5.3984e-01, -3.4725e-01,  ..., -4.3685e-01,\n",
       "               8.5513e-01, -2.2945e-01],\n",
       "             [ 8.7712e-01, -1.9372e-02, -8.7122e-02,  ..., -1.5379e-01,\n",
       "              -7.8581e-02, -1.0645e+00],\n",
       "             [-4.9247e-01,  3.9556e-01,  1.6823e-01,  ..., -6.1454e-01,\n",
       "               2.9115e-01, -3.8398e-01],\n",
       "             ...,\n",
       "             [ 2.7144e-01,  1.8478e+00,  3.5846e-01,  ..., -3.3485e+00,\n",
       "               3.0667e-01,  8.5043e-01],\n",
       "             [ 1.0158e+00,  1.6955e+00,  5.2189e-01,  ..., -3.1719e+00,\n",
       "              -6.8707e-02,  4.8756e-01],\n",
       "             [ 3.8996e-01,  1.8734e+00,  3.2702e-01,  ..., -3.4005e+00,\n",
       "               3.0370e-01,  8.8831e-01]],\n",
       "  \n",
       "            [[ 2.6854e-01,  5.7007e-02,  2.7600e-01,  ...,  2.7626e-01,\n",
       "              -2.6608e-01,  8.5073e-02],\n",
       "             [ 1.3142e-01,  4.8065e-01, -1.3366e+00,  ...,  1.2941e+00,\n",
       "               7.9773e-01, -7.2365e-01],\n",
       "             [-4.3432e-01,  2.4347e-02, -1.3807e-01,  ...,  6.7102e-01,\n",
       "               4.0416e-01,  1.6151e-01],\n",
       "             ...,\n",
       "             [-4.5139e+00,  2.8216e-01, -1.2896e+00,  ...,  1.0030e+00,\n",
       "              -2.3359e-01,  2.3094e+00],\n",
       "             [-4.1916e+00,  4.3797e-01, -2.4180e+00,  ...,  8.3489e-01,\n",
       "              -1.9207e-01,  1.9308e+00],\n",
       "             [-4.6746e+00,  3.8080e-01, -1.2599e+00,  ...,  1.0072e+00,\n",
       "              -3.0726e-01,  2.6478e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-2.8984e-01, -9.5053e-01, -4.2101e-01,  ...,  2.6560e-01,\n",
       "              -4.4052e-02,  2.6516e-01],\n",
       "             [-1.4658e-01,  3.2608e-01, -6.0178e-01,  ...,  4.3249e-01,\n",
       "              -2.7029e-01, -6.7224e-02],\n",
       "             [-1.7531e-01, -1.1327e-01,  7.2336e-01,  ...,  4.1022e-02,\n",
       "              -9.7736e-02,  1.3922e-02],\n",
       "             ...,\n",
       "             [-1.4306e-01,  3.6944e-02,  2.5615e+00,  ..., -3.1224e-01,\n",
       "              -1.7184e-01, -3.8713e-01],\n",
       "             [-2.9149e-01,  1.1917e-01,  2.5551e+00,  ...,  4.2880e-02,\n",
       "              -1.7940e-01, -4.1340e-01],\n",
       "             [-1.2743e-01,  3.1615e-02,  2.5359e+00,  ..., -3.2899e-01,\n",
       "              -1.4861e-01, -4.1738e-01]],\n",
       "  \n",
       "            [[ 1.6713e-01,  2.3165e-01,  3.0481e-01,  ...,  1.8356e-01,\n",
       "              -1.8613e-01,  3.7517e-01],\n",
       "             [-4.1503e-02,  2.6801e-01, -9.8477e-03,  ...,  3.7615e-01,\n",
       "              -7.9453e-02,  1.3847e-01],\n",
       "             [-9.6029e-02, -6.3760e-02,  7.4975e-02,  ...,  1.1340e-01,\n",
       "              -2.1548e-02,  7.4171e-03],\n",
       "             ...,\n",
       "             [-2.3708e-02, -7.6344e-02, -2.0201e-01,  ...,  1.3697e-01,\n",
       "               4.9620e-02,  4.1098e-02],\n",
       "             [ 1.4721e-01,  2.4555e-01, -1.9471e-01,  ...,  3.7734e-01,\n",
       "               1.9247e-02,  9.8152e-02],\n",
       "             [-1.1669e-02, -8.5673e-02, -2.1044e-01,  ...,  1.3666e-01,\n",
       "               6.7921e-02,  4.0517e-02]],\n",
       "  \n",
       "            [[ 3.9783e-01, -2.7834e-01, -2.9279e-01,  ..., -2.1629e-01,\n",
       "              -4.5279e-01,  1.4681e-01],\n",
       "             [-6.7982e-01,  4.9688e-01,  4.3375e-01,  ..., -4.7558e-01,\n",
       "              -1.1380e-01,  6.4052e-01],\n",
       "             [ 3.2737e-01, -1.1600e+00, -2.9227e-02,  ..., -1.1916e-01,\n",
       "              -2.7516e-01,  1.2488e-02],\n",
       "             ...,\n",
       "             [ 2.4549e-01, -1.2891e+00, -5.1070e-02,  ...,  1.8678e-02,\n",
       "              -1.9584e-01, -3.1466e-01],\n",
       "             [-7.5442e-01,  5.1603e-01,  3.1397e-01,  ..., -5.4113e-01,\n",
       "               4.7700e-01, -4.0424e-01],\n",
       "             [ 2.4675e-01, -1.2790e+00, -6.7882e-02,  ...,  4.0275e-02,\n",
       "              -2.0008e-01, -2.8999e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-4.5116e-02,  1.7099e-01, -6.1685e-02,  ..., -7.5759e-02,\n",
       "               1.6197e-01, -6.9073e-02],\n",
       "             [ 1.5362e-01, -3.1335e-01, -1.8791e-01,  ...,  8.0334e-01,\n",
       "               2.9655e-01,  4.4419e-01],\n",
       "             [-2.9417e-02,  1.2834e-01, -7.8101e-02,  ..., -7.4429e-01,\n",
       "               2.5614e-03,  2.5955e-02],\n",
       "             ...,\n",
       "             [ 1.4462e-01, -3.7819e-02, -1.7253e-01,  ..., -2.3780e-01,\n",
       "               9.1400e-02,  1.2483e-01],\n",
       "             [ 8.9699e-02, -1.9563e-01, -1.7455e-01,  ...,  1.9868e-01,\n",
       "               7.7451e-02,  3.6947e-01],\n",
       "             [ 1.6891e-01, -5.0240e-02, -1.2973e-01,  ..., -2.1291e-01,\n",
       "               8.8717e-02,  1.2839e-01]],\n",
       "  \n",
       "            [[ 1.2768e-01,  7.3946e-02,  7.6204e-01,  ..., -1.0533e+00,\n",
       "               8.6982e-01, -3.7628e-01],\n",
       "             [-3.4573e-01, -6.5017e-01, -8.0730e-01,  ...,  3.5180e-01,\n",
       "               8.0292e-01,  3.6796e-01],\n",
       "             [ 1.0631e-01,  4.1224e-01,  8.8968e-01,  ..., -6.1037e-01,\n",
       "               6.9314e-01,  4.0451e-02],\n",
       "             ...,\n",
       "             [ 4.7435e-02,  1.4579e-01, -1.1626e-02,  ..., -6.3331e-01,\n",
       "               4.7573e-01,  1.0259e-02],\n",
       "             [-6.3796e-01, -3.6450e-01, -7.6878e-01,  ...,  3.8392e-01,\n",
       "               9.1132e-02,  5.9173e-01],\n",
       "             [ 5.5140e-02,  1.0602e-01, -7.1952e-02,  ..., -6.3976e-01,\n",
       "               4.5596e-01, -2.5325e-03]],\n",
       "  \n",
       "            [[-8.5043e-01, -2.4333e-01, -4.2240e-01,  ...,  2.6253e-01,\n",
       "              -1.2320e-01, -8.8310e-02],\n",
       "             [ 3.1755e-02,  3.1480e-01,  8.9462e-02,  ...,  1.5860e-01,\n",
       "               2.6110e-01, -9.5993e-01],\n",
       "             [-9.3306e-02,  7.9739e-03,  4.9194e-02,  ...,  1.3967e-01,\n",
       "               1.8511e-02, -6.3664e-02],\n",
       "             ...,\n",
       "             [ 2.5211e-01, -8.7274e-02,  2.3610e-01,  ...,  2.0613e-01,\n",
       "              -4.5992e-02, -5.6887e-02],\n",
       "             [-3.2330e-01,  3.4576e-01, -2.3071e-02,  ...,  2.5203e-01,\n",
       "               1.8238e-01, -2.1287e-01],\n",
       "             [ 2.5353e-01, -9.7441e-02,  2.4608e-01,  ...,  2.1098e-01,\n",
       "              -5.0427e-02, -5.1211e-02]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 2.8918e-01,  1.2644e+00,  3.2592e-01,  ..., -8.0289e-02,\n",
       "              -1.6458e+00,  2.1959e+00],\n",
       "             [ 9.1457e-01, -5.9631e-01, -1.5752e+00,  ...,  5.4293e-02,\n",
       "              -3.7180e-01, -3.6848e-01],\n",
       "             [-8.5443e-02,  4.9928e-01,  1.9010e-03,  ..., -1.3677e-01,\n",
       "              -1.5002e-01,  3.7018e-01],\n",
       "             ...,\n",
       "             [ 1.2450e-01,  3.2955e+00, -1.6791e-01,  ..., -3.3784e+00,\n",
       "              -9.2105e-01, -2.1060e-01],\n",
       "             [ 1.6267e+00,  2.7565e+00, -1.0007e+00,  ..., -3.6766e+00,\n",
       "              -5.2225e-01, -1.2959e+00],\n",
       "             [ 1.0742e-01,  3.3835e+00, -1.5965e-01,  ..., -3.4897e+00,\n",
       "              -1.0027e+00, -2.1723e-01]],\n",
       "  \n",
       "            [[ 7.0905e-01, -1.6613e+00, -4.2812e-01,  ..., -1.6996e+00,\n",
       "               6.3429e-01,  7.1628e-02],\n",
       "             [ 1.4047e+00,  8.8538e-01, -9.8983e-02,  ..., -2.7486e-02,\n",
       "               1.8568e-01, -1.0226e+00],\n",
       "             [-1.1055e+00, -1.3358e+00,  4.9133e-01,  ...,  1.7235e-02,\n",
       "               3.8130e-01,  5.7154e-01],\n",
       "             ...,\n",
       "             [-6.1291e+00, -4.4223e+00,  1.8581e+00,  ...,  1.7797e+00,\n",
       "               3.2886e-01,  3.0593e+00],\n",
       "             [-4.1021e+00, -2.9515e+00,  1.8637e+00,  ...,  1.7410e+00,\n",
       "               3.9948e-01,  2.7388e+00],\n",
       "             [-6.1482e+00, -4.5917e+00,  1.8784e+00,  ...,  1.8406e+00,\n",
       "               3.2767e-01,  3.1094e+00]],\n",
       "  \n",
       "            [[-6.6846e-01,  1.7176e-01, -7.1971e-01,  ...,  3.0749e-01,\n",
       "              -5.1775e-01,  2.2710e+00],\n",
       "             [-7.2725e-01, -1.7903e+00, -2.5658e-01,  ...,  3.4214e-01,\n",
       "               5.3023e-01,  6.1467e-01],\n",
       "             [ 1.0476e+00, -5.9777e-02, -5.9343e-01,  ...,  4.2200e-01,\n",
       "               4.4247e-01, -2.2999e-01],\n",
       "             ...,\n",
       "             [ 2.9605e+00,  4.0557e+00,  7.3473e+00,  ...,  4.1034e+00,\n",
       "              -9.4784e-01, -1.9909e+00],\n",
       "             [ 1.8999e+00,  3.4891e+00,  8.1249e+00,  ...,  3.8013e+00,\n",
       "              -6.2681e-01, -1.5564e+00],\n",
       "             [ 2.9621e+00,  4.5328e+00,  7.4465e+00,  ...,  4.1889e+00,\n",
       "              -1.0214e+00, -2.1078e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.5119e+00, -3.5413e-01, -9.7055e-01,  ..., -2.7067e-01,\n",
       "               5.3213e-01,  8.0726e-02],\n",
       "             [-1.9718e-01,  4.4353e-01, -6.1901e-01,  ...,  2.2208e+00,\n",
       "              -9.6678e-01,  1.6465e-01],\n",
       "             [ 3.0909e-01, -9.3570e-01, -5.2592e-01,  ...,  9.2744e-02,\n",
       "               1.0825e+00,  8.7890e-01],\n",
       "             ...,\n",
       "             [ 6.6495e-02, -4.7285e-01, -4.4333e+00,  ..., -4.1169e+00,\n",
       "               4.5144e-01, -2.9160e+00],\n",
       "             [-8.5149e-01,  4.1586e-01, -4.3870e+00,  ..., -3.2010e+00,\n",
       "              -1.0747e+00, -3.2399e+00],\n",
       "             [ 9.1686e-02, -7.0371e-01, -4.4745e+00,  ..., -4.5097e+00,\n",
       "               3.0333e-01, -3.0811e+00]],\n",
       "  \n",
       "            [[ 1.2214e+00, -6.1433e-01,  7.3578e-01,  ...,  1.0523e+00,\n",
       "              -7.2978e-01,  8.5789e-02],\n",
       "             [ 1.6784e+00, -3.5812e-01,  5.7494e-01,  ...,  9.6930e-01,\n",
       "               4.6297e-01,  1.6093e+00],\n",
       "             [-3.6772e-01,  2.6426e-01,  5.1815e-01,  ...,  6.0338e-02,\n",
       "               5.1054e-01,  5.7864e-01],\n",
       "             ...,\n",
       "             [ 5.4237e-01,  2.1640e+00,  5.8478e-01,  ..., -8.5628e-01,\n",
       "               5.6154e+00, -6.1026e-01],\n",
       "             [ 2.1409e+00,  1.8291e+00,  1.6437e-01,  ..., -7.4168e-01,\n",
       "               5.6343e+00, -3.9523e-02],\n",
       "             [ 5.9729e-01,  2.2197e+00,  5.6205e-01,  ..., -8.7712e-01,\n",
       "               5.6763e+00, -7.4367e-01]],\n",
       "  \n",
       "            [[ 1.0824e+00, -8.8322e-01, -5.8215e-01,  ...,  9.8842e-02,\n",
       "              -7.9274e-01, -9.6895e-03],\n",
       "             [ 3.4742e-01, -9.5856e-01,  1.0344e-01,  ..., -5.1646e-01,\n",
       "              -3.1679e-01, -1.0867e+00],\n",
       "             [ 1.6602e+00, -5.5901e-01,  4.2316e-01,  ...,  3.5425e-02,\n",
       "               7.4290e-01, -7.3806e-01],\n",
       "             ...,\n",
       "             [ 8.7255e+00, -4.2076e+00,  9.5962e-01,  ..., -7.4126e-01,\n",
       "               4.1605e+00, -6.1550e-01],\n",
       "             [ 8.0094e+00, -4.2375e+00,  4.7914e-01,  ..., -1.0832e+00,\n",
       "               4.3223e+00, -1.0895e+00],\n",
       "             [ 8.9928e+00, -4.2666e+00,  7.4117e-01,  ..., -8.4989e-01,\n",
       "               4.5812e+00, -4.6102e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-6.6852e-02, -5.6592e-01,  4.2532e-01,  ..., -7.0091e-01,\n",
       "               3.8756e-01, -6.1264e-01],\n",
       "             [-1.9859e-01, -8.7832e-01, -1.9238e-01,  ...,  5.8986e-01,\n",
       "              -7.1951e-01,  1.2023e-01],\n",
       "             [-7.3895e-02,  1.5089e-01, -1.6994e-01,  ...,  2.5684e-01,\n",
       "               8.8383e-02, -4.5007e-01],\n",
       "             ...,\n",
       "             [-3.4077e-01, -9.1102e-02,  2.4719e-02,  ...,  3.1956e-01,\n",
       "              -1.8878e-01, -3.4889e-01],\n",
       "             [-7.8042e-01, -1.6842e-01,  2.9399e-01,  ...,  5.2441e-01,\n",
       "              -4.1002e-01,  2.9431e-01],\n",
       "             [-3.5606e-01, -8.4383e-02,  3.4986e-02,  ...,  3.3102e-01,\n",
       "              -1.8385e-01, -3.4531e-01]],\n",
       "  \n",
       "            [[ 8.2340e-01,  1.4297e-01,  2.9584e-01,  ..., -7.8567e-01,\n",
       "              -2.5546e-01, -2.4159e-01],\n",
       "             [ 1.5664e-01,  3.6113e-01, -4.1939e-01,  ..., -9.2128e-01,\n",
       "              -5.1178e-01, -5.5919e-02],\n",
       "             [ 3.5955e-01,  1.9441e-01, -1.8975e-01,  ..., -1.0018e-02,\n",
       "               3.9915e-02, -1.1263e-01],\n",
       "             ...,\n",
       "             [ 9.5230e-02,  6.3742e-02, -3.8110e-01,  ...,  1.6581e-01,\n",
       "              -2.9900e-02, -2.9628e-02],\n",
       "             [ 5.4784e-02,  1.9885e-01, -2.4651e-01,  ..., -6.0623e-01,\n",
       "              -2.0140e-01,  1.5841e-01],\n",
       "             [ 9.6416e-02,  4.8562e-02, -3.7818e-01,  ...,  1.5322e-01,\n",
       "              -2.9755e-02, -2.2827e-02]],\n",
       "  \n",
       "            [[-2.9850e-01, -7.9912e-02,  1.1419e-01,  ..., -5.4689e-01,\n",
       "               1.1660e-01,  3.3313e-01],\n",
       "             [ 5.9552e-01, -4.6070e-02, -7.1615e-01,  ...,  2.4028e-02,\n",
       "              -3.9326e-02, -2.5013e-01],\n",
       "             [-1.1187e-01,  6.3909e-03,  2.8814e-02,  ...,  5.0684e-02,\n",
       "               1.7826e-03,  1.5778e-02],\n",
       "             ...,\n",
       "             [-1.2377e-01, -1.0111e-01,  6.1354e-03,  ...,  1.1384e-01,\n",
       "               2.5782e-02, -6.1261e-02],\n",
       "             [ 6.5164e-03,  3.9157e-01, -5.8301e-01,  ..., -4.2655e-02,\n",
       "              -3.2794e-01, -2.4056e-01],\n",
       "             [-1.3216e-01, -1.1875e-01,  1.1836e-02,  ...,  1.0380e-01,\n",
       "               2.5231e-02, -6.9330e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.3784e-01,  2.3087e-01,  2.9162e-01,  ...,  4.1505e-01,\n",
       "              -4.9935e-01, -1.4532e-02],\n",
       "             [ 8.9929e-03, -4.3563e-01, -2.1139e-01,  ..., -1.4202e-01,\n",
       "               2.8848e-01,  6.4669e-02],\n",
       "             [-2.4237e-01,  5.2177e-01,  1.6602e-01,  ..., -5.7325e-03,\n",
       "              -6.3978e-01, -1.0185e-01],\n",
       "             ...,\n",
       "             [-2.0690e-02,  2.4675e-01,  1.8040e-01,  ..., -5.5178e-02,\n",
       "              -5.7811e-01,  2.8422e-02],\n",
       "             [-5.6921e-02, -2.6193e-01, -7.2066e-02,  ..., -2.2721e-01,\n",
       "               3.2617e-01, -1.0868e-01],\n",
       "             [-1.1236e-02,  2.3373e-01,  1.8179e-01,  ..., -7.4333e-02,\n",
       "              -5.5246e-01,  1.8711e-02]],\n",
       "  \n",
       "            [[-2.4285e-01,  5.7736e-01,  3.2711e-01,  ...,  2.5097e-01,\n",
       "              -3.5418e-01,  2.3419e-01],\n",
       "             [-6.3471e-02, -1.6851e-01, -8.1014e-01,  ..., -5.7526e-01,\n",
       "               1.9905e-01,  5.3377e-01],\n",
       "             [ 1.4993e-01,  2.3217e-01,  9.3562e-02,  ..., -1.9085e-01,\n",
       "               1.7783e-01, -4.2146e-03],\n",
       "             ...,\n",
       "             [-1.1577e-01,  3.2358e-01,  3.3604e-01,  ..., -1.0065e-01,\n",
       "               2.0470e-01, -8.4269e-02],\n",
       "             [-2.5959e-01, -1.2849e-01, -6.1569e-01,  ...,  3.7953e-01,\n",
       "               2.2843e-01,  6.9070e-01],\n",
       "             [-1.2440e-01,  3.3724e-01,  3.2618e-01,  ..., -8.2896e-02,\n",
       "               2.1031e-01, -8.6497e-02]],\n",
       "  \n",
       "            [[ 6.8262e-01,  5.9938e-02, -7.9621e-01,  ...,  8.9056e-02,\n",
       "               9.6512e-02,  3.0698e-01],\n",
       "             [ 4.0905e-01,  3.1792e-01, -2.6229e-01,  ..., -3.6171e-01,\n",
       "              -2.7358e-03,  1.5832e-01],\n",
       "             [ 4.5659e-01,  1.7325e-01, -6.3922e-01,  ...,  4.3049e-01,\n",
       "               6.2543e-01,  1.9892e-01],\n",
       "             ...,\n",
       "             [ 1.8475e-01,  1.4191e-01, -4.6684e-01,  ...,  2.4161e-01,\n",
       "               4.6827e-01,  3.2062e-01],\n",
       "             [ 4.0275e-01,  2.5731e-01,  2.6473e-01,  ...,  8.8358e-03,\n",
       "               1.0509e-01,  2.5707e-01],\n",
       "             [ 1.6854e-01,  1.4154e-01, -4.5306e-01,  ...,  2.3733e-01,\n",
       "               4.6155e-01,  3.3401e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-6.3325e-01,  3.9469e-01,  2.9316e-01,  ...,  6.4813e-02,\n",
       "              -9.1779e-01, -4.2052e-02],\n",
       "             [-4.5004e-02, -1.8257e+00,  1.7290e+00,  ...,  9.2967e-01,\n",
       "               1.7760e+00,  1.1929e+00],\n",
       "             [ 5.7945e-01, -2.4468e+00,  1.0202e+00,  ..., -4.1458e-01,\n",
       "               2.5667e-01,  6.5768e-01],\n",
       "             ...,\n",
       "             [ 5.7077e-01, -1.2703e+00,  1.2887e+00,  ...,  4.6652e-01,\n",
       "              -3.8547e+00,  8.7635e-01],\n",
       "             [-5.8669e-02, -9.1068e-01,  1.7302e+00,  ...,  1.4910e+00,\n",
       "              -3.3909e+00,  2.5928e-01],\n",
       "             [ 5.5127e-01, -1.1525e+00,  1.2543e+00,  ...,  5.1223e-01,\n",
       "              -4.0491e+00,  8.4678e-01]],\n",
       "  \n",
       "            [[-4.3842e-01, -5.9148e-01, -2.7844e-01,  ..., -2.2067e-01,\n",
       "              -4.7638e-01,  5.8577e-01],\n",
       "             [ 1.2357e+00, -7.4975e-01,  2.6301e-02,  ..., -5.0183e-01,\n",
       "               9.2459e-02, -6.7021e-01],\n",
       "             [-5.9259e-01,  1.4132e-02,  8.0689e-01,  ..., -7.2636e-02,\n",
       "              -2.7348e-01, -3.1937e-01],\n",
       "             ...,\n",
       "             [ 1.0297e+00,  6.8606e-01, -2.3024e-01,  ...,  5.9897e-01,\n",
       "               2.2278e-01,  2.2106e+00],\n",
       "             [ 2.6319e+00,  2.7007e-01, -5.6663e-01,  ...,  6.9342e-01,\n",
       "               2.1320e-01,  2.3100e+00],\n",
       "             [ 9.8669e-01,  6.4093e-01, -1.1399e-01,  ...,  6.5037e-01,\n",
       "               7.5651e-02,  2.3170e+00]],\n",
       "  \n",
       "            [[ 1.6223e-01,  5.1118e-01, -2.0861e-01,  ...,  9.5185e-01,\n",
       "              -3.0350e-01, -1.0133e-02],\n",
       "             [-5.3412e-01,  1.4999e+00, -9.6452e-01,  ...,  7.6231e-01,\n",
       "              -1.3834e-01, -2.8390e+00],\n",
       "             [ 1.0632e+00,  5.6889e-01, -9.1059e-01,  ..., -1.6277e+00,\n",
       "              -1.8745e+00,  7.3168e-02],\n",
       "             ...,\n",
       "             [-4.7926e+00,  2.8693e-01, -1.5259e+00,  ...,  7.9731e-02,\n",
       "              -4.7079e+00, -9.4559e-01],\n",
       "             [-5.5974e+00,  7.4334e-01, -1.5003e+00,  ...,  1.1226e+00,\n",
       "              -3.0924e+00, -3.3718e+00],\n",
       "             [-4.9517e+00,  2.0994e-01, -1.2985e+00,  ...,  1.7235e-01,\n",
       "              -4.7512e+00, -9.2952e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-1.9460e+00,  1.6198e+00, -2.2637e-01,  ...,  8.5561e-01,\n",
       "               1.5478e+00, -2.0620e+00],\n",
       "             [ 1.3899e-01,  1.6465e+00,  2.1241e+00,  ...,  2.0562e+00,\n",
       "               1.2509e+00, -8.2310e-01],\n",
       "             [-1.3150e+00,  4.2455e-01, -6.8421e-01,  ..., -4.7261e-01,\n",
       "               1.4553e-01, -9.8936e-01],\n",
       "             ...,\n",
       "             [-1.5762e+01,  1.1767e+01, -3.2362e+00,  ...,  2.4840e+00,\n",
       "               1.2166e+01, -8.4330e+00],\n",
       "             [-1.5866e+01,  1.2999e+01, -8.5682e-01,  ...,  5.1198e+00,\n",
       "               1.4119e+01, -9.2889e+00],\n",
       "             [-1.6218e+01,  1.1830e+01, -3.4768e+00,  ...,  2.2817e+00,\n",
       "               1.2677e+01, -8.6229e+00]],\n",
       "  \n",
       "            [[ 4.0023e-01, -4.7454e-01,  3.7902e-02,  ...,  3.6929e-01,\n",
       "               2.0590e-01, -2.5502e-01],\n",
       "             [ 1.7048e+00,  1.2144e-01,  8.7871e-01,  ..., -6.6316e-01,\n",
       "               5.0351e-01,  2.3513e+00],\n",
       "             [ 4.2953e-01,  2.3597e-01,  8.4505e-01,  ...,  5.1996e-01,\n",
       "              -3.1822e-01, -4.3976e-01],\n",
       "             ...,\n",
       "             [-1.2560e+00, -2.3782e+00, -1.1954e+00,  ..., -1.1961e+00,\n",
       "              -2.1249e+00, -2.2535e+00],\n",
       "             [-2.7344e-01, -2.2317e+00, -9.4684e-01,  ..., -1.3955e+00,\n",
       "              -1.7475e+00, -5.8950e-01],\n",
       "             [-1.4261e+00, -2.4795e+00, -1.1940e+00,  ..., -1.2528e+00,\n",
       "              -2.3218e+00, -2.2929e+00]],\n",
       "  \n",
       "            [[ 1.6767e-01, -6.3861e-02,  2.8639e-01,  ...,  9.1864e-02,\n",
       "              -3.9977e-01, -1.8771e-01],\n",
       "             [-1.9732e+00, -7.8003e-01,  6.9140e-01,  ..., -2.2954e+00,\n",
       "              -2.7288e-01, -9.5027e-01],\n",
       "             [ 3.1338e-02,  4.9833e-02,  3.7996e-01,  ..., -2.0243e-01,\n",
       "              -3.0730e-01, -3.8437e-01],\n",
       "             ...,\n",
       "             [-5.2304e-01,  9.8854e-01, -6.6637e-01,  ..., -7.2729e-01,\n",
       "               6.1644e-01,  8.7698e-01],\n",
       "             [-1.2670e+00,  8.9094e-01, -2.3781e-01,  ..., -1.9714e+00,\n",
       "               1.3418e+00,  4.7336e-01],\n",
       "             [-5.4655e-01,  1.0230e+00, -7.3006e-01,  ..., -7.4941e-01,\n",
       "               6.7928e-01,  9.2925e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-7.9187e-02, -3.1501e-02,  4.9325e-02,  ...,  1.0304e-01,\n",
       "              -9.0569e-02,  2.0801e-02],\n",
       "             [-7.3204e-01, -4.4449e-02,  2.3747e-01,  ..., -4.6206e-01,\n",
       "               5.8424e-01,  1.0133e+00],\n",
       "             [-7.6069e-02,  4.8222e-01,  3.1537e-03,  ...,  5.0171e-01,\n",
       "              -7.3100e-02, -2.9966e-01],\n",
       "             ...,\n",
       "             [ 2.2420e-01,  3.6184e-01,  5.1837e-02,  ...,  4.5890e-01,\n",
       "               2.0513e-01, -4.6064e-01],\n",
       "             [ 2.4331e-01,  6.1725e-01,  1.9288e-01,  ..., -3.0622e-03,\n",
       "               9.2698e-01,  2.3394e-01],\n",
       "             [ 2.3159e-01,  3.5013e-01,  4.3568e-02,  ...,  4.5835e-01,\n",
       "               2.0230e-01, -4.5020e-01]],\n",
       "  \n",
       "            [[ 1.8976e-02,  1.2167e-02, -7.1562e-02,  ...,  3.4018e-01,\n",
       "              -2.2299e-02, -1.6386e-01],\n",
       "             [-5.5225e-01, -4.4769e-01, -3.1093e-01,  ..., -1.4474e-01,\n",
       "              -4.7709e-01,  1.2398e+00],\n",
       "             [ 4.8762e-01,  5.1955e-01,  2.5512e-01,  ...,  6.6379e-01,\n",
       "              -7.6514e-01, -2.7496e-01],\n",
       "             ...,\n",
       "             [ 2.5473e-01,  3.4666e-01,  1.3168e-01,  ...,  1.4664e-01,\n",
       "              -3.8548e-01, -4.6793e-01],\n",
       "             [-6.2654e-01, -1.7804e-01,  2.0550e-01,  ..., -3.5974e-02,\n",
       "               5.2182e-01,  4.5127e-01],\n",
       "             [ 2.5811e-01,  3.3385e-01,  1.2555e-01,  ...,  1.4417e-01,\n",
       "              -3.1794e-01, -4.6807e-01]],\n",
       "  \n",
       "            [[ 4.6790e-02, -1.3087e-01,  4.9169e-02,  ..., -3.1279e-02,\n",
       "              -1.6421e-01,  1.5845e-01],\n",
       "             [ 1.7891e-01,  2.9642e-02, -1.0641e+00,  ..., -4.1416e-01,\n",
       "               3.8544e-01, -7.6640e-01],\n",
       "             [ 8.1145e-02,  9.4949e-02,  7.5022e-01,  ..., -1.5809e-01,\n",
       "               8.5609e-02,  5.2451e-01],\n",
       "             ...,\n",
       "             [ 8.9220e-03,  6.5013e-02,  5.3820e-01,  ..., -5.9273e-02,\n",
       "               1.7857e-01,  1.6612e-01],\n",
       "             [ 1.7468e-01,  1.7579e-01, -6.5151e-01,  ..., -2.4798e-01,\n",
       "               2.5008e-01, -6.1831e-01],\n",
       "             [ 5.7972e-03,  4.6068e-02,  5.5280e-01,  ..., -8.8765e-02,\n",
       "               1.8200e-01,  1.8204e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-1.0552e-01, -3.1239e-02, -2.2913e-02,  ...,  9.4098e-03,\n",
       "               3.1498e-02,  2.6320e-01],\n",
       "             [ 8.1381e-02, -4.2269e-01,  5.0452e-01,  ..., -4.2982e-02,\n",
       "               2.9911e-01,  5.1423e-01],\n",
       "             [ 3.7543e-02, -6.1714e-01, -2.3438e-01,  ...,  2.3281e-01,\n",
       "              -4.8434e-03,  9.7607e-02],\n",
       "             ...,\n",
       "             [-2.1548e-01, -3.6638e-01,  9.0460e-02,  ...,  3.9202e-01,\n",
       "               1.8590e-02,  1.8929e-01],\n",
       "             [-2.3925e-01, -3.3439e-01,  4.0127e-01,  ..., -2.1146e-01,\n",
       "               1.0109e-01,  3.9606e-01],\n",
       "             [-2.1557e-01, -3.4411e-01,  9.0680e-02,  ...,  3.9539e-01,\n",
       "               2.8538e-02,  1.6579e-01]],\n",
       "  \n",
       "            [[ 1.7947e-01, -3.5917e-03, -1.7183e-01,  ...,  4.9090e-02,\n",
       "               2.9142e-02,  3.9662e-02],\n",
       "             [ 5.3916e-01, -1.2580e+00, -5.2479e-02,  ...,  1.9864e-01,\n",
       "               6.5587e-02,  6.7887e-01],\n",
       "             [ 5.7233e-02,  3.9852e-01, -1.0671e-01,  ...,  5.6947e-02,\n",
       "              -9.3032e-03, -4.4979e-01],\n",
       "             ...,\n",
       "             [ 3.6108e-02,  4.5078e-01,  1.0899e-01,  ..., -1.2599e-01,\n",
       "               1.8311e-01, -4.7433e-01],\n",
       "             [ 7.3815e-01, -5.6228e-01,  1.5581e-01,  ...,  2.6655e-01,\n",
       "               4.7017e-01,  1.4620e-01],\n",
       "             [ 4.3145e-02,  4.5767e-01,  1.1392e-01,  ..., -1.2423e-01,\n",
       "               2.0611e-01, -4.9023e-01]],\n",
       "  \n",
       "            [[ 2.1530e-01,  1.5696e-01, -9.2665e-02,  ...,  2.6955e-01,\n",
       "              -8.6116e-02,  2.6278e-02],\n",
       "             [ 2.9228e-01,  4.1495e-01,  1.5488e+00,  ...,  4.1444e-01,\n",
       "              -2.3215e-01,  6.1795e-02],\n",
       "             [ 1.4042e+00, -2.7485e-01, -2.1201e-01,  ...,  1.1163e-02,\n",
       "               1.8750e-01, -1.1495e-02],\n",
       "             ...,\n",
       "             [ 2.9736e-01, -8.4980e-01,  3.9279e-01,  ..., -4.5134e-01,\n",
       "              -1.9227e-01, -4.2966e-01],\n",
       "             [-4.6369e-01, -1.2587e+00,  1.9488e+00,  ..., -2.1268e-01,\n",
       "              -3.7608e-01,  1.9360e-01],\n",
       "             [ 2.3984e-01, -8.9690e-01,  4.0535e-01,  ..., -4.4959e-01,\n",
       "              -2.0263e-01, -4.3026e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-1.2052e-01,  3.1460e-01, -1.2498e-01,  ..., -5.7691e-01,\n",
       "              -1.0106e-01,  5.1160e-01],\n",
       "             [ 8.8265e-01,  4.4766e-01,  3.4790e-02,  ..., -2.4390e-01,\n",
       "              -2.1853e+00, -2.0706e+00],\n",
       "             [-2.3373e-01,  4.9361e-02, -9.1293e-02,  ..., -1.1603e+00,\n",
       "              -1.3555e-01,  1.3860e-01],\n",
       "             ...,\n",
       "             [ 8.0619e-01,  2.9996e-01,  3.9072e-01,  ..., -2.3891e-01,\n",
       "              -1.0860e+00,  3.4973e+00],\n",
       "             [ 1.8534e+00,  2.5182e-01,  6.7582e-01,  ...,  1.1078e-01,\n",
       "              -2.3131e+00,  1.5638e+00],\n",
       "             [ 8.5917e-01,  2.9573e-01,  4.0553e-01,  ..., -2.3097e-01,\n",
       "              -1.0892e+00,  3.5896e+00]],\n",
       "  \n",
       "            [[-8.9517e-02,  4.9057e-01,  1.4619e-01,  ...,  3.7974e-01,\n",
       "              -2.5649e-02, -2.7030e-02],\n",
       "             [ 1.0619e+00, -7.4962e-01, -2.7398e-01,  ..., -1.1415e+00,\n",
       "              -4.3345e-02, -1.0186e+00],\n",
       "             [ 7.8154e-01,  1.1212e-01, -1.9386e-01,  ..., -4.0771e-03,\n",
       "               1.0709e+00, -5.0827e-01],\n",
       "             ...,\n",
       "             [-1.1508e+00,  6.3215e+00,  1.3684e+00,  ...,  4.3259e+00,\n",
       "               7.8122e-01, -2.0161e+00],\n",
       "             [-8.8067e-01,  6.4905e+00,  1.1133e+00,  ...,  4.0546e+00,\n",
       "               6.1298e-01, -2.8505e+00],\n",
       "             [-9.1681e-01,  6.6029e+00,  1.2692e+00,  ...,  4.5555e+00,\n",
       "               7.2940e-01, -2.0970e+00]],\n",
       "  \n",
       "            [[-3.3719e-02,  1.2957e-01, -5.7693e-01,  ...,  1.2116e-01,\n",
       "              -2.3101e-02,  8.2890e-01],\n",
       "             [-2.5057e-01,  1.2633e+00,  1.0270e+00,  ...,  5.1710e-01,\n",
       "              -1.2780e+00, -7.3460e-01],\n",
       "             [-8.3250e-01, -1.0980e-01,  1.1351e+00,  ...,  6.6763e-01,\n",
       "              -1.2242e+00, -3.3540e-01],\n",
       "             ...,\n",
       "             [ 4.2551e-01, -1.4327e+00,  1.1612e+00,  ..., -2.3588e+00,\n",
       "              -1.0415e+00,  7.6158e+00],\n",
       "             [ 1.4797e+00, -8.2905e-02,  1.1546e+00,  ..., -2.7280e+00,\n",
       "              -1.4252e+00,  8.0833e+00],\n",
       "             [ 6.5094e-01, -1.4681e+00,  1.1309e+00,  ..., -2.5426e+00,\n",
       "              -9.6533e-01,  7.6542e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-2.1414e-01,  4.7462e-02, -1.5474e-01,  ..., -8.3162e-02,\n",
       "              -1.1102e+00,  1.9495e-01],\n",
       "             [ 4.5022e-01, -2.3045e+00,  2.3217e+00,  ..., -3.9795e-01,\n",
       "               1.3822e+00,  1.4771e-01],\n",
       "             [ 5.5677e-01, -1.2439e+00,  1.7905e+00,  ..., -3.9029e-02,\n",
       "               1.0830e+00, -1.2510e+00],\n",
       "             ...,\n",
       "             [ 1.3848e+00, -3.9311e+00,  9.7458e-01,  ..., -4.3478e-01,\n",
       "              -4.0477e+00, -1.6007e-01],\n",
       "             [ 1.2519e+00, -4.1817e+00,  2.3294e+00,  ..., -6.6730e-01,\n",
       "              -3.9034e+00,  6.9753e-01],\n",
       "             [ 1.4027e+00, -4.0292e+00,  8.6477e-01,  ..., -4.2729e-01,\n",
       "              -4.1051e+00, -1.1145e-01]],\n",
       "  \n",
       "            [[ 2.4719e-01, -7.1770e-03,  1.7221e-02,  ..., -2.0889e-01,\n",
       "               1.0311e-02, -1.1938e-01],\n",
       "             [ 2.6888e+00,  4.0968e-01,  4.9493e-01,  ...,  5.7549e-01,\n",
       "               1.4951e+00,  2.6204e-01],\n",
       "             [ 7.0993e-01, -8.1806e-01,  8.7712e-01,  ...,  2.9560e-01,\n",
       "               7.3689e-01, -5.6554e-01],\n",
       "             ...,\n",
       "             [ 1.7369e+00, -1.6013e+00,  1.2524e+00,  ..., -7.3715e-02,\n",
       "               7.1026e-01, -1.0380e+00],\n",
       "             [ 2.7488e+00, -1.1873e+00,  1.3597e+00,  ..., -3.1033e-01,\n",
       "               2.1820e+00, -3.4525e-01],\n",
       "             [ 1.7638e+00, -1.6432e+00,  1.2377e+00,  ..., -1.4097e-01,\n",
       "               7.4573e-01, -1.0845e+00]],\n",
       "  \n",
       "            [[-8.3548e-01,  4.1693e-02, -2.6349e-01,  ...,  2.7683e-01,\n",
       "              -2.3358e-01, -4.7644e-01],\n",
       "             [ 1.1163e+00, -1.9889e+00,  4.0046e-02,  ..., -8.8493e-01,\n",
       "              -1.9877e+00,  1.6819e+00],\n",
       "             [ 4.9914e-01, -3.2179e-02, -2.8286e-01,  ..., -9.6201e-01,\n",
       "              -9.1476e-02, -4.0798e-01],\n",
       "             ...,\n",
       "             [-3.2654e+00, -1.3647e+00, -1.0656e+00,  ..., -1.4477e-01,\n",
       "               1.4681e+00,  5.2390e-01],\n",
       "             [-2.3891e+00, -2.5547e+00, -6.8467e-01,  ..., -6.8380e-01,\n",
       "              -3.6610e-01,  8.2402e-01],\n",
       "             [-3.4756e+00, -1.3652e+00, -1.1184e+00,  ..., -8.8397e-02,\n",
       "               1.6425e+00,  5.7475e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-1.0710e-01, -3.8192e-02, -5.6153e-02,  ...,  7.6919e-02,\n",
       "              -2.3796e-02,  1.4593e-02],\n",
       "             [ 2.3329e-01,  1.0652e-01,  8.2068e-01,  ...,  2.8625e-01,\n",
       "               7.3824e-01,  5.1752e-01],\n",
       "             [ 4.6713e-01,  9.5628e-02,  1.8415e-01,  ...,  4.3346e-01,\n",
       "               1.5455e-01, -1.9398e-01],\n",
       "             ...,\n",
       "             [ 1.4668e+00,  2.4883e-01,  7.0544e-01,  ...,  1.0487e-01,\n",
       "              -2.1463e-01, -6.4136e-01],\n",
       "             [ 1.7038e+00,  4.3160e-01,  6.9455e-01,  ...,  7.3743e-02,\n",
       "              -8.7207e-02, -2.9821e-01],\n",
       "             [ 1.4859e+00,  2.4740e-01,  7.3056e-01,  ...,  1.0831e-01,\n",
       "              -2.1551e-01, -6.3957e-01]],\n",
       "  \n",
       "            [[-8.0627e-02, -4.6858e-02,  1.3376e-03,  ..., -2.3709e-03,\n",
       "              -2.2702e-04, -5.5303e-02],\n",
       "             [-1.3234e+00, -5.9427e-01, -5.8242e-01,  ...,  5.1768e-01,\n",
       "              -3.0003e-01,  2.1547e-01],\n",
       "             [-3.8883e-01,  1.3008e-01,  4.3000e-01,  ...,  2.3015e-01,\n",
       "               4.6659e-01, -3.7167e-01],\n",
       "             ...,\n",
       "             [ 3.2133e-02,  4.7576e-01,  5.4518e-01,  ...,  1.9310e-01,\n",
       "               6.1286e-02,  3.8637e-01],\n",
       "             [-6.2409e-01, -3.2735e-01, -2.3400e-01,  ...,  7.0215e-01,\n",
       "               5.8598e-03,  5.2216e-01],\n",
       "             [ 5.6396e-02,  4.5940e-01,  5.5989e-01,  ...,  2.0900e-01,\n",
       "               5.2629e-02,  3.7191e-01]],\n",
       "  \n",
       "            [[ 4.4798e-02, -1.3215e-01,  8.4773e-02,  ..., -1.5579e-01,\n",
       "               5.5140e-02,  1.4401e-01],\n",
       "             [-9.7926e-02,  8.6342e-01, -3.8702e-01,  ...,  1.8498e-01,\n",
       "               1.6992e-01, -5.4536e-02],\n",
       "             [-2.1782e-01, -1.8004e-01,  2.6145e-01,  ..., -4.0524e-01,\n",
       "              -4.4305e-01,  8.6285e-01],\n",
       "             ...,\n",
       "             [-2.4447e-02, -2.0777e-01,  4.9620e-02,  ..., -3.0636e-01,\n",
       "              -2.5961e-01,  5.1677e-01],\n",
       "             [ 3.2635e-02,  4.6856e-01, -1.9789e-01,  ...,  4.4220e-01,\n",
       "               1.8311e-01,  1.6280e-02],\n",
       "             [-4.9732e-02, -1.9359e-01,  3.8774e-02,  ..., -2.9885e-01,\n",
       "              -2.4444e-01,  5.0236e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.0042e-02, -1.0485e-02,  1.6918e-01,  ..., -7.4782e-02,\n",
       "              -7.1054e-02, -5.4051e-02],\n",
       "             [-2.7242e-01,  6.4058e-02,  1.0032e+00,  ..., -2.6584e-02,\n",
       "              -2.5675e-01,  4.9826e-01],\n",
       "             [-1.8478e-02,  2.5151e-01, -6.4204e-01,  ..., -3.9094e-01,\n",
       "               7.0915e-01,  3.8277e-01],\n",
       "             ...,\n",
       "             [-4.5560e-01, -2.1959e-01,  5.4481e-02,  ..., -6.3666e-01,\n",
       "              -5.4701e-01, -7.2765e-01],\n",
       "             [-6.8903e-01,  5.7498e-02,  2.0126e-01,  ..., -5.4965e-01,\n",
       "              -6.2776e-01, -6.8481e-01],\n",
       "             [-4.6340e-01, -2.1821e-01,  4.2120e-02,  ..., -6.3281e-01,\n",
       "              -5.6399e-01, -7.1893e-01]],\n",
       "  \n",
       "            [[ 3.1571e-02, -2.4925e-02,  1.2156e-01,  ..., -9.6619e-02,\n",
       "              -1.8544e-01, -9.2648e-02],\n",
       "             [ 4.5478e-01, -1.1751e-01, -3.2476e-01,  ..., -1.6675e-01,\n",
       "              -4.4801e-01, -9.4666e-01],\n",
       "             [ 6.9083e-01, -2.0501e-01,  8.8175e-01,  ...,  5.0497e-01,\n",
       "              -1.7642e-01, -3.1971e-01],\n",
       "             ...,\n",
       "             [-3.1124e-01, -2.5737e-01,  1.0384e-01,  ...,  5.7330e-01,\n",
       "              -7.7170e-02,  1.1473e-01],\n",
       "             [-3.1404e-01, -4.7380e-01, -3.4186e-02,  ...,  2.3507e-01,\n",
       "              -1.0089e+00,  1.4769e-01],\n",
       "             [-3.0170e-01, -2.7659e-01,  8.7204e-02,  ...,  5.7619e-01,\n",
       "              -4.2394e-02,  1.6308e-01]],\n",
       "  \n",
       "            [[-2.0394e-01,  4.8224e-03, -1.5186e-01,  ...,  1.0724e-02,\n",
       "              -2.8669e-02, -2.4235e-02],\n",
       "             [ 3.7194e-01, -3.2409e-01,  1.2554e-01,  ...,  1.2304e-01,\n",
       "               5.2175e-01,  7.7896e-03],\n",
       "             [ 1.6817e-01, -3.2303e-01, -8.3657e-01,  ...,  3.4013e-01,\n",
       "              -5.8837e-01,  7.5414e-01],\n",
       "             ...,\n",
       "             [ 3.2087e-01, -2.2216e-01, -5.2580e-01,  ..., -2.4853e-01,\n",
       "              -4.3614e-02,  4.2875e-01],\n",
       "             [ 9.1567e-01, -6.4019e-01, -9.8221e-02,  ...,  2.6370e-02,\n",
       "               3.2114e-01, -8.2904e-02],\n",
       "             [ 3.1945e-01, -2.0866e-01, -5.1928e-01,  ..., -2.5009e-01,\n",
       "              -3.9465e-02,  4.0865e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 0.1522, -0.1238,  0.0806,  ...,  0.0079,  0.1238,  0.1840],\n",
       "             [-0.4660, -1.2609, -0.1456,  ...,  1.9608, -0.7999,  0.7594],\n",
       "             [-0.5371, -2.2926, -0.3693,  ..., -0.1720, -0.1752,  1.0457],\n",
       "             ...,\n",
       "             [-1.4090, -2.2825, -0.7236,  ...,  1.3987, -0.7897,  0.5247],\n",
       "             [-1.3876, -2.0831, -0.0911,  ...,  2.9715, -1.4077, -1.0845],\n",
       "             [-1.3838, -2.2687, -0.6756,  ...,  1.4140, -0.7528,  0.4453]],\n",
       "  \n",
       "            [[ 0.1666, -0.3944, -0.1043,  ...,  1.1979, -0.4242,  0.2155],\n",
       "             [ 0.4579, -1.2979,  0.1309,  ...,  1.7841,  0.7092, -0.4384],\n",
       "             [-0.7685,  0.1752, -1.3169,  ...,  3.5756, -0.7979,  2.9075],\n",
       "             ...,\n",
       "             [-0.4242, -0.9696,  0.0135,  ...,  1.7306,  0.3568,  0.5882],\n",
       "             [-0.2232, -1.3041, -0.9986,  ...,  0.6639,  1.0196, -0.1685],\n",
       "             [-0.4844, -0.9683,  0.0211,  ...,  1.6929,  0.3137,  0.6046]],\n",
       "  \n",
       "            [[ 0.2471, -0.2090,  0.0849,  ..., -0.0971, -0.2162, -0.0145],\n",
       "             [-2.1001, -0.3690,  2.2352,  ..., -0.5417, -1.1098,  2.3830],\n",
       "             [-0.0043, -0.6452, -0.4240,  ...,  0.7181, -0.9739,  1.4241],\n",
       "             ...,\n",
       "             [ 0.6807, -0.3870, -0.6354,  ...,  0.0532, -2.2537,  1.1704],\n",
       "             [-1.1359,  0.2690,  1.1584,  ...,  0.2841, -2.4995,  2.7685],\n",
       "             [ 0.6803, -0.3590, -0.6710,  ...,  0.0700, -2.2370,  1.1229]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-0.1001, -0.1053, -0.0964,  ...,  0.1972,  0.0656,  0.3014],\n",
       "             [-1.4928, -0.5012, -0.5930,  ...,  0.4150,  0.2703,  0.7185],\n",
       "             [-0.1107,  1.0035,  0.7458,  ...,  0.6817,  1.0631,  0.7142],\n",
       "             ...,\n",
       "             [-0.9490,  1.2392,  0.1111,  ...,  0.0168,  0.1435,  0.7578],\n",
       "             [-1.4154, -0.1837, -0.7714,  ...,  0.0888, -0.2381,  0.2331],\n",
       "             [-0.9152,  1.2111,  0.1225,  ..., -0.0385,  0.0925,  0.7444]],\n",
       "  \n",
       "            [[-0.1148,  0.5221,  0.2127,  ...,  0.2002, -0.1479,  0.3012],\n",
       "             [-2.0446, -3.5580, -2.9182,  ...,  0.2825,  1.2641, -1.4347],\n",
       "             [-1.5641, -2.9386, -3.0378,  ..., -0.1576, -0.0808, -0.8935],\n",
       "             ...,\n",
       "             [-0.7796, -0.8852,  3.4144,  ..., -0.8310, -0.4667, -2.1323],\n",
       "             [-0.7379, -1.6652,  3.6758,  ..., -0.9510, -0.0729, -3.1649],\n",
       "             [-0.7528, -0.5006,  3.4325,  ..., -0.8210, -0.5759, -2.1394]],\n",
       "  \n",
       "            [[ 0.1148, -0.4909, -0.0899,  ..., -0.3996,  0.1366, -0.4292],\n",
       "             [ 1.0752, -0.3143,  0.7523,  ...,  1.0655, -0.6817,  0.4163],\n",
       "             [-1.6916, -1.1988,  2.1872,  ...,  1.7348, -0.7664,  0.1665],\n",
       "             ...,\n",
       "             [ 0.0400, -3.0988,  2.4965,  ...,  0.6559, -1.1022, -3.4828],\n",
       "             [ 1.1552, -3.5060,  2.3746,  ...,  1.0693, -0.8055, -3.3478],\n",
       "             [-0.0836, -3.1210,  2.5807,  ...,  0.6501, -1.0899, -3.5258]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-0.0204,  0.0455, -0.1136,  ..., -0.0685, -0.0491, -0.0063],\n",
       "             [-1.5833, -0.1715,  0.8137,  ...,  1.5860, -0.4219,  0.2143],\n",
       "             [-0.0774,  1.2746,  0.3464,  ...,  0.2322, -0.3886, -0.5177],\n",
       "             ...,\n",
       "             [ 1.2403,  0.4966, -0.9015,  ...,  1.0825,  0.0485, -0.4068],\n",
       "             [-0.3811, -0.0931, -0.0660,  ...,  0.8470, -1.0236,  0.4968],\n",
       "             [ 1.2177,  0.4691, -0.8821,  ...,  1.0692,  0.0138, -0.3618]],\n",
       "  \n",
       "            [[ 0.1180,  0.1181,  0.1384,  ..., -0.0786,  0.0879, -0.0900],\n",
       "             [ 0.4230, -0.0881, -1.2667,  ...,  0.6964,  0.2801,  0.9957],\n",
       "             [-0.1096, -0.4304, -0.8970,  ...,  1.3079, -0.0702, -0.0203],\n",
       "             ...,\n",
       "             [ 1.0570,  0.3197,  0.2069,  ..., -3.3632,  0.2860, -0.2088],\n",
       "             [ 1.2921,  1.0605, -0.2923,  ..., -3.7712,  0.5944,  0.5460],\n",
       "             [ 1.0448,  0.3312,  0.2002,  ..., -3.4037,  0.2634, -0.1713]],\n",
       "  \n",
       "            [[ 0.1363,  0.0990,  0.2185,  ..., -0.0363,  0.0923,  0.1448],\n",
       "             [ 0.0458, -0.7376,  0.3980,  ...,  0.3582,  0.1635, -0.3253],\n",
       "             [-0.0984, -0.0613, -0.0065,  ...,  0.3470, -0.0763,  0.3218],\n",
       "             ...,\n",
       "             [-0.2028,  0.2188,  0.0738,  ..., -0.1002,  0.6261,  0.2141],\n",
       "             [-0.5296, -0.3441,  0.4984,  ...,  0.1910,  0.8891, -0.0182],\n",
       "             [-0.2107,  0.1922,  0.0421,  ..., -0.1268,  0.6032,  0.2125]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 0.0251,  0.0863,  0.1066,  ...,  0.0969,  0.0687, -0.0166],\n",
       "             [ 0.7474,  0.6776,  0.0560,  ...,  0.1618,  0.1376,  1.3513],\n",
       "             [ 0.0627,  1.1009,  0.7630,  ..., -0.1159,  0.5759,  0.0866],\n",
       "             ...,\n",
       "             [-0.2568, -0.0754, -0.1326,  ..., -0.9518,  0.5726, -0.1901],\n",
       "             [-0.3014, -0.4887, -1.5597,  ..., -0.4300,  0.0628,  1.3290],\n",
       "             [-0.2783, -0.0800, -0.1557,  ..., -0.9657,  0.5462, -0.1784]],\n",
       "  \n",
       "            [[-0.0421, -0.0243, -0.1207,  ...,  0.1521, -0.0094,  0.0965],\n",
       "             [-0.7639,  0.5166, -0.1799,  ..., -0.1223, -0.1309,  0.1492],\n",
       "             [ 1.2735, -0.3839, -0.8504,  ...,  1.0511, -0.5509,  0.5284],\n",
       "             ...,\n",
       "             [ 0.8194, -0.2286, -1.0354,  ...,  0.4890, -0.2652,  0.2940],\n",
       "             [-1.2244, -0.0647, -0.0435,  ...,  0.0621, -0.3496, -0.1790],\n",
       "             [ 0.7925, -0.2165, -1.0473,  ...,  0.4959, -0.3003,  0.2679]],\n",
       "  \n",
       "            [[-0.0684, -0.0282,  0.1385,  ..., -0.0458, -0.0772,  0.1848],\n",
       "             [-0.9698,  0.3077,  0.7843,  ...,  0.1362, -0.3643, -0.6377],\n",
       "             [ 0.8484, -1.9875, -0.3901,  ..., -0.5127,  0.7504,  0.3240],\n",
       "             ...,\n",
       "             [ 0.2670, -0.6692,  1.3751,  ..., -0.1892, -0.7515,  0.1590],\n",
       "             [-0.6329,  0.0805,  1.8714,  ...,  0.2167, -1.0237,  0.0263],\n",
       "             [ 0.2379, -0.6431,  1.3933,  ..., -0.1740, -0.7878,  0.1467]]]]],\n",
       "         grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 4.5990e-01, -1.6423e-01, -7.0473e-02,  ...,  1.2789e-02,\n",
       "               3.3595e-01, -3.0549e-01],\n",
       "             [ 1.2482e+00,  3.7890e-02, -1.9409e+00,  ..., -8.0578e-01,\n",
       "              -1.2600e-01,  6.0470e-01],\n",
       "             [ 8.8900e-01,  2.4907e-01, -4.4895e-01,  ..., -1.1019e+00,\n",
       "               8.3893e-01,  6.2958e-01],\n",
       "             ...,\n",
       "             [ 1.0981e+00,  1.6426e-01, -1.0801e+00,  ..., -1.6687e-01,\n",
       "               3.7372e-02, -2.3268e+00],\n",
       "             [ 8.1382e-01, -6.7716e-01, -1.4475e+00,  ...,  3.4822e-01,\n",
       "               4.2177e-01, -1.7670e+00],\n",
       "             [ 1.0660e+00,  9.8135e-02, -1.1025e+00,  ..., -2.1228e-01,\n",
       "              -5.9497e-03, -2.3879e+00]],\n",
       "  \n",
       "            [[ 1.1706e-01,  3.4325e-02,  3.3315e-01,  ...,  6.6974e-02,\n",
       "               3.6118e-01, -1.7523e-01],\n",
       "             [-5.3648e-02, -1.2139e+00, -6.2853e-01,  ...,  5.2832e-01,\n",
       "               1.6742e+00,  5.4137e-02],\n",
       "             [-4.6883e-01, -1.1231e-02, -1.2756e+00,  ...,  3.3899e-01,\n",
       "               1.0640e+00, -1.8882e-01],\n",
       "             ...,\n",
       "             [-5.1580e-01,  1.6949e+00, -5.7431e+00,  ...,  3.9008e-01,\n",
       "               5.9018e-01, -1.3713e+00],\n",
       "             [ 3.1070e-01,  6.7545e-01, -5.6964e+00,  ...,  5.7468e-01,\n",
       "               1.3603e+00, -9.8501e-01],\n",
       "             [-5.2418e-01,  1.6924e+00, -5.8958e+00,  ...,  4.2089e-01,\n",
       "               4.8843e-01, -1.3733e+00]],\n",
       "  \n",
       "            [[ 9.6266e-02,  5.3009e-01, -3.6293e-01,  ...,  3.0148e-02,\n",
       "               1.0204e-01,  2.1953e-01],\n",
       "             [ 1.0387e-02, -7.9064e-01, -8.9884e-01,  ..., -2.7221e-01,\n",
       "               1.5673e+00, -1.2077e+00],\n",
       "             [-1.1311e+00, -9.4840e-01, -5.1545e-02,  ...,  3.5841e-01,\n",
       "               1.2506e+00, -2.8423e+00],\n",
       "             ...,\n",
       "             [-7.4123e-02, -1.0736e+00, -1.7238e+00,  ..., -1.1126e+00,\n",
       "               3.7543e-01, -6.6150e-01],\n",
       "             [ 2.7488e-02, -1.5760e+00, -1.9093e+00,  ..., -1.0375e+00,\n",
       "              -8.9292e-02, -4.8788e-01],\n",
       "             [-7.4714e-02, -1.0654e+00, -1.6442e+00,  ..., -1.1032e+00,\n",
       "               3.8112e-01, -6.3468e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-3.4160e-02,  2.7792e-01,  1.9446e-01,  ...,  2.1648e-01,\n",
       "              -4.0505e-01,  2.1871e-01],\n",
       "             [-2.9885e+00, -1.8224e-01,  6.8868e-01,  ..., -3.1841e-01,\n",
       "              -1.9787e-01, -1.2035e-01],\n",
       "             [-1.0867e+00,  2.2213e+00,  4.0476e-02,  ...,  2.8628e-01,\n",
       "              -2.5353e+00,  1.4230e+00],\n",
       "             ...,\n",
       "             [ 6.9197e-01,  3.2871e+00,  3.2218e-01,  ...,  2.0622e+00,\n",
       "              -1.9082e+00,  7.6810e-01],\n",
       "             [-6.0836e-01,  1.9449e+00,  2.4467e+00,  ...,  1.4759e+00,\n",
       "              -8.3232e-01, -2.7693e-01],\n",
       "             [ 8.3263e-01,  3.2847e+00,  3.5998e-01,  ...,  2.1708e+00,\n",
       "              -1.8807e+00,  6.9558e-01]],\n",
       "  \n",
       "            [[-4.4253e-02, -5.1474e-03,  1.1470e-01,  ..., -6.7926e-02,\n",
       "               3.6750e-01,  2.0194e-01],\n",
       "             [ 7.7663e-01,  1.7269e+00, -1.8497e+00,  ..., -1.6134e+00,\n",
       "              -1.6338e+00, -4.3356e-01],\n",
       "             [ 1.3555e+00,  1.3743e+00,  1.1733e+00,  ..., -1.1017e+00,\n",
       "              -2.4731e-01,  1.7001e+00],\n",
       "             ...,\n",
       "             [-1.3996e+00,  2.8939e+00,  1.2424e+00,  ..., -3.8124e-01,\n",
       "              -3.6470e-02,  1.3434e+00],\n",
       "             [-3.0022e+00,  2.7067e+00, -7.0257e-02,  ..., -1.1823e+00,\n",
       "              -9.0575e-01, -1.5187e-01],\n",
       "             [-1.4168e+00,  2.8616e+00,  1.3390e+00,  ..., -3.8588e-01,\n",
       "              -3.8711e-02,  1.3494e+00]],\n",
       "  \n",
       "            [[ 4.9678e-02,  5.1285e-02,  8.7136e-02,  ..., -7.4805e-02,\n",
       "              -2.3733e-01, -1.7542e-01],\n",
       "             [ 3.6371e-02,  1.9350e-01, -1.0161e-02,  ..., -1.0966e+00,\n",
       "              -5.7417e-01, -1.7524e-01],\n",
       "             [-1.8043e+00, -4.1836e-01,  1.1041e+00,  ..., -2.9041e+00,\n",
       "              -2.7024e-01,  7.4560e-01],\n",
       "             ...,\n",
       "             [-2.0449e+00, -1.3430e+00, -1.3380e-01,  ..., -3.9875e+00,\n",
       "               6.8061e-01,  6.3249e-01],\n",
       "             [-1.1337e+00, -1.1366e+00,  5.2844e-01,  ..., -3.6932e+00,\n",
       "              -1.0545e+00, -1.4105e-01],\n",
       "             [-2.0809e+00, -1.3452e+00, -7.8858e-02,  ..., -3.9452e+00,\n",
       "               7.0290e-01,  6.1458e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-5.7782e-02, -4.7315e-02,  2.4928e-02,  ...,  5.5143e-02,\n",
       "               6.4048e-02,  4.7465e-02],\n",
       "             [-1.3786e-01,  6.7696e-01,  7.4004e-01,  ...,  9.2297e-01,\n",
       "               5.6387e-01, -8.7097e-02],\n",
       "             [ 3.1798e-01, -2.2610e-01, -6.8674e-01,  ...,  4.5118e-01,\n",
       "               6.5257e-01, -5.1282e-01],\n",
       "             ...,\n",
       "             [ 3.6494e-02, -4.3679e-01, -1.4050e-01,  ..., -2.1511e+00,\n",
       "              -5.5579e-01,  1.9740e-01],\n",
       "             [ 1.1142e-02,  3.8077e-01, -2.3947e-01,  ..., -7.8761e-01,\n",
       "               5.5888e-01, -1.3959e-01],\n",
       "             [ 5.5540e-02, -3.7554e-01, -2.0669e-01,  ..., -2.1263e+00,\n",
       "              -5.6430e-01,  1.8848e-01]],\n",
       "  \n",
       "            [[-6.3957e-02, -7.9304e-02,  1.5967e-02,  ...,  6.7480e-03,\n",
       "              -1.2040e-01, -2.2468e-01],\n",
       "             [ 2.3844e-02,  1.0196e+00,  1.2122e-01,  ...,  1.3954e+00,\n",
       "              -2.0906e+00, -6.7638e-01],\n",
       "             [ 6.9860e-01,  5.0115e-01,  6.4690e-01,  ...,  1.1850e+00,\n",
       "              -6.9568e-01,  1.3303e-01],\n",
       "             ...,\n",
       "             [ 1.5618e+00, -1.5785e-01,  6.5925e-01,  ...,  1.2683e+00,\n",
       "              -1.2242e+00, -6.0200e-01],\n",
       "             [ 2.9950e-01,  3.6721e-01,  3.3727e-01,  ...,  8.7919e-01,\n",
       "              -1.8889e+00, -1.1078e+00],\n",
       "             [ 1.4817e+00, -1.6266e-01,  6.1728e-01,  ...,  1.1992e+00,\n",
       "              -1.1882e+00, -6.3248e-01]],\n",
       "  \n",
       "            [[ 1.0753e-01, -4.3963e-02, -6.6306e-02,  ...,  4.0179e-01,\n",
       "              -9.0919e-02, -1.4785e-02],\n",
       "             [-2.7449e-01, -3.0073e-01,  3.2270e-01,  ..., -2.4372e+00,\n",
       "              -3.1224e-02, -7.9692e-01],\n",
       "             [-4.3965e-01, -1.3882e+00,  2.5976e-01,  ..., -2.1991e+00,\n",
       "              -1.6639e-01, -1.2122e+00],\n",
       "             ...,\n",
       "             [-9.1309e-01, -2.7926e+00, -2.6798e-01,  ..., -2.8105e+00,\n",
       "              -2.2858e+00,  4.1534e-01],\n",
       "             [-6.0546e-03, -2.2705e+00,  2.7309e-01,  ..., -2.5844e+00,\n",
       "              -1.4688e+00,  1.0231e+00],\n",
       "             [-9.3313e-01, -2.8158e+00, -2.6005e-01,  ..., -2.7976e+00,\n",
       "              -2.2347e+00,  4.3881e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 7.5646e-02,  3.5870e-03, -1.6026e-01,  ..., -3.9409e-02,\n",
       "              -1.7806e-01,  1.5072e-01],\n",
       "             [ 2.7148e-01, -7.3097e-01,  2.0453e-01,  ..., -7.3614e-01,\n",
       "              -1.3660e-01, -4.7905e-01],\n",
       "             [-5.6934e-02, -1.1385e-01,  2.4108e-01,  ...,  1.0072e-01,\n",
       "              -2.1553e-01,  1.0231e+00],\n",
       "             ...,\n",
       "             [-4.6304e-01, -5.3788e-02,  2.9625e-01,  ...,  1.2628e-02,\n",
       "              -4.7854e-01,  1.0911e+00],\n",
       "             [ 9.5285e-01, -2.9438e-01,  7.5982e-01,  ..., -1.7827e-01,\n",
       "              -8.8304e-02,  2.8551e-01],\n",
       "             [-4.4781e-01, -1.5304e-03,  2.7036e-01,  ...,  1.2610e-02,\n",
       "              -4.6309e-01,  1.0847e+00]],\n",
       "  \n",
       "            [[ 1.5349e-01, -8.3581e-03,  1.8578e-02,  ..., -1.0560e-01,\n",
       "               6.0635e-02,  2.1364e-02],\n",
       "             [-2.3644e+00, -3.4632e-02, -1.1961e+00,  ..., -1.2036e+00,\n",
       "               6.7561e-01, -5.9303e-01],\n",
       "             [-3.2115e-02, -4.0962e-01,  9.3737e-01,  ..., -1.0937e+00,\n",
       "              -6.3260e-01,  3.3339e-01],\n",
       "             ...,\n",
       "             [-1.1529e-01,  6.1468e-01,  2.5898e-01,  ..., -5.7044e-01,\n",
       "              -4.6099e-02,  9.0789e-01],\n",
       "             [-1.2440e+00,  8.6880e-01, -8.5127e-01,  ..., -6.7322e-01,\n",
       "               1.4799e+00, -9.1919e-01],\n",
       "             [-1.0191e-01,  6.2414e-01,  2.4962e-01,  ..., -5.3921e-01,\n",
       "              -3.5162e-02,  8.6342e-01]],\n",
       "  \n",
       "            [[ 2.7211e-02, -7.5024e-02, -5.6496e-02,  ..., -3.7664e-03,\n",
       "               1.7206e-02,  1.3883e-03],\n",
       "             [-2.2256e-01,  2.0972e+00,  6.6128e-01,  ..., -8.3051e-01,\n",
       "               5.4045e-01, -1.5901e-02],\n",
       "             [ 1.8208e-01, -3.1738e-01, -3.1187e-02,  ...,  1.2640e+00,\n",
       "               5.1007e-01, -8.3236e-01],\n",
       "             ...,\n",
       "             [-1.5674e-01, -1.0412e+00,  2.9375e-01,  ...,  1.7144e-01,\n",
       "               1.7886e-01, -8.4005e-01],\n",
       "             [-3.7356e-01,  6.1266e-01,  3.6698e-02,  ..., -4.7902e-01,\n",
       "              -1.5491e-01, -2.3358e-01],\n",
       "             [-1.5318e-01, -1.0205e+00,  2.9034e-01,  ...,  1.7920e-01,\n",
       "               1.9913e-01, -7.7656e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 1.3771e-01,  9.0157e-01, -3.5496e-01,  ..., -2.0243e-01,\n",
       "               9.8999e-02,  7.9731e-02],\n",
       "             [ 6.8603e-01, -8.9343e-01,  1.3114e+00,  ..., -1.1600e-01,\n",
       "               5.5447e-01, -8.8110e-01],\n",
       "             [ 7.1423e-01,  2.9217e-01,  1.4583e-02,  ..., -1.1546e+00,\n",
       "               2.3820e-01,  1.0250e+00],\n",
       "             ...,\n",
       "             [ 1.2194e+00,  7.9311e+00,  1.5823e+00,  ...,  5.1556e-01,\n",
       "               1.9357e+00,  2.5451e+00],\n",
       "             [ 1.1668e+00,  7.9978e+00,  2.5361e+00,  ...,  7.6883e-01,\n",
       "               1.9658e+00,  1.1794e+00],\n",
       "             [ 1.2941e+00,  8.0683e+00,  1.5205e+00,  ...,  5.8429e-01,\n",
       "               1.9638e+00,  2.4792e+00]],\n",
       "  \n",
       "            [[ 6.1901e-02,  8.8106e-01, -2.8147e-01,  ..., -2.6773e-01,\n",
       "              -1.8376e-01, -1.1549e-02],\n",
       "             [-8.8878e-01,  5.5974e-01, -3.4638e-01,  ...,  7.9472e-01,\n",
       "              -1.6082e-01,  8.5831e-01],\n",
       "             [ 1.8869e+00,  1.1415e-01, -1.9564e+00,  ..., -1.6116e+00,\n",
       "               7.9366e-01, -8.6390e-01],\n",
       "             ...,\n",
       "             [-1.4537e-01,  2.8966e+00, -4.6524e+00,  ..., -2.7780e+00,\n",
       "               8.6829e-01, -1.1170e+00],\n",
       "             [-2.0415e+00,  2.8823e+00, -2.8646e+00,  ..., -1.3703e+00,\n",
       "               9.7047e-01, -5.9055e-02],\n",
       "             [-2.5966e-01,  3.1442e+00, -4.6484e+00,  ..., -2.7134e+00,\n",
       "               8.0791e-01, -9.9943e-01]],\n",
       "  \n",
       "            [[ 2.1769e-01, -3.8806e-01,  1.8626e-01,  ...,  2.7785e-01,\n",
       "               3.8709e-01,  3.7514e-01],\n",
       "             [-3.6386e-01,  9.5170e-01, -8.2031e-01,  ..., -1.9245e+00,\n",
       "               7.9807e-01, -1.5011e+00],\n",
       "             [ 6.4954e-01, -4.5205e-01,  1.4459e+00,  ...,  5.1824e-01,\n",
       "               1.2267e-01, -3.4894e-01],\n",
       "             ...,\n",
       "             [-1.5725e+00,  1.9465e+00,  1.6578e+00,  ..., -9.3065e-01,\n",
       "               1.0014e+00, -1.8643e-01],\n",
       "             [-1.3059e+00,  2.3995e+00,  5.2118e-02,  ..., -1.3595e+00,\n",
       "               3.1534e+00, -5.7495e-01],\n",
       "             [-1.6305e+00,  2.0022e+00,  1.6790e+00,  ..., -1.0458e+00,\n",
       "               1.0953e+00, -1.9653e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 7.2850e-02, -9.8556e-02,  6.3139e-02,  ...,  9.9210e-02,\n",
       "               5.8900e-02, -1.7060e-01],\n",
       "             [ 5.8704e-01,  1.0792e+00,  7.3844e-01,  ...,  2.3723e+00,\n",
       "               5.5190e-01,  8.6259e-01],\n",
       "             [ 1.6026e+00,  8.4365e-01, -1.4142e+00,  ...,  1.1797e+00,\n",
       "              -5.2442e-01,  1.8882e-02],\n",
       "             ...,\n",
       "             [ 2.1647e+00,  3.9805e-01, -1.2591e+00,  ...,  3.8199e-01,\n",
       "              -2.1057e+00, -9.5578e-02],\n",
       "             [ 2.2274e+00,  1.2014e+00, -2.0953e-01,  ...,  8.7797e-01,\n",
       "              -2.2200e+00,  5.7655e-01],\n",
       "             [ 2.1243e+00,  4.3669e-01, -1.2126e+00,  ...,  3.5034e-01,\n",
       "              -2.1348e+00, -1.6700e-01]],\n",
       "  \n",
       "            [[ 1.3089e-01, -1.3481e-02,  9.4816e-02,  ...,  3.0806e-02,\n",
       "               5.2371e-03, -1.3107e+00],\n",
       "             [ 1.1604e-01,  5.0168e-01,  1.2727e+00,  ..., -1.1964e+00,\n",
       "              -2.5043e-01,  2.5933e+00],\n",
       "             [-1.1752e+00, -6.4684e-02, -4.6941e-01,  ..., -1.4915e+00,\n",
       "               6.2559e-01,  2.9623e+00],\n",
       "             ...,\n",
       "             [-1.1976e+00,  1.1133e+00,  8.7948e-01,  ..., -5.4465e-01,\n",
       "               9.8191e-01, -1.7758e+00],\n",
       "             [-1.0035e+00,  9.2925e-01,  1.8043e+00,  ..., -1.1288e+00,\n",
       "               4.4028e-01, -1.9522e+00],\n",
       "             [-1.2144e+00,  1.1093e+00,  9.0577e-01,  ..., -5.4871e-01,\n",
       "               1.0409e+00, -2.0743e+00]],\n",
       "  \n",
       "            [[-2.7150e-01, -3.4428e-01, -7.1111e-02,  ...,  5.7818e-01,\n",
       "               4.7107e-01, -2.5103e-01],\n",
       "             [-1.9029e-01,  2.2406e+00,  6.1711e-01,  ..., -7.3203e-01,\n",
       "               3.6917e-02, -1.0914e-01],\n",
       "             [-6.8767e-01, -1.1247e+00, -6.2679e-01,  ..., -8.4440e-01,\n",
       "              -7.2531e-01,  6.7224e-01],\n",
       "             ...,\n",
       "             [-2.2645e+00, -2.3166e-01,  5.8330e-01,  ..., -3.9381e-01,\n",
       "              -2.5768e+00,  1.0639e+00],\n",
       "             [-1.5708e+00,  1.0621e+00,  1.7766e-01,  ..., -6.1433e-02,\n",
       "              -1.9710e+00,  9.7734e-01],\n",
       "             [-2.2770e+00, -2.7306e-01,  5.7732e-01,  ..., -2.9944e-01,\n",
       "              -2.6747e+00,  1.0706e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 1.7769e-01, -1.9868e-02,  3.6486e-02,  ...,  9.6218e-03,\n",
       "               6.7924e-02,  3.1965e-03],\n",
       "             [ 4.3672e-02,  4.0235e-02,  4.7228e-01,  ...,  8.4840e-01,\n",
       "               1.6675e-01,  3.4818e-01],\n",
       "             [-5.6196e-01, -4.6746e-01,  5.8312e-01,  ...,  6.6042e-01,\n",
       "              -5.3383e-01,  3.6734e-01],\n",
       "             ...,\n",
       "             [ 2.9145e-01,  2.3742e-01,  6.9251e-01,  ..., -3.0051e-02,\n",
       "              -6.0954e-01,  4.8869e-02],\n",
       "             [ 2.4541e-01,  4.7130e-01,  4.6727e-01,  ...,  8.7736e-01,\n",
       "               4.5745e-01, -6.9198e-02],\n",
       "             [ 3.2747e-01,  2.8414e-01,  6.3209e-01,  ..., -2.3813e-03,\n",
       "              -5.5888e-01,  7.7983e-02]],\n",
       "  \n",
       "            [[-4.0079e-02,  4.7627e-02, -2.0990e-01,  ..., -1.3484e-01,\n",
       "               3.0816e-02,  6.4082e-02],\n",
       "             [ 4.6453e-01, -4.0604e-01,  8.2549e-01,  ...,  1.5971e-01,\n",
       "               9.2278e-02,  9.9117e-01],\n",
       "             [ 5.3281e-01,  7.3788e-01,  2.8048e-01,  ...,  3.3176e-01,\n",
       "              -1.3099e+00, -2.4246e-01],\n",
       "             ...,\n",
       "             [ 3.0422e-01, -2.4027e-01, -5.5547e-01,  ..., -8.0873e-01,\n",
       "              -2.7658e-01, -2.8836e-02],\n",
       "             [-6.6797e-01,  3.9216e-02,  9.4326e-01,  ..., -6.2509e-02,\n",
       "               3.0423e-01,  4.2278e-01],\n",
       "             [ 3.3224e-01, -2.2553e-01, -5.4199e-01,  ..., -8.0184e-01,\n",
       "              -2.6850e-01, -8.1512e-02]],\n",
       "  \n",
       "            [[ 7.2285e-02, -3.1630e-02, -1.6655e-01,  ..., -8.9160e-02,\n",
       "              -2.0111e-01, -6.5453e-02],\n",
       "             [ 7.7623e-01,  6.3914e-01, -2.2372e-01,  ..., -1.4663e-01,\n",
       "               2.6967e-01, -3.3026e-01],\n",
       "             [ 1.9590e-01,  4.8709e-01, -1.5344e-01,  ..., -5.6191e-01,\n",
       "               1.9368e-02,  5.6774e-02],\n",
       "             ...,\n",
       "             [ 2.8809e-01,  5.4727e-02, -1.7595e-01,  ..., -1.3927e-01,\n",
       "               1.0250e-01, -1.7690e-01],\n",
       "             [ 2.5719e-01,  3.9549e-01,  1.2690e-01,  ...,  7.1759e-01,\n",
       "               6.5449e-01, -7.9504e-01],\n",
       "             [ 2.5784e-01,  2.2483e-02, -1.6551e-01,  ..., -1.5103e-01,\n",
       "               1.4847e-01, -1.6177e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-4.5902e-02, -9.6472e-02, -1.7944e-02,  ...,  6.7100e-02,\n",
       "              -6.8915e-03, -4.4650e-02],\n",
       "             [ 5.2386e-01,  1.1234e-01,  7.9435e-01,  ...,  8.2008e-01,\n",
       "              -1.4411e-02, -6.8194e-01],\n",
       "             [-6.9178e-01, -2.1721e-01, -4.8476e-02,  ..., -1.6245e+00,\n",
       "               5.4425e-01, -1.1310e-01],\n",
       "             ...,\n",
       "             [-9.9717e-01, -6.3572e-01, -5.6292e-01,  ..., -6.4437e-03,\n",
       "               3.7575e-01, -4.7072e-01],\n",
       "             [-6.0714e-01,  7.0789e-02,  3.4317e-01,  ...,  4.4393e-01,\n",
       "               5.5097e-01, -2.8036e-01],\n",
       "             [-1.0006e+00, -5.9933e-01, -5.5422e-01,  ..., -1.7138e-02,\n",
       "               3.6980e-01, -4.7932e-01]],\n",
       "  \n",
       "            [[-9.2612e-02,  1.9121e-02,  4.8481e-02,  ..., -2.0642e-02,\n",
       "              -1.2847e-01,  3.2826e-02],\n",
       "             [-2.6852e-01,  4.9444e-01, -6.7064e-01,  ...,  9.5245e-02,\n",
       "               2.3633e-01, -1.0949e+00],\n",
       "             [ 1.6142e-01,  1.0994e+00, -9.6061e-01,  ...,  1.2240e+00,\n",
       "              -3.3590e-01, -7.3955e-01],\n",
       "             ...,\n",
       "             [ 7.2288e-02, -4.5554e-02, -8.1182e-02,  ..., -6.1590e-01,\n",
       "              -2.8048e-01, -1.2142e+00],\n",
       "             [-2.2214e-01, -1.5382e-01, -6.1776e-01,  ...,  7.5930e-02,\n",
       "              -2.6566e-01, -1.3068e+00],\n",
       "             [ 5.4183e-02, -8.2436e-02, -3.4182e-02,  ..., -5.7980e-01,\n",
       "              -2.7879e-01, -1.2075e+00]],\n",
       "  \n",
       "            [[ 1.3356e-02, -1.4301e-01, -3.2877e-02,  ...,  4.6615e-03,\n",
       "              -4.0957e-02,  1.0125e-02],\n",
       "             [ 4.9277e-01,  1.4792e+00, -1.0851e+00,  ...,  1.6033e+00,\n",
       "              -6.9564e-01, -2.0382e-01],\n",
       "             [-7.9968e-01,  3.1177e-01, -1.1993e+00,  ...,  2.4692e-01,\n",
       "               7.2428e-02,  1.2972e-01],\n",
       "             ...,\n",
       "             [-3.5555e-01, -5.3964e-01, -1.0287e+00,  ..., -2.0806e-01,\n",
       "              -3.9654e-02,  1.2974e+00],\n",
       "             [ 6.7501e-01,  4.1135e-02, -1.1275e+00,  ...,  1.0525e+00,\n",
       "              -2.5639e-01,  1.0310e+00],\n",
       "             [-3.9419e-01, -5.3765e-01, -9.6850e-01,  ..., -2.4740e-01,\n",
       "              -1.8159e-03,  1.2894e+00]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 6.8537e-01,  2.1427e-01, -4.5352e-02,  ..., -3.5849e-03,\n",
       "              -2.8943e-02, -6.6344e-02],\n",
       "             [-1.6954e+00, -2.2446e+00,  8.0909e-01,  ...,  1.0270e+00,\n",
       "              -1.7578e+00, -3.4627e-01],\n",
       "             [ 1.2369e+00,  1.0438e-01,  1.9135e+00,  ..., -8.9543e-01,\n",
       "               1.1045e+00, -2.0674e+00],\n",
       "             ...,\n",
       "             [ 2.2138e-01,  1.4968e+00,  1.6126e+00,  ..., -1.6487e+00,\n",
       "               1.9645e-01, -2.2584e+00],\n",
       "             [-1.7065e+00, -2.3399e-01,  1.5907e+00,  ..., -7.9670e-01,\n",
       "              -9.3064e-01, -1.2596e+00],\n",
       "             [ 2.9130e-01,  1.5144e+00,  1.5056e+00,  ..., -1.6140e+00,\n",
       "               1.7768e-01, -2.1949e+00]],\n",
       "  \n",
       "            [[ 2.1328e-01, -1.8435e-01, -1.6182e-01,  ...,  5.5558e-02,\n",
       "               2.4471e-02,  1.4388e-01],\n",
       "             [ 7.7110e-01,  7.4403e-02, -5.3017e-01,  ..., -3.8544e-01,\n",
       "               1.2777e+00,  4.4977e-01],\n",
       "             [ 7.8479e-01,  2.9403e-01, -1.8430e-01,  ...,  2.9821e-01,\n",
       "               4.1774e-01,  7.2198e-01],\n",
       "             ...,\n",
       "             [ 6.2793e-01,  1.3856e+00, -7.5469e-01,  ...,  8.9061e-01,\n",
       "              -8.9901e-01, -3.8924e-01],\n",
       "             [ 1.1514e+00,  4.7978e-01, -5.7582e-01,  ...,  7.4075e-01,\n",
       "              -1.2570e+00, -9.9033e-01],\n",
       "             [ 7.0615e-01,  1.3269e+00, -8.2562e-01,  ...,  9.2904e-01,\n",
       "              -9.7526e-01, -3.4804e-01]],\n",
       "  \n",
       "            [[-8.6636e-02,  1.2201e-01,  1.5518e-02,  ..., -4.4909e-02,\n",
       "              -1.5601e-01, -1.0241e-01],\n",
       "             [ 8.8407e-01, -1.3988e-01, -3.1171e-01,  ..., -5.3115e-01,\n",
       "               5.9184e-01,  3.0933e+00],\n",
       "             [ 1.3505e+00,  7.0934e-01,  1.1093e+00,  ...,  4.7968e-01,\n",
       "              -7.9301e-02, -1.3391e+00],\n",
       "             ...,\n",
       "             [ 7.1144e-01, -9.2167e-01,  1.3211e+00,  ...,  6.1275e-01,\n",
       "               5.7988e-01, -1.3222e+00],\n",
       "             [ 5.3658e-01, -1.5493e+00,  6.5235e-01,  ...,  1.0666e+00,\n",
       "               5.0489e-01,  1.1325e-01],\n",
       "             [ 7.3646e-01, -1.0215e+00,  1.1981e+00,  ...,  6.8063e-01,\n",
       "               6.5228e-01, -1.3886e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 2.6494e-02,  7.8296e-01,  1.8132e-01,  ...,  7.4352e-02,\n",
       "               4.1801e-02, -1.4314e-01],\n",
       "             [-1.1518e+00, -2.9566e+00,  1.5535e+00,  ...,  7.1298e-01,\n",
       "               1.2990e+00, -1.9996e+00],\n",
       "             [-1.3578e+00, -1.6476e+00, -1.8125e-01,  ...,  1.2472e+00,\n",
       "               6.5089e-01, -1.2741e+00],\n",
       "             ...,\n",
       "             [-1.8210e+00,  8.9778e-01, -1.4465e+00,  ...,  1.1802e-01,\n",
       "              -1.3118e+00, -1.8856e+00],\n",
       "             [-2.3797e+00,  6.0693e-01, -7.0705e-01,  ...,  4.1585e-01,\n",
       "              -2.2993e-01, -2.1793e+00],\n",
       "             [-1.7358e+00,  1.0732e+00, -1.4490e+00,  ...,  6.9380e-02,\n",
       "              -1.3049e+00, -1.8447e+00]],\n",
       "  \n",
       "            [[ 2.0060e-01,  7.5593e-02, -2.8681e-02,  ...,  4.2734e-01,\n",
       "              -2.1390e-02, -7.7349e-02],\n",
       "             [-2.6036e+00, -5.6494e-01,  3.4092e-01,  ..., -1.9044e+00,\n",
       "               6.0452e-01,  7.2233e-01],\n",
       "             [ 8.2164e-01, -7.3673e-01, -5.6234e-01,  ..., -2.9345e+00,\n",
       "               1.5968e-01, -1.6516e+00],\n",
       "             ...,\n",
       "             [-4.9517e-01, -2.2784e-01,  4.8798e-01,  ...,  1.6577e+00,\n",
       "              -9.3673e-01, -1.1740e+00],\n",
       "             [-2.2997e+00, -2.5739e-01,  1.4306e-01,  ...,  2.1128e+00,\n",
       "               4.8050e-01,  9.2496e-01],\n",
       "             [-5.1131e-01, -2.2419e-01,  4.5266e-01,  ...,  1.8985e+00,\n",
       "              -9.4349e-01, -1.1182e+00]],\n",
       "  \n",
       "            [[-2.2082e-01,  2.9831e-01,  3.0861e-02,  ..., -1.0502e-01,\n",
       "              -1.8531e-02,  2.1701e-01],\n",
       "             [ 2.3554e+00, -1.2918e+00, -1.3569e+00,  ...,  6.8881e-01,\n",
       "               1.6159e+00,  9.3151e-01],\n",
       "             [ 9.6836e-01,  9.8888e-01, -1.3195e+00,  ...,  3.7121e-01,\n",
       "              -1.1329e+00, -3.6213e-01],\n",
       "             ...,\n",
       "             [ 7.5807e-01,  1.1804e+00,  1.0688e-01,  ..., -1.4068e+00,\n",
       "              -2.3224e+00, -8.0463e-01],\n",
       "             [ 1.7630e+00, -3.3871e-01,  1.2034e+00,  ..., -6.9985e-01,\n",
       "              -7.2815e-01,  1.8362e-01],\n",
       "             [ 6.9994e-01,  1.2109e+00,  2.3022e-01,  ..., -1.4998e+00,\n",
       "              -2.3277e+00, -7.7729e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-1.2125e-01,  2.2008e-02, -6.5314e-02,  ...,  9.2584e-02,\n",
       "               3.5623e-02, -3.7969e-04],\n",
       "             [ 2.0111e+00,  5.1528e-01,  4.3825e-01,  ...,  1.8468e+00,\n",
       "               7.3727e-01, -3.9190e-01],\n",
       "             [ 6.8265e-02,  8.6446e-01,  6.6308e-01,  ...,  4.7508e-01,\n",
       "              -7.2122e-01,  7.6963e-01],\n",
       "             ...,\n",
       "             [-3.6802e-01,  3.4177e-01,  7.8910e-01,  ...,  6.6477e-01,\n",
       "              -5.5517e-01,  6.0912e-01],\n",
       "             [ 1.0162e+00,  7.3033e-01,  3.6478e-01,  ...,  1.0292e+00,\n",
       "              -1.2334e-01,  6.9081e-02],\n",
       "             [-2.8647e-01,  3.2836e-01,  7.4716e-01,  ...,  6.4711e-01,\n",
       "              -5.1705e-01,  5.5538e-01]],\n",
       "  \n",
       "            [[-7.2903e-02,  4.9875e-02, -5.0968e-02,  ...,  5.6493e-02,\n",
       "              -7.1445e-02, -9.9216e-02],\n",
       "             [-3.3335e-02,  2.4220e-01,  3.5223e-02,  ...,  1.1298e-01,\n",
       "              -5.1401e-01, -2.3367e-01],\n",
       "             [-3.0075e-01,  8.0762e-01, -2.8344e-01,  ...,  1.5244e-01,\n",
       "              -1.1921e+00,  3.8304e-01],\n",
       "             ...,\n",
       "             [-2.3796e-02,  1.2921e+00, -1.0582e+00,  ...,  1.2896e+00,\n",
       "              -4.9185e-01,  4.8608e-01],\n",
       "             [-1.2282e+00,  5.2326e-01, -7.8570e-01,  ...,  1.9048e+00,\n",
       "              -2.1608e-01,  4.4203e-01],\n",
       "             [-8.1367e-02,  1.2290e+00, -1.0009e+00,  ...,  1.2673e+00,\n",
       "              -3.9385e-01,  4.7337e-01]],\n",
       "  \n",
       "            [[ 8.0475e-03, -1.6137e-02, -1.0294e-01,  ...,  7.2216e-02,\n",
       "              -1.7923e-01,  1.1483e-01],\n",
       "             [ 4.3647e-01, -1.1528e+00,  1.7065e+00,  ..., -4.8328e-01,\n",
       "               4.6387e-02,  5.5228e-01],\n",
       "             [ 2.8434e-01, -5.1202e-01,  2.5652e-01,  ..., -1.3514e+00,\n",
       "              -1.3213e+00, -7.1779e-01],\n",
       "             ...,\n",
       "             [ 4.5747e-01, -1.6742e-02, -1.9834e-01,  ..., -4.8238e-02,\n",
       "              -9.2763e-01, -6.0046e-02],\n",
       "             [-1.9865e-01, -3.8533e-01,  1.0194e+00,  ..., -8.0254e-03,\n",
       "              -1.0806e-01,  2.1529e-01],\n",
       "             [ 4.5575e-01,  3.2244e-02, -2.0221e-01,  ..., -1.0476e-01,\n",
       "              -9.0852e-01, -6.5058e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 9.4708e-02,  4.8268e-02, -1.7377e-01,  ...,  4.2699e-02,\n",
       "               1.0605e-01,  5.5429e-03],\n",
       "             [-4.3943e-01, -3.5992e-01,  1.0120e+00,  ..., -2.1329e-01,\n",
       "              -4.1847e-02,  4.4506e-01],\n",
       "             [ 6.3501e-01, -6.0898e-02, -1.4341e+00,  ...,  5.3269e-01,\n",
       "              -2.5241e-02, -1.1466e+00],\n",
       "             ...,\n",
       "             [-2.6318e-01, -1.2083e+00, -2.5827e+00,  ...,  1.7172e+00,\n",
       "               2.2955e+00, -1.0936e+00],\n",
       "             [-1.6644e+00, -1.1592e+00, -1.3508e+00,  ...,  1.0737e+00,\n",
       "               2.6780e+00, -8.1391e-01],\n",
       "             [-3.6019e-01, -1.1988e+00, -2.5079e+00,  ...,  1.6575e+00,\n",
       "               2.3004e+00, -1.0948e+00]],\n",
       "  \n",
       "            [[-1.6755e-02,  8.8640e-02, -2.5329e-01,  ...,  3.8905e-02,\n",
       "              -2.4743e-01,  1.7618e-02],\n",
       "             [ 8.9692e-01, -1.2612e-01,  1.3151e+00,  ...,  7.0664e-02,\n",
       "              -9.2286e-01, -1.3151e-01],\n",
       "             [ 8.4719e-01,  2.4142e-01,  2.6738e-01,  ..., -3.8943e-01,\n",
       "              -2.7978e-01, -5.1305e-01],\n",
       "             ...,\n",
       "             [ 1.3561e+00,  4.0042e-02,  1.0466e+00,  ..., -5.1884e-01,\n",
       "              -1.0089e+00,  7.1590e-01],\n",
       "             [ 8.0763e-01,  4.0774e-01,  1.5238e+00,  ...,  5.9635e-01,\n",
       "              -1.2650e+00,  7.1216e-01],\n",
       "             [ 1.2497e+00,  4.4024e-02,  1.0708e+00,  ..., -5.1786e-01,\n",
       "              -1.0033e+00,  6.9646e-01]],\n",
       "  \n",
       "            [[-7.2593e-03,  1.8752e-02, -1.1463e-01,  ..., -1.3021e-02,\n",
       "               7.3455e-02,  1.1168e-01],\n",
       "             [ 1.2464e-01, -4.2998e-01,  2.6983e-01,  ..., -3.5254e-01,\n",
       "               1.6719e-01, -6.7581e-01],\n",
       "             [ 7.8095e-01, -9.6077e-01,  4.0577e-01,  ...,  3.1818e-03,\n",
       "              -2.4233e-01,  3.1054e-01],\n",
       "             ...,\n",
       "             [ 1.5630e-01, -3.2900e-01, -8.1370e-02,  ..., -2.8281e-01,\n",
       "              -1.2955e-01,  4.1290e-01],\n",
       "             [ 1.3419e-01, -4.0031e-01, -1.2824e-01,  ...,  1.6867e-01,\n",
       "              -1.1820e-01, -8.9405e-01],\n",
       "             [ 1.2531e-01, -3.6874e-01, -9.4275e-02,  ..., -3.7238e-01,\n",
       "              -1.3620e-01,  5.1232e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-0.1725, -0.1518, -0.0961,  ...,  0.1084, -0.2766, -0.0596],\n",
       "             [ 0.7464, -0.5116, -0.6193,  ...,  1.3030,  0.3052, -0.9409],\n",
       "             [-2.5683,  0.0542, -2.2359,  ..., -0.3944, -1.0062, -0.2594],\n",
       "             ...,\n",
       "             [-3.9871,  0.6364, -0.6240,  ..., -1.3251, -0.6507,  1.5664],\n",
       "             [-3.0463,  0.1919, -0.4736,  ...,  0.2192, -0.3783,  1.0022],\n",
       "             [-4.0613,  0.6211, -0.5828,  ..., -1.3436, -0.6599,  1.5943]],\n",
       "  \n",
       "            [[-0.2496,  0.0091, -0.6572,  ...,  0.6731, -1.0039, -0.4439],\n",
       "             [ 0.6287, -0.6484, -1.1698,  ...,  0.9320, -0.3400,  0.8464],\n",
       "             [ 1.0040, -0.8979, -1.5295,  ...,  0.0955,  1.0207,  0.2944],\n",
       "             ...,\n",
       "             [ 1.4279, -3.2427,  0.1518,  ...,  3.0343,  3.0296,  0.3985],\n",
       "             [ 0.8271, -3.7780,  0.5777,  ...,  2.9330,  2.2817,  1.5809],\n",
       "             [ 1.4319, -3.2761,  0.1365,  ...,  3.0843,  3.0228,  0.3730]],\n",
       "  \n",
       "            [[-0.2305, -1.0570, -0.0190,  ..., -0.9610,  0.0973, -0.1769],\n",
       "             [ 1.1877,  1.6812, -0.0397,  ...,  2.8911,  0.3245,  1.9362],\n",
       "             [ 2.5933, -0.0926,  0.6095,  ...,  2.9049,  0.1403,  0.3236],\n",
       "             ...,\n",
       "             [ 0.6820, -2.9862,  1.4882,  ..., -1.5353, -1.4353,  2.3769],\n",
       "             [ 1.2176, -2.6057,  0.9623,  ..., -1.6224, -1.4099,  3.5245],\n",
       "             [ 0.7311, -3.1263,  1.5489,  ..., -1.6933, -1.4675,  2.4115]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-0.1501, -0.1770,  0.0240,  ..., -0.0088, -0.1996, -0.1223],\n",
       "             [-0.1600, -0.7143,  0.5969,  ..., -0.1437,  0.9807,  0.5207],\n",
       "             [-1.0315, -0.1915, -0.9737,  ..., -0.1807,  2.7774, -0.5854],\n",
       "             ...,\n",
       "             [ 2.5506,  2.5869,  2.4115,  ...,  3.3484, -1.0548, -0.9053],\n",
       "             [ 2.2443,  2.0326,  2.3781,  ...,  2.7347, -1.9070, -0.1328],\n",
       "             [ 2.6559,  2.6714,  2.5148,  ...,  3.4722, -1.2035, -0.9089]],\n",
       "  \n",
       "            [[-0.0109, -0.3856,  0.1653,  ..., -0.0477,  0.0577,  0.1044],\n",
       "             [ 1.6053, -0.7605,  0.1032,  ...,  0.5636, -0.1899,  0.6509],\n",
       "             [ 2.5088, -2.8085, -0.4755,  ..., -0.2166, -0.8767,  0.8165],\n",
       "             ...,\n",
       "             [ 1.6020, -0.7100, -0.6736,  ...,  1.9320, -1.6346,  2.1945],\n",
       "             [ 2.0030,  0.4486, -0.3469,  ...,  1.6738, -1.8228,  2.1408],\n",
       "             [ 1.5387, -0.6330, -0.6183,  ...,  1.9330, -1.7079,  2.2002]],\n",
       "  \n",
       "            [[-0.0486, -0.1727,  0.0658,  ...,  0.0131, -0.0802,  0.3659],\n",
       "             [-0.2701, -0.3370,  1.0234,  ..., -0.0816, -0.0574, -1.3917],\n",
       "             [ 1.1309,  0.0310,  0.8943,  ...,  0.0368,  0.9252,  1.6798],\n",
       "             ...,\n",
       "             [ 0.9999, -1.8376,  0.3742,  ...,  1.1552, -0.9392, -0.5580],\n",
       "             [ 0.3958, -2.2266,  1.0668,  ...,  1.7600, -1.6932, -2.6588],\n",
       "             [ 0.9568, -1.8587,  0.3589,  ...,  1.2013, -0.9659, -0.7194]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 0.0312, -0.0368,  0.0323,  ..., -0.2076, -0.1007,  0.1123],\n",
       "             [ 0.3485,  0.0500, -1.0935,  ..., -1.1667,  0.0831, -1.5754],\n",
       "             [-0.3529,  0.7031, -1.1933,  ..., -1.1075, -0.5890,  0.1130],\n",
       "             ...,\n",
       "             [ 0.2540,  0.0776, -0.8328,  ..., -0.2318,  0.0531, -0.4256],\n",
       "             [-0.3853,  0.2326, -0.9210,  ..., -0.0453,  0.1055, -0.6854],\n",
       "             [ 0.2738,  0.0930, -0.8134,  ..., -0.2573,  0.0633, -0.4171]],\n",
       "  \n",
       "            [[ 0.0123, -0.0266, -0.0128,  ..., -0.0382, -0.0596,  0.0701],\n",
       "             [-0.4253,  1.3090, -1.0143,  ..., -0.1980,  0.4484,  0.7133],\n",
       "             [ 0.5735,  1.3106, -0.2318,  ..., -0.0358,  1.3607, -0.2642],\n",
       "             ...,\n",
       "             [ 0.3378,  0.7209, -0.2407,  ..., -0.0363,  0.4884,  0.2024],\n",
       "             [-0.2335,  0.4285, -0.8415,  ..., -0.0657, -0.2582,  1.2119],\n",
       "             [ 0.3568,  0.6982, -0.2684,  ...,  0.0369,  0.4604,  0.2416]],\n",
       "  \n",
       "            [[ 0.0264,  0.0569, -0.0903,  ...,  0.0139,  0.0220, -0.0556],\n",
       "             [-0.0226,  1.1313, -0.4856,  ...,  0.4479,  0.1301,  0.1747],\n",
       "             [-0.3229,  0.2677,  0.9708,  ..., -1.2685,  1.6110, -1.1742],\n",
       "             ...,\n",
       "             [-1.0428,  0.4100, -0.0278,  ...,  0.1193,  0.4352,  0.0973],\n",
       "             [-0.3195,  0.4140, -0.4357,  ...,  0.5867,  0.2708,  0.0982],\n",
       "             [-1.0422,  0.3663, -0.0104,  ...,  0.1104,  0.4047,  0.0441]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 0.1105,  0.1602,  0.0478,  ...,  0.0070, -0.1864,  0.0194],\n",
       "             [-1.1568,  0.4072,  1.7580,  ..., -0.8772,  0.4214, -1.2068],\n",
       "             [-0.6857,  0.0191,  0.4976,  ..., -0.5413,  0.6075,  0.7398],\n",
       "             ...,\n",
       "             [-0.1869, -0.6568,  0.7816,  ...,  0.0395,  0.2143,  0.5802],\n",
       "             [-1.2585, -0.7074,  1.2800,  ..., -0.2988,  0.5696, -0.3933],\n",
       "             [-0.2174, -0.6372,  0.7504,  ...,  0.0530,  0.2524,  0.6088]],\n",
       "  \n",
       "            [[ 0.0688, -0.0814,  0.1127,  ..., -0.0949,  0.0803, -0.0690],\n",
       "             [-0.8122,  0.0583,  0.6352,  ...,  0.4150, -0.2034,  0.4368],\n",
       "             [ 0.0730, -0.6045,  0.1518,  ...,  0.0512,  0.0130,  0.2126],\n",
       "             ...,\n",
       "             [-0.0592, -1.2808,  0.9065,  ..., -0.2051,  0.4805, -0.5722],\n",
       "             [-0.9348, -0.1863,  0.6156,  ...,  0.7969, -0.4455,  0.7270],\n",
       "             [-0.0547, -1.2175,  0.9086,  ..., -0.2053,  0.4505, -0.5660]],\n",
       "  \n",
       "            [[-0.1646, -0.1855, -0.0153,  ..., -0.1758, -0.0934, -0.1412],\n",
       "             [-0.2879, -0.2852,  0.3780,  ..., -0.8950,  1.0840,  0.0219],\n",
       "             [-0.5403, -0.8192, -0.3513,  ...,  0.0673,  0.3819,  0.3494],\n",
       "             ...,\n",
       "             [-0.5440, -0.7614,  0.0929,  ..., -0.3380,  0.9736,  0.5806],\n",
       "             [-0.5529, -0.5317,  0.1874,  ..., -0.5057,  0.8674, -0.0787],\n",
       "             [-0.5204, -0.7341,  0.0621,  ..., -0.3713,  0.9285,  0.5746]]]]],\n",
       "         grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 5.6166e-02, -4.2002e-01,  3.8002e-01,  ...,  3.8938e-01,\n",
       "              -4.0163e-01,  2.1940e-01],\n",
       "             [-9.0036e-01,  2.5164e-01,  1.0827e+00,  ...,  8.3210e-01,\n",
       "               1.2399e+00, -5.2130e-01],\n",
       "             [-1.2126e+00, -4.3218e-01, -8.0373e-01,  ...,  3.1813e-01,\n",
       "               2.3243e+00, -8.6250e-01],\n",
       "             ...,\n",
       "             [ 1.0634e+00,  3.6366e+00, -4.3412e+00,  ...,  2.6793e+00,\n",
       "              -1.2157e+00,  1.7478e+00],\n",
       "             [ 1.1861e+00,  3.9963e+00, -3.7035e+00,  ...,  2.0344e+00,\n",
       "              -5.5688e-01,  1.3168e+00],\n",
       "             [ 1.1166e+00,  3.7670e+00, -4.3342e+00,  ...,  2.7086e+00,\n",
       "              -1.2816e+00,  1.7340e+00]],\n",
       "  \n",
       "            [[ 1.5011e+00, -2.4253e-01, -1.8437e+00,  ..., -1.6119e+00,\n",
       "              -2.3475e+00,  1.6712e-01],\n",
       "             [ 1.2258e+00,  8.9521e-01,  9.8843e-01,  ...,  3.5756e-01,\n",
       "               1.7873e-01, -6.6019e-01],\n",
       "             [-1.1977e-01, -1.6766e+00,  1.5651e-01,  ..., -2.2401e+00,\n",
       "               2.1953e+00,  7.0043e-01],\n",
       "             ...,\n",
       "             [ 5.7594e-01, -3.2278e+00, -8.8773e-01,  ..., -2.5146e+00,\n",
       "              -1.4448e-01, -7.8484e-01],\n",
       "             [ 1.0864e+00, -1.9657e+00, -1.2011e-02,  ..., -9.0885e-01,\n",
       "              -7.2953e-01, -2.3621e+00],\n",
       "             [ 6.5725e-01, -3.2337e+00, -9.7704e-01,  ..., -2.4551e+00,\n",
       "              -3.2943e-01, -9.6910e-01]],\n",
       "  \n",
       "            [[ 1.2856e+00, -1.9217e-01, -9.8805e-01,  ..., -2.8358e-01,\n",
       "              -7.0646e-01, -5.0923e-02],\n",
       "             [ 2.4959e-01, -7.2976e-01,  1.9933e+00,  ...,  6.2721e-01,\n",
       "               4.9795e-01,  1.4856e+00],\n",
       "             [ 1.2986e+00, -2.7844e+00, -9.0845e-01,  ...,  6.6660e-01,\n",
       "               3.2374e-02,  9.0445e-01],\n",
       "             ...,\n",
       "             [ 6.4543e-01, -1.5075e+00, -2.7834e+00,  ...,  3.0820e+00,\n",
       "              -5.0923e+00, -2.9048e+00],\n",
       "             [ 5.0992e-01, -1.7386e+00, -1.8178e+00,  ...,  3.2099e+00,\n",
       "              -5.4081e+00, -3.0420e+00],\n",
       "             [ 6.7985e-01, -1.4265e+00, -2.8677e+00,  ...,  3.2350e+00,\n",
       "              -5.1615e+00, -3.0517e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 5.9247e-01, -8.0191e-02, -4.8320e-01,  ..., -4.2196e-02,\n",
       "               1.2568e-01, -1.1414e-01],\n",
       "             [ 1.4345e-01, -1.2341e-01, -2.9809e-01,  ..., -2.7742e-01,\n",
       "              -1.0967e+00,  1.4368e+00],\n",
       "             [-1.3090e+00, -7.5311e-01, -8.4933e-01,  ..., -3.0142e-01,\n",
       "              -5.3921e-01, -9.8285e-01],\n",
       "             ...,\n",
       "             [-2.0666e+00,  3.3036e-01, -3.5907e-01,  ...,  1.8929e-01,\n",
       "              -3.3404e+00, -1.5453e+00],\n",
       "             [-1.3719e+00,  3.4992e-01, -5.9277e-01,  ..., -4.4082e-01,\n",
       "              -3.1663e+00, -1.3159e+00],\n",
       "             [-1.9634e+00,  4.2107e-01, -3.5189e-01,  ...,  1.6480e-01,\n",
       "              -3.4214e+00, -1.5606e+00]],\n",
       "  \n",
       "            [[ 2.7973e-01,  6.7818e-01, -1.7939e-01,  ..., -2.2796e-01,\n",
       "              -7.6827e-02, -4.5045e-01],\n",
       "             [ 8.6182e-01, -2.2256e+00,  1.8621e+00,  ...,  1.5628e+00,\n",
       "              -1.3158e+00,  1.3275e+00],\n",
       "             [ 2.3336e+00, -2.0955e-01,  1.7138e-01,  ...,  1.7505e+00,\n",
       "               5.6696e-02,  1.0674e+00],\n",
       "             ...,\n",
       "             [-4.1930e+00, -7.0871e-01,  2.0013e+00,  ...,  1.5869e+00,\n",
       "               2.0522e+00, -4.4835e-01],\n",
       "             [-4.8349e+00, -1.9820e+00,  1.4516e+00,  ...,  1.1793e+00,\n",
       "               1.2066e+00, -5.4100e-01],\n",
       "             [-4.3638e+00, -7.4093e-01,  2.0279e+00,  ...,  1.6074e+00,\n",
       "               2.0619e+00, -5.4973e-01]],\n",
       "  \n",
       "            [[-2.1772e-01, -1.0921e-01, -3.6384e-02,  ...,  1.1472e-01,\n",
       "               1.4872e-01,  7.3530e-02],\n",
       "             [-1.8979e-01,  5.0915e-02, -1.9807e-01,  ..., -1.1196e+00,\n",
       "               1.6558e-01, -3.7173e-01],\n",
       "             [-4.4602e-01,  6.2886e-01,  1.0505e+00,  ...,  3.5230e-01,\n",
       "               1.0677e+00,  1.1299e+00],\n",
       "             ...,\n",
       "             [-1.7302e-02, -6.5697e-01, -1.4642e+00,  ...,  3.7024e-01,\n",
       "              -1.2113e+00,  1.9589e+00],\n",
       "             [ 1.8845e-01, -2.6722e-01, -1.1788e+00,  ...,  8.1768e-01,\n",
       "              -7.9566e-01,  1.1179e+00],\n",
       "             [-4.7589e-03, -7.2274e-01, -1.4505e+00,  ...,  4.2071e-01,\n",
       "              -1.1965e+00,  1.8555e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 2.8610e-02,  1.2853e-02, -1.2269e-01,  ..., -1.1257e-02,\n",
       "               2.9263e-02, -1.5723e-01],\n",
       "             [-2.0106e-01,  2.7768e-01,  2.5367e+00,  ...,  3.7636e-01,\n",
       "              -5.3303e-02, -7.3146e-02],\n",
       "             [-7.5599e-02, -1.8415e-02,  5.8910e-01,  ...,  3.6737e-01,\n",
       "              -3.0556e-01,  3.4870e-01],\n",
       "             ...,\n",
       "             [-2.5798e-01,  6.6433e-01,  4.4798e-02,  ...,  4.2006e-01,\n",
       "               2.4894e-01,  2.7327e-02],\n",
       "             [ 2.0591e-01, -5.9180e-02,  6.9254e-01,  ...,  8.0513e-01,\n",
       "               1.4813e-01, -2.2537e-01],\n",
       "             [-2.3338e-01,  7.0667e-01,  4.6292e-02,  ...,  4.4014e-01,\n",
       "               2.7583e-01,  5.9266e-03]],\n",
       "  \n",
       "            [[ 4.5970e-02, -7.6100e-02, -3.1406e-02,  ...,  4.2137e-03,\n",
       "               2.9127e-02,  8.3232e-02],\n",
       "             [-6.6416e-01,  1.1792e+00,  7.2355e-01,  ...,  6.1842e-01,\n",
       "               1.3291e+00,  1.3557e-01],\n",
       "             [-1.0069e-01,  3.1926e-01,  3.5223e-01,  ..., -3.6035e-01,\n",
       "               6.9251e-01, -4.2601e-01],\n",
       "             ...,\n",
       "             [-6.4514e-03,  4.0228e-01,  6.0551e-01,  ...,  4.1727e-01,\n",
       "              -3.4731e-01, -7.8465e-01],\n",
       "             [-6.6201e-01,  8.0433e-01,  5.1797e-01,  ...,  6.3021e-01,\n",
       "               4.6219e-01,  1.2468e-01],\n",
       "             [ 4.2583e-02,  4.0989e-01,  6.0239e-01,  ...,  3.7964e-01,\n",
       "              -3.5404e-01, -7.1969e-01]],\n",
       "  \n",
       "            [[ 4.0437e-02, -3.9120e-03, -1.8649e-01,  ...,  1.9926e-02,\n",
       "               1.0219e-01,  2.2396e-02],\n",
       "             [ 4.3508e-01,  1.3748e+00, -1.8874e-02,  ...,  7.6127e-01,\n",
       "              -2.5055e-02,  7.8726e-01],\n",
       "             [ 5.1185e-01,  1.2991e+00, -6.2457e-03,  ...,  5.4653e-01,\n",
       "               5.6407e-01, -9.0313e-01],\n",
       "             ...,\n",
       "             [-2.9639e-01,  4.4766e-01, -8.5618e-01,  ..., -4.7889e-01,\n",
       "               4.4166e-01,  9.8398e-02],\n",
       "             [ 6.7280e-02,  9.1599e-02, -3.8354e-01,  ...,  7.7066e-01,\n",
       "              -1.1416e-02,  7.9639e-01],\n",
       "             [-2.6635e-01,  4.1709e-01, -8.4446e-01,  ..., -4.8175e-01,\n",
       "               4.1025e-01,  7.1735e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.9750e-01,  6.8529e-03, -1.4536e-02,  ...,  3.4890e-03,\n",
       "               1.0463e-01,  7.3523e-02],\n",
       "             [ 1.1149e+00,  8.2626e-01,  1.5518e-02,  ...,  9.0221e-01,\n",
       "              -4.6150e-01, -7.2491e-01],\n",
       "             [-7.0829e-01,  3.8576e-02,  4.5917e-01,  ...,  1.3060e-01,\n",
       "              -4.5200e-01,  6.5450e-02],\n",
       "             ...,\n",
       "             [ 1.6055e-01, -4.4959e-02, -6.3658e-01,  ..., -3.9343e-02,\n",
       "               7.7898e-02,  4.1651e-02],\n",
       "             [ 7.6647e-01,  3.1331e-01, -7.0777e-01,  ...,  2.7662e-01,\n",
       "              -3.3727e-01, -7.9312e-01],\n",
       "             [ 2.2596e-01,  2.5328e-02, -6.5446e-01,  ..., -3.9717e-02,\n",
       "               6.9850e-02,  4.4686e-02]],\n",
       "  \n",
       "            [[-9.8877e-02, -4.6599e-03, -3.8561e-03,  ...,  1.1805e-01,\n",
       "               9.6648e-02, -5.6808e-02],\n",
       "             [ 5.3670e-01, -5.6381e-01,  4.5461e-01,  ...,  5.7934e-01,\n",
       "               8.2111e-02, -4.6666e-01],\n",
       "             [-9.4565e-01, -5.2629e-02, -2.8324e-02,  ...,  6.6512e-01,\n",
       "               2.9557e-01, -4.9475e-01],\n",
       "             ...,\n",
       "             [ 2.4102e-01, -9.1359e-01,  1.8441e-01,  ...,  4.5317e-01,\n",
       "              -1.4818e-01, -7.8914e-01],\n",
       "             [ 4.9549e-01, -1.6623e-01,  3.3844e-01,  ...,  4.1715e-01,\n",
       "               2.2420e-01, -3.4012e-01],\n",
       "             [ 2.2019e-01, -9.0551e-01,  1.9570e-01,  ...,  4.5624e-01,\n",
       "              -1.2058e-01, -7.7331e-01]],\n",
       "  \n",
       "            [[ 1.3056e-01,  2.2470e-02,  5.5132e-02,  ..., -9.6777e-03,\n",
       "               4.2867e-02,  8.0464e-02],\n",
       "             [-7.1081e-01, -5.3793e-01,  7.0893e-02,  ...,  1.6059e+00,\n",
       "               8.8559e-01,  1.6009e+00],\n",
       "             [-7.5900e-01,  3.2864e-01,  2.9079e-01,  ...,  6.1201e-01,\n",
       "               1.7177e-01, -4.9661e-01],\n",
       "             ...,\n",
       "             [ 2.3875e-01,  7.7463e-01, -8.4165e-01,  ..., -3.9675e-01,\n",
       "              -4.2066e-01, -5.9187e-02],\n",
       "             [ 6.4349e-02,  3.0666e-01, -4.1727e-01,  ...,  6.6901e-01,\n",
       "               3.8666e-01,  4.8330e-01],\n",
       "             [ 1.9610e-01,  7.6599e-01, -8.8106e-01,  ..., -3.4469e-01,\n",
       "              -4.2299e-01, -3.2146e-02]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-8.0509e-01, -1.5786e-01,  4.0455e-01,  ...,  5.1511e-02,\n",
       "               7.3780e-01, -2.3081e-01],\n",
       "             [ 2.0069e+00, -1.2103e+00, -3.2134e-01,  ...,  2.0463e+00,\n",
       "              -1.7248e+00, -8.9961e-02],\n",
       "             [ 1.1574e+00, -1.6836e+00,  2.9710e-03,  ...,  1.5643e+00,\n",
       "              -8.6848e-01, -2.4166e-01],\n",
       "             ...,\n",
       "             [-2.2699e+00, -1.2972e+00, -4.7365e+00,  ..., -1.1557e+00,\n",
       "              -1.4084e+00,  4.0186e+00],\n",
       "             [-2.7211e+00, -1.4570e+00, -4.5784e+00,  ..., -8.5334e-02,\n",
       "              -2.4154e+00,  3.2588e+00],\n",
       "             [-2.3536e+00, -1.3064e+00, -4.8190e+00,  ..., -1.1959e+00,\n",
       "              -1.4213e+00,  4.0587e+00]],\n",
       "  \n",
       "            [[-9.3736e-01, -5.2172e-02,  3.7902e-01,  ...,  2.6934e-01,\n",
       "              -7.0688e-01,  4.3571e-01],\n",
       "             [-4.7712e-01, -1.7614e+00,  1.4042e+00,  ..., -1.9707e+00,\n",
       "               1.1611e+00, -1.2010e+00],\n",
       "             [ 1.8771e+00,  1.2277e+00, -8.7057e-01,  ..., -1.0569e-01,\n",
       "               2.4207e-01, -1.4343e+00],\n",
       "             ...,\n",
       "             [ 2.3636e+00,  1.5779e+00,  1.3397e+00,  ...,  3.4857e+00,\n",
       "               7.2268e-01,  1.8681e+00],\n",
       "             [ 1.8394e+00,  4.1946e-01,  1.4512e+00,  ...,  1.6969e+00,\n",
       "               1.3930e+00,  1.2559e+00],\n",
       "             [ 2.3054e+00,  1.4767e+00,  1.3321e+00,  ...,  3.3702e+00,\n",
       "               7.6097e-01,  1.8429e+00]],\n",
       "  \n",
       "            [[ 1.2431e-01, -1.9596e-01, -3.7100e-01,  ..., -2.9573e-01,\n",
       "               1.2071e-01, -1.8707e-01],\n",
       "             [-6.1152e-01, -1.1212e+00,  7.7419e-01,  ..., -7.6331e-02,\n",
       "              -1.2396e+00,  2.1420e+00],\n",
       "             [ 1.0026e+00, -8.8863e-01,  1.1224e+00,  ...,  4.6487e-01,\n",
       "               1.1094e+00,  1.5170e+00],\n",
       "             ...,\n",
       "             [ 2.7676e+00,  9.1408e-01,  4.2194e-01,  ...,  2.1513e+00,\n",
       "              -1.1351e+00,  4.1803e+00],\n",
       "             [ 2.0050e+00,  6.5823e-01,  5.1451e-01,  ...,  2.4423e+00,\n",
       "              -2.2873e+00,  4.5584e+00],\n",
       "             [ 2.7651e+00,  9.9362e-01,  3.5207e-01,  ...,  2.1813e+00,\n",
       "              -1.2712e+00,  4.2558e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.1101e+00, -1.4262e+00,  1.3988e+00,  ...,  1.5434e+00,\n",
       "               4.4440e-01,  1.8507e+00],\n",
       "             [ 4.2952e-01,  6.6493e-01, -4.2849e-01,  ...,  1.0792e+00,\n",
       "               1.2027e+00,  1.5726e+00],\n",
       "             [-1.9389e-01, -8.9660e-01,  1.8397e+00,  ..., -1.0483e+00,\n",
       "              -7.0499e-01,  1.8053e+00],\n",
       "             ...,\n",
       "             [-5.1135e+00, -7.6571e-01,  1.1407e+00,  ..., -2.9861e+00,\n",
       "               2.8615e+00,  1.0116e+00],\n",
       "             [-6.4364e+00,  2.7259e-01,  6.3638e-01,  ..., -1.7919e+00,\n",
       "               4.2762e+00,  7.6588e-01],\n",
       "             [-5.3074e+00, -7.0848e-01,  1.2337e+00,  ..., -2.9743e+00,\n",
       "               2.9812e+00,  9.4407e-01]],\n",
       "  \n",
       "            [[-2.6173e-01,  1.5671e-01, -2.8757e-01,  ..., -3.8101e-01,\n",
       "              -2.0463e-01,  9.5139e-02],\n",
       "             [-1.8710e+00, -7.0214e-01,  1.0967e+00,  ...,  3.2663e-01,\n",
       "              -1.6948e-02,  9.2062e-01],\n",
       "             [-1.9617e+00, -4.0355e-01, -1.0796e-01,  ..., -5.0139e-01,\n",
       "              -1.1405e-01,  1.2661e-01],\n",
       "             ...,\n",
       "             [ 1.1409e+00, -2.0893e+00, -2.8737e+00,  ..., -4.1695e+00,\n",
       "              -5.0880e+00, -3.1322e+00],\n",
       "             [ 4.7062e-01, -1.8036e+00, -1.9398e+00,  ..., -3.6469e+00,\n",
       "              -4.8252e+00, -2.7708e+00],\n",
       "             [ 1.1975e+00, -2.1957e+00, -2.9622e+00,  ..., -4.2747e+00,\n",
       "              -5.1553e+00, -3.2145e+00]],\n",
       "  \n",
       "            [[-1.7156e-01, -5.8267e-02,  7.7460e-01,  ...,  3.0813e-01,\n",
       "               7.6626e-01, -1.3414e-01],\n",
       "             [-1.4913e+00,  4.0281e-01, -1.0599e+00,  ...,  1.0718e+00,\n",
       "              -1.2053e+00,  1.2962e-02],\n",
       "             [-1.0439e+00, -1.8174e+00,  1.0110e+00,  ...,  1.6480e+00,\n",
       "              -2.0945e+00, -1.0982e+00],\n",
       "             ...,\n",
       "             [-2.2487e+00,  2.8088e+00,  8.0010e+00,  ...,  6.3947e-02,\n",
       "              -4.6936e+00,  1.1263e+00],\n",
       "             [-2.5354e+00,  3.3772e+00,  7.4754e+00,  ...,  5.1553e-01,\n",
       "              -4.7355e+00,  1.6568e+00],\n",
       "             [-2.3555e+00,  2.9145e+00,  8.1140e+00,  ..., -7.9245e-02,\n",
       "              -4.6512e+00,  1.2580e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 5.8372e-02,  3.1311e-02, -1.4783e-01,  ...,  1.1438e-01,\n",
       "               1.3060e-02,  2.3378e-01],\n",
       "             [ 8.4837e-01,  2.4797e-01, -1.5988e+00,  ..., -1.0958e+00,\n",
       "               5.3335e-01, -8.2253e-02],\n",
       "             [-1.1586e-01, -1.2235e-01, -8.2173e-01,  ..., -8.7066e-01,\n",
       "               5.5435e-01,  3.5810e-01],\n",
       "             ...,\n",
       "             [ 1.4167e-01,  5.4759e-01, -2.4479e-01,  ..., -5.1462e-01,\n",
       "               3.6379e-01,  5.7067e-01],\n",
       "             [ 4.0622e-01,  5.5799e-01, -4.2934e-01,  ..., -8.4903e-01,\n",
       "               4.0601e-01,  3.4820e-01],\n",
       "             [ 1.3251e-01,  5.3892e-01, -1.9960e-01,  ..., -4.7376e-01,\n",
       "               3.4650e-01,  5.9987e-01]],\n",
       "  \n",
       "            [[ 5.3258e-02,  6.8870e-02,  1.0893e-01,  ...,  5.2471e-02,\n",
       "               2.4778e-02, -1.1068e-01],\n",
       "             [-4.8206e-01, -7.2946e-01,  3.7010e-01,  ..., -1.4647e-01,\n",
       "               1.0994e+00, -5.5253e-01],\n",
       "             [ 1.0664e-01,  6.6453e-01, -1.1115e-01,  ...,  1.1596e+00,\n",
       "              -1.2741e+00, -5.4519e-01],\n",
       "             ...,\n",
       "             [-2.2164e-01,  1.0101e+00,  1.4202e-01,  ...,  3.0003e-01,\n",
       "               5.4102e-01, -1.6095e-01],\n",
       "             [-4.7768e-01,  3.5226e-01,  6.7886e-01,  ..., -3.2422e-01,\n",
       "               6.7535e-01, -1.3981e+00],\n",
       "             [-2.1656e-01,  9.7024e-01,  1.8375e-01,  ...,  2.6772e-01,\n",
       "               5.7848e-01, -1.5512e-01]],\n",
       "  \n",
       "            [[-3.3564e-02, -1.6812e-02, -2.7630e-02,  ..., -2.0661e-01,\n",
       "              -2.0624e-02,  1.5071e-01],\n",
       "             [-1.8632e-02,  2.6415e+00, -9.8896e-01,  ..., -2.5421e-01,\n",
       "              -2.5654e-01, -1.0942e+00],\n",
       "             [ 4.3388e-01,  1.3108e+00, -8.4434e-01,  ..., -9.1371e-01,\n",
       "               6.9896e-01, -4.4035e-01],\n",
       "             ...,\n",
       "             [-2.0192e-01,  4.9667e-01, -5.9729e-02,  ..., -2.8124e-01,\n",
       "               4.8397e-01, -1.0883e+00],\n",
       "             [-4.9207e-01,  8.5241e-01,  2.2433e-01,  ..., -1.7091e-01,\n",
       "              -8.6071e-02, -1.4243e+00],\n",
       "             [-2.1454e-01,  4.5218e-01, -6.6088e-02,  ..., -2.8687e-01,\n",
       "               4.1840e-01, -1.0410e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 3.5601e-02, -8.6612e-02,  4.3746e-02,  ..., -1.2505e-01,\n",
       "               1.2904e-02, -6.3012e-02],\n",
       "             [ 2.0843e-01,  1.2784e+00, -1.0727e-01,  ...,  4.7901e-03,\n",
       "              -2.2344e-01, -8.6769e-01],\n",
       "             [ 2.0866e-01,  2.9216e-01,  1.1828e+00,  ...,  1.3029e-01,\n",
       "              -7.2860e-01, -5.2975e-01],\n",
       "             ...,\n",
       "             [ 8.6252e-01,  1.8769e-01, -3.4279e-02,  ...,  2.3206e-01,\n",
       "              -1.0320e-01, -2.3436e-01],\n",
       "             [-5.9651e-03,  3.8795e-01,  4.9396e-01,  ..., -6.1235e-01,\n",
       "               5.2938e-01, -1.1389e+00],\n",
       "             [ 7.7963e-01,  2.0020e-01, -3.7951e-02,  ...,  2.3512e-01,\n",
       "              -1.3809e-01, -2.2619e-01]],\n",
       "  \n",
       "            [[-8.3317e-02, -1.1641e-01,  1.3257e-01,  ..., -2.0688e-02,\n",
       "              -4.3591e-02,  1.5613e-01],\n",
       "             [-8.7765e-01, -1.8110e-01,  9.0772e-04,  ...,  3.4550e-01,\n",
       "               4.3784e-01, -8.5332e-01],\n",
       "             [-5.8751e-01,  4.5552e-01, -1.4623e-01,  ..., -3.3811e-01,\n",
       "               7.7644e-01, -1.4031e-02],\n",
       "             ...,\n",
       "             [-7.8987e-01,  2.7637e-01, -1.8619e-01,  ...,  7.2958e-01,\n",
       "               3.9757e-01, -2.3940e-01],\n",
       "             [-8.4212e-01,  2.0331e-02,  1.6109e-01,  ..., -9.0122e-02,\n",
       "               8.0954e-01, -2.9008e-01],\n",
       "             [-7.9814e-01,  3.0385e-01, -1.7667e-01,  ...,  7.3262e-01,\n",
       "               3.8308e-01, -1.7138e-01]],\n",
       "  \n",
       "            [[-1.0493e-01, -6.0311e-02, -1.7198e-02,  ...,  1.2304e-01,\n",
       "              -1.2854e-01,  2.3725e-03],\n",
       "             [ 1.2170e-02,  8.6749e-01,  1.5740e+00,  ...,  1.1157e+00,\n",
       "               3.6348e-01,  2.1335e-01],\n",
       "             [-4.7504e-01, -3.1139e-01, -2.4286e-02,  ...,  2.8995e-01,\n",
       "               9.6717e-01, -8.7367e-02],\n",
       "             ...,\n",
       "             [-1.6668e-01, -3.2424e-01,  5.3034e-02,  ..., -1.2974e-01,\n",
       "               1.0413e-01,  2.8462e-01],\n",
       "             [-1.8066e-01,  7.1741e-01,  7.3482e-01,  ...,  3.4216e-01,\n",
       "              -5.0389e-01, -6.7674e-01],\n",
       "             [-1.5682e-01, -3.5008e-01,  2.5355e-02,  ..., -9.4031e-02,\n",
       "               7.2551e-02,  1.9553e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-3.5626e-02,  9.6058e-02,  2.6860e-02,  ..., -8.9990e-02,\n",
       "              -1.9404e-01,  1.8616e-01],\n",
       "             [-1.9478e-01,  1.2086e+00,  2.5259e+00,  ..., -1.0809e+00,\n",
       "              -2.8474e+00,  1.1295e+00],\n",
       "             [-6.7777e-01,  2.0095e+00,  1.1494e+00,  ..., -6.2736e-01,\n",
       "              -7.5870e-01,  1.4799e+00],\n",
       "             ...,\n",
       "             [-1.7186e+00,  3.9500e+00, -1.8548e+00,  ..., -1.7138e+00,\n",
       "               8.2255e-01,  1.2757e+00],\n",
       "             [-9.1280e-01,  3.4560e+00, -1.8048e-01,  ..., -2.4396e+00,\n",
       "               7.2674e-01,  2.0220e+00],\n",
       "             [-1.6321e+00,  4.0120e+00, -1.9003e+00,  ..., -1.7738e+00,\n",
       "               7.9802e-01,  1.2192e+00]],\n",
       "  \n",
       "            [[ 1.4859e-01,  4.1946e-01, -2.2613e+00,  ..., -5.9651e-01,\n",
       "              -1.5599e-02, -9.8328e-02],\n",
       "             [ 8.3303e-02,  1.3943e+00,  3.4867e+00,  ...,  4.4353e-01,\n",
       "              -1.2919e+00,  8.0335e-01],\n",
       "             [ 1.5714e+00,  1.1583e+00,  2.1803e+00,  ...,  1.0743e+00,\n",
       "              -6.1733e-01,  7.0874e-01],\n",
       "             ...,\n",
       "             [ 5.2559e-01, -1.1066e+00, -4.1979e+00,  ...,  1.3285e+00,\n",
       "              -1.4407e+00, -1.0478e+00],\n",
       "             [ 3.3856e-01, -1.2271e+00, -3.6175e+00,  ...,  1.3964e+00,\n",
       "              -1.5940e+00,  1.1369e-01],\n",
       "             [ 5.3394e-01, -1.1370e+00, -4.4529e+00,  ...,  1.3289e+00,\n",
       "              -1.4031e+00, -1.0683e+00]],\n",
       "  \n",
       "            [[-1.0476e+00,  7.8033e-01,  3.7009e-01,  ..., -7.8949e-01,\n",
       "              -1.7861e-01, -3.5614e-01],\n",
       "             [-4.4819e-01, -2.3335e+00, -3.8445e-01,  ...,  3.4558e-01,\n",
       "               8.7486e-01,  1.2523e+00],\n",
       "             [ 3.9366e-01, -2.0292e+00,  2.0745e-01,  ...,  1.0650e+00,\n",
       "              -1.4602e-01,  1.7951e+00],\n",
       "             ...,\n",
       "             [-1.4020e+00, -4.2545e+00,  2.2487e+00,  ..., -1.4094e+00,\n",
       "              -4.9603e-01, -3.9276e+00],\n",
       "             [-1.7254e+00, -4.2003e+00,  1.7347e+00,  ..., -2.0224e+00,\n",
       "               1.0840e-02, -3.5232e+00],\n",
       "             [-1.4327e+00, -4.3681e+00,  2.2557e+00,  ..., -1.5592e+00,\n",
       "              -4.9671e-01, -4.0285e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-4.5795e-01, -1.7472e-01, -7.6552e-01,  ..., -2.3013e-02,\n",
       "               4.5836e-01,  3.8976e-01],\n",
       "             [-1.4109e-01, -1.3885e+00,  1.5742e+00,  ..., -1.4984e+00,\n",
       "              -1.7863e-01,  1.8931e+00],\n",
       "             [ 3.5257e-02,  7.6608e-01,  1.1566e+00,  ..., -2.7326e+00,\n",
       "               7.7622e-01,  1.4166e+00],\n",
       "             ...,\n",
       "             [-2.4759e+00, -2.0726e+00,  1.4003e+00,  ..., -4.5185e+00,\n",
       "               1.4408e-01, -6.7974e-01],\n",
       "             [-1.8249e+00, -3.9657e+00,  4.7548e-01,  ..., -4.5437e+00,\n",
       "              -1.1691e+00, -5.9599e-01],\n",
       "             [-2.5894e+00, -2.2503e+00,  1.3842e+00,  ..., -4.4890e+00,\n",
       "               1.0650e-01, -6.8354e-01]],\n",
       "  \n",
       "            [[ 2.8074e-01,  9.7224e-01,  2.6496e-01,  ...,  1.9854e-02,\n",
       "              -1.1354e-01,  1.5440e-01],\n",
       "             [ 2.9735e-01, -8.5662e-01, -1.7143e-01,  ..., -4.2318e-01,\n",
       "               2.3457e+00,  1.4826e+00],\n",
       "             [ 4.0542e-01, -1.6285e-01, -1.6692e+00,  ...,  1.7143e+00,\n",
       "              -9.8580e-01,  2.0312e+00],\n",
       "             ...,\n",
       "             [-9.2772e-01, -1.4200e+00,  2.5189e-01,  ..., -7.2397e-01,\n",
       "              -1.5358e+00,  4.7808e-01],\n",
       "             [-2.3484e-01, -7.2803e-01,  9.8886e-02,  ..., -1.3927e+00,\n",
       "               4.9278e-01,  3.0268e-01],\n",
       "             [-9.3088e-01, -1.3284e+00,  3.6368e-01,  ..., -7.9778e-01,\n",
       "              -1.5207e+00,  4.1118e-01]],\n",
       "  \n",
       "            [[-8.6930e-02,  2.1569e-02,  7.8588e-02,  ..., -8.8409e-02,\n",
       "               2.9754e-01,  2.5058e-01],\n",
       "             [-7.8535e-01,  1.4004e+00, -7.1586e-02,  ...,  5.8438e-01,\n",
       "              -1.3186e+00, -3.3546e-01],\n",
       "             [ 3.1227e-01,  2.2924e-01,  3.0289e-01,  ...,  3.2061e-01,\n",
       "              -5.5615e-01,  1.3075e+00],\n",
       "             ...,\n",
       "             [ 1.0487e+00, -5.8908e-01, -9.7855e-01,  ...,  2.8311e+00,\n",
       "               2.3205e+00,  3.5746e+00],\n",
       "             [ 2.9843e-01,  5.6715e-01, -7.6179e-01,  ...,  2.3176e+00,\n",
       "               2.0353e+00,  3.0026e+00],\n",
       "             [ 1.0540e+00, -5.5265e-01, -8.9185e-01,  ...,  2.8961e+00,\n",
       "               2.4416e+00,  3.6135e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 1.8299e-01, -7.4142e-02, -7.0574e-02,  ...,  5.6379e-02,\n",
       "              -7.3365e-02, -4.6098e-02],\n",
       "             [ 1.3737e+00,  1.5504e+00,  4.2807e-01,  ..., -6.1071e-01,\n",
       "               6.7113e-01,  6.9169e-01],\n",
       "             [ 1.9625e+00, -4.4112e-01, -9.7198e-01,  ..., -9.5132e-01,\n",
       "              -1.0951e+00, -5.1565e-01],\n",
       "             ...,\n",
       "             [ 5.5925e-01, -3.1237e-02, -9.9782e-01,  ...,  1.0609e-01,\n",
       "              -1.6642e+00, -5.4093e-01],\n",
       "             [ 2.9720e-01,  9.5041e-01,  6.8391e-03,  ..., -5.8683e-02,\n",
       "              -1.4329e-01,  1.0249e-01],\n",
       "             [ 5.0838e-01,  2.2538e-02, -9.5318e-01,  ...,  9.3157e-02,\n",
       "              -1.6114e+00, -4.9090e-01]],\n",
       "  \n",
       "            [[-9.6329e-02,  1.6889e-02,  3.2881e-02,  ..., -4.6500e-02,\n",
       "               1.3689e-01,  1.4687e-02],\n",
       "             [ 1.9450e+00, -4.7742e-02,  1.2742e+00,  ...,  4.0864e-02,\n",
       "              -1.5111e+00,  2.9228e-02],\n",
       "             [ 9.9490e-01, -2.9075e-01,  2.7348e-01,  ...,  1.2490e+00,\n",
       "              -1.4511e+00, -8.1816e-01],\n",
       "             ...,\n",
       "             [ 1.3516e+00, -2.2480e-02,  2.8711e-01,  ..., -2.5484e-01,\n",
       "              -9.3909e-01, -7.5213e-01],\n",
       "             [ 1.7167e+00, -6.5455e-01,  5.1284e-01,  ...,  6.2249e-02,\n",
       "              -7.6691e-01, -1.0805e+00],\n",
       "             [ 1.3261e+00, -2.6576e-02,  2.4561e-01,  ..., -2.9411e-01,\n",
       "              -9.1086e-01, -7.8877e-01]],\n",
       "  \n",
       "            [[ 8.6372e-02, -1.3211e-01,  9.9665e-02,  ...,  1.7509e-03,\n",
       "              -1.4730e-02, -4.9049e-02],\n",
       "             [-7.9739e-01,  9.1485e-02, -1.3586e-01,  ...,  6.1312e-01,\n",
       "              -7.3886e-01, -2.3035e+00],\n",
       "             [-4.8863e-01,  6.2138e-01,  3.5179e-01,  ..., -4.8970e-01,\n",
       "              -6.8810e-01, -1.2448e+00],\n",
       "             ...,\n",
       "             [ 7.0990e-02, -3.7396e-02,  4.5078e-01,  ..., -6.8112e-01,\n",
       "              -2.5030e-01, -6.7158e-01],\n",
       "             [-2.6501e-01, -1.5887e-01, -3.6186e-01,  ..., -4.9099e-01,\n",
       "              -5.7055e-01, -1.5924e+00],\n",
       "             [ 9.8731e-02, -2.6620e-02,  4.2697e-01,  ..., -6.7852e-01,\n",
       "              -3.0273e-01, -6.7067e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.1563e-01,  7.7200e-02,  2.8498e-02,  ..., -9.3503e-02,\n",
       "              -1.6608e-02,  8.3871e-02],\n",
       "             [ 2.7667e-01,  2.0680e-01, -4.1773e-01,  ...,  1.9115e-01,\n",
       "              -1.1667e+00, -1.3154e+00],\n",
       "             [ 7.5924e-01, -2.7966e-01, -9.2384e-02,  ..., -1.0054e+00,\n",
       "              -4.3723e-01, -4.4194e-01],\n",
       "             ...,\n",
       "             [ 9.6144e-01,  4.1668e-01, -6.2466e-01,  ..., -4.5936e-01,\n",
       "              -5.4238e-01, -5.5113e-01],\n",
       "             [ 3.8808e-01,  3.5090e-03, -1.8062e+00,  ...,  4.4019e-01,\n",
       "              -4.4628e-01, -7.7932e-01],\n",
       "             [ 8.9202e-01,  3.8933e-01, -6.4758e-01,  ..., -4.9424e-01,\n",
       "              -4.9389e-01, -5.1038e-01]],\n",
       "  \n",
       "            [[-7.9168e-02,  5.1150e-02,  7.2856e-02,  ...,  1.0386e-01,\n",
       "               8.3135e-02, -1.1139e-01],\n",
       "             [-1.1810e+00, -5.0970e-01, -4.6497e-01,  ..., -3.2378e+00,\n",
       "              -4.2474e-01, -1.1595e-01],\n",
       "             [ 4.7672e-02, -3.7796e-01,  5.4111e-01,  ...,  7.2553e-01,\n",
       "               7.5427e-01, -1.9289e-01],\n",
       "             ...,\n",
       "             [-3.7523e-02,  3.5921e-01,  6.7582e-01,  ...,  7.2582e-01,\n",
       "               4.6708e-01, -3.8831e-01],\n",
       "             [-2.4542e-01, -4.6032e-01,  3.3361e-01,  ..., -1.9231e+00,\n",
       "              -3.2129e-01, -7.4452e-02],\n",
       "             [ 1.3230e-02,  3.5396e-01,  6.9046e-01,  ...,  6.3652e-01,\n",
       "               4.6656e-01, -3.6635e-01]],\n",
       "  \n",
       "            [[ 7.2860e-02,  1.9324e-01,  1.8858e-01,  ...,  1.1595e-02,\n",
       "               1.3806e-02, -1.3101e-01],\n",
       "             [ 9.3098e-02,  3.1130e-01,  7.3523e-01,  ..., -1.0186e+00,\n",
       "              -6.8469e-02,  5.8171e-01],\n",
       "             [-9.7061e-02,  4.9559e-01,  8.4866e-01,  ...,  6.7414e-02,\n",
       "              -1.7660e+00,  9.1924e-02],\n",
       "             ...,\n",
       "             [-3.2793e-01,  6.8008e-01,  9.2097e-01,  ...,  1.2371e-01,\n",
       "              -2.8955e-01,  4.2327e-02],\n",
       "             [-5.7163e-01,  4.5604e-01,  4.4431e-01,  ...,  4.0308e-01,\n",
       "               7.0040e-02, -7.2576e-01],\n",
       "             [-2.9994e-01,  7.0878e-01,  9.1150e-01,  ...,  1.3040e-01,\n",
       "              -3.1020e-01,  7.5951e-02]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 8.9367e-02, -7.8190e-01,  4.0781e-03,  ..., -6.3994e-02,\n",
       "              -3.4035e-01,  1.1557e-04],\n",
       "             [ 4.8697e-01,  8.7834e-01, -5.9861e-01,  ..., -1.1025e+00,\n",
       "               8.5854e-01,  1.7138e+00],\n",
       "             [-1.2371e-01,  6.2749e-01, -1.7620e-01,  ..., -3.3778e-01,\n",
       "              -6.9913e-01,  2.1432e-01],\n",
       "             ...,\n",
       "             [-1.1898e+00, -1.2330e+00,  1.2196e-01,  ..., -1.4832e+00,\n",
       "               1.3773e+00,  1.3300e+00],\n",
       "             [-1.1495e+00, -9.4264e-01, -6.8237e-01,  ..., -2.1637e+00,\n",
       "               1.5807e+00,  2.3811e+00],\n",
       "             [-1.1984e+00, -1.3661e+00,  1.2195e-01,  ..., -1.4987e+00,\n",
       "               1.3976e+00,  1.3283e+00]],\n",
       "  \n",
       "            [[-1.1463e-01,  1.5414e-01, -3.3556e-01,  ..., -1.2091e-01,\n",
       "              -6.0446e-02,  1.5057e+00],\n",
       "             [-2.0503e+00, -1.6268e+00, -8.0584e-03,  ...,  5.7306e-01,\n",
       "              -2.2473e-02, -2.4654e+00],\n",
       "             [-1.1604e+00, -8.0109e-02,  1.2182e+00,  ..., -2.7711e-01,\n",
       "               2.9322e-01, -3.7471e+00],\n",
       "             ...,\n",
       "             [-2.2172e+00,  4.4815e-01,  3.4076e+00,  ...,  3.1115e-01,\n",
       "              -4.0088e-01,  5.4946e+00],\n",
       "             [-2.7808e+00, -1.3333e+00,  2.5807e+00,  ...,  5.9998e-01,\n",
       "              -1.3772e-01,  5.6201e+00],\n",
       "             [-2.1908e+00,  4.5444e-01,  3.4168e+00,  ...,  2.7999e-01,\n",
       "              -4.5920e-01,  5.8262e+00]],\n",
       "  \n",
       "            [[ 7.0791e-02, -5.5965e-01, -2.2243e-02,  ...,  4.8049e-01,\n",
       "               5.3726e-01, -1.5731e-01],\n",
       "             [-8.5409e-01,  1.7755e+00, -2.9613e-01,  ..., -1.0768e+00,\n",
       "               1.1004e-01,  6.1195e-01],\n",
       "             [-5.3527e-01, -7.5270e-01, -1.5838e-01,  ...,  6.5029e-04,\n",
       "               5.7236e-01, -4.0013e-01],\n",
       "             ...,\n",
       "             [-3.7295e+00,  5.0293e-01, -1.8564e+00,  ...,  4.3107e+00,\n",
       "               7.7906e+00, -1.5311e+00],\n",
       "             [-4.3253e+00,  1.8817e+00, -1.8383e+00,  ...,  1.9279e+00,\n",
       "               7.9243e+00, -1.8689e+00],\n",
       "             [-3.6699e+00,  5.5439e-01, -1.8270e+00,  ...,  4.3574e+00,\n",
       "               7.9080e+00, -1.6615e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.3915e-01,  2.1767e-01, -6.5029e-01,  ...,  9.7640e-01,\n",
       "               6.4497e-01, -1.0495e+00],\n",
       "             [ 9.5624e-01,  1.6763e+00,  1.1208e+00,  ..., -1.3697e+00,\n",
       "              -8.4024e-01,  3.0501e+00],\n",
       "             [-7.7095e-01, -4.2066e-01,  2.7154e-01,  ..., -2.9740e+00,\n",
       "              -6.2394e-01,  2.0342e+00],\n",
       "             ...,\n",
       "             [ 1.2427e+00, -6.2246e+00, -1.7313e+00,  ...,  2.9262e+00,\n",
       "              -5.5176e-01,  1.9014e+00],\n",
       "             [ 2.4950e+00, -5.2072e+00, -2.0803e+00,  ...,  3.2605e+00,\n",
       "              -1.2203e+00,  3.4013e+00],\n",
       "             [ 1.2940e+00, -6.3190e+00, -1.7960e+00,  ...,  3.1776e+00,\n",
       "              -5.6973e-01,  1.8997e+00]],\n",
       "  \n",
       "            [[ 5.7570e-01,  7.4366e-01,  6.8049e-01,  ...,  1.1685e+00,\n",
       "              -4.2434e-01,  7.9872e-01],\n",
       "             [ 1.6935e+00, -1.6617e+00, -1.2794e+00,  ..., -6.9484e-01,\n",
       "              -2.1966e-01, -5.1784e-01],\n",
       "             [ 2.1669e+00, -7.9123e-01, -3.3049e-01,  ...,  9.4472e-01,\n",
       "               4.2394e-01, -1.8480e-01],\n",
       "             ...,\n",
       "             [ 1.9234e+00, -2.4876e+00, -3.0990e+00,  ...,  6.9780e-01,\n",
       "              -1.1071e+00, -2.2631e+00],\n",
       "             [ 1.5725e+00, -3.5509e+00, -3.8835e+00,  ..., -7.6856e-01,\n",
       "              -1.8001e-01, -2.1097e+00],\n",
       "             [ 1.8032e+00, -2.5010e+00, -3.1748e+00,  ...,  6.4680e-01,\n",
       "              -1.1083e+00, -2.2558e+00]],\n",
       "  \n",
       "            [[ 1.0457e-01, -1.8363e-01,  1.3462e-01,  ..., -1.2287e-01,\n",
       "               1.0313e-01, -2.4325e-01],\n",
       "             [ 1.9624e-02,  6.7755e-01,  9.9019e-01,  ..., -4.9596e-01,\n",
       "              -4.3477e-01,  8.3797e-01],\n",
       "             [-4.9421e-01, -1.6213e-01,  6.3992e-01,  ...,  2.0407e-01,\n",
       "              -6.4433e-01, -1.7984e-01],\n",
       "             ...,\n",
       "             [-5.2600e-01, -1.1804e-01,  8.1786e-02,  ...,  6.4444e+00,\n",
       "              -2.6129e+00, -5.5552e+00],\n",
       "             [-4.7832e-02,  9.1389e-01, -9.5729e-02,  ...,  6.6711e+00,\n",
       "              -2.5463e+00, -5.3890e+00],\n",
       "             [-5.1681e-01, -1.5662e-01,  5.9885e-02,  ...,  6.5897e+00,\n",
       "              -2.6515e+00, -5.5742e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-1.7617e-02,  1.9100e-02,  7.4215e-02,  ...,  5.5277e-03,\n",
       "              -3.7850e-02,  7.7476e-02],\n",
       "             [-7.0967e-01,  7.4358e-01,  1.7536e+00,  ...,  9.0167e-01,\n",
       "               6.8327e-01, -1.9166e+00],\n",
       "             [-5.1703e-01,  1.5444e+00, -1.0815e+00,  ...,  5.8523e-01,\n",
       "               7.1184e-01, -6.0517e-01],\n",
       "             ...,\n",
       "             [-5.0766e-01,  3.9992e-01, -6.2626e-01,  ...,  5.6122e-01,\n",
       "               2.6277e-01,  3.5103e-01],\n",
       "             [-8.5739e-01,  2.7689e-01, -1.8926e-01,  ...,  1.0571e+00,\n",
       "               2.1564e-01, -1.1946e-01],\n",
       "             [-4.8577e-01,  3.5587e-01, -6.6658e-01,  ...,  5.6549e-01,\n",
       "               2.4234e-01,  3.8590e-01]],\n",
       "  \n",
       "            [[ 3.3914e-02, -8.8856e-02, -5.5313e-03,  ...,  1.1858e-01,\n",
       "               1.3372e-01,  4.0809e-02],\n",
       "             [-4.7111e-01,  7.0224e-01,  4.9681e-01,  ...,  1.3935e-01,\n",
       "              -7.9133e-01,  1.1294e+00],\n",
       "             [-8.0564e-01,  5.3137e-01,  2.7531e-01,  ..., -5.9608e-01,\n",
       "               3.3773e-02, -7.8056e-01],\n",
       "             ...,\n",
       "             [-2.2093e-01,  2.4333e-01, -4.2353e-01,  ..., -3.3910e-01,\n",
       "               6.4202e-01, -3.8438e-01],\n",
       "             [-6.4428e-01,  2.7128e-01, -1.1813e-01,  ..., -1.7804e-01,\n",
       "              -2.8194e-02,  3.4588e-01],\n",
       "             [-2.6635e-01,  2.7081e-01, -4.3918e-01,  ..., -3.1688e-01,\n",
       "               6.7675e-01, -3.7627e-01]],\n",
       "  \n",
       "            [[-2.5022e-02,  4.6746e-04,  7.1227e-02,  ..., -2.1691e-02,\n",
       "              -1.4388e-01,  1.2917e-01],\n",
       "             [ 1.3207e+00, -9.7167e-02, -4.3254e-02,  ..., -1.1794e-01,\n",
       "               4.7581e-01, -6.0767e-01],\n",
       "             [-5.9027e-01,  5.4489e-02,  4.0488e-01,  ...,  4.8344e-01,\n",
       "               4.6009e-01,  1.9037e-02],\n",
       "             ...,\n",
       "             [-6.4645e-01,  5.0527e-03, -9.9323e-02,  ...,  3.4828e-01,\n",
       "              -3.6676e-01, -7.5016e-01],\n",
       "             [ 5.8888e-01,  1.7997e-01,  3.4016e-01,  ..., -2.4735e-01,\n",
       "              -4.2223e-01, -5.4837e-01],\n",
       "             [-6.2072e-01,  6.2121e-03, -5.1855e-02,  ...,  3.6348e-01,\n",
       "              -3.7332e-01, -7.2578e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-1.2634e-01, -9.0111e-03,  6.1170e-02,  ..., -2.8844e-02,\n",
       "               1.1346e-02,  1.3515e-01],\n",
       "             [-4.6222e-01, -7.3574e-01,  7.6296e-01,  ..., -3.4225e-01,\n",
       "               9.4332e-01, -2.9572e-01],\n",
       "             [-6.2965e-01,  4.1410e-01,  6.0440e-01,  ...,  3.3087e-02,\n",
       "               8.8669e-01, -4.3701e-01],\n",
       "             ...,\n",
       "             [-4.1360e-01,  4.2841e-01,  3.4588e-01,  ..., -1.7670e-01,\n",
       "               2.7978e-01, -1.8215e-01],\n",
       "             [-8.1077e-01,  2.0147e-01,  5.1268e-01,  ..., -6.9632e-01,\n",
       "               5.9417e-01,  5.2836e-02],\n",
       "             [-4.2159e-01,  4.2881e-01,  3.4142e-01,  ..., -1.9776e-01,\n",
       "               2.7369e-01, -1.9171e-01]],\n",
       "  \n",
       "            [[-2.8028e-02,  8.4329e-02, -7.4562e-02,  ...,  1.1567e-01,\n",
       "               4.5594e-02,  5.4924e-02],\n",
       "             [-1.5705e-01, -1.3401e+00, -1.5046e+00,  ..., -1.4752e+00,\n",
       "               5.2599e-01,  1.6321e+00],\n",
       "             [ 4.3184e-01,  2.3969e-01, -8.8045e-01,  ..., -2.0772e+00,\n",
       "               1.4693e+00,  8.0424e-01],\n",
       "             ...,\n",
       "             [-1.1475e-01, -3.9556e-02, -2.5399e-01,  ..., -8.1381e-01,\n",
       "               2.5289e-01,  5.5467e-01],\n",
       "             [ 2.7709e-02, -2.2001e+00, -1.2571e+00,  ..., -1.8843e+00,\n",
       "               2.2824e-01,  1.2075e+00],\n",
       "             [-1.0162e-01, -5.9535e-02, -2.6836e-01,  ..., -7.9802e-01,\n",
       "               1.9247e-01,  5.9190e-01]],\n",
       "  \n",
       "            [[-1.1343e-01, -2.1909e-02,  1.3382e-01,  ...,  5.6986e-02,\n",
       "              -2.1418e-01, -6.8748e-02],\n",
       "             [ 6.5334e-01,  1.3428e+00, -1.5310e-01,  ..., -2.3468e-01,\n",
       "               2.0447e-01,  3.7732e-01],\n",
       "             [ 6.7185e-01,  3.5975e-01,  2.5317e-01,  ...,  5.9677e-01,\n",
       "              -1.3562e+00,  7.8783e-01],\n",
       "             ...,\n",
       "             [ 7.2587e-01, -1.9175e-02, -1.8541e-01,  ...,  4.8094e-01,\n",
       "              -4.2995e-01,  7.1530e-01],\n",
       "             [ 5.5944e-01,  6.2292e-01,  1.6546e-01,  ...,  8.0500e-02,\n",
       "               1.5836e-01,  1.7326e-01],\n",
       "             [ 6.9656e-01, -7.5633e-02, -2.3236e-01,  ...,  4.6187e-01,\n",
       "              -4.0555e-01,  7.0178e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 2.5721e-01,  1.2138e+00,  8.1850e-02,  ..., -1.5436e-02,\n",
       "              -2.6535e-02, -3.3984e-01],\n",
       "             [ 5.7023e-01, -1.6326e+00, -1.3452e+00,  ..., -9.7856e-01,\n",
       "              -9.3806e-02,  1.1103e-01],\n",
       "             [-6.5884e-01, -7.3669e-01,  5.5879e-01,  ..., -1.6449e+00,\n",
       "               1.3863e+00,  5.7571e-01],\n",
       "             ...,\n",
       "             [-2.5891e+00,  3.6271e+00,  7.8342e-01,  ..., -2.4252e+00,\n",
       "               3.6595e+00,  1.6286e+00],\n",
       "             [-1.9184e+00,  2.8134e+00, -1.8229e-01,  ..., -3.1005e+00,\n",
       "               2.8074e+00,  1.3658e+00],\n",
       "             [-2.6316e+00,  3.7973e+00,  7.7992e-01,  ..., -2.4313e+00,\n",
       "               3.6154e+00,  1.5645e+00]],\n",
       "  \n",
       "            [[-3.5678e-01, -2.6492e-01, -1.1649e-01,  ..., -1.0741e-01,\n",
       "               4.2998e-01, -9.8164e-02],\n",
       "             [ 3.8823e-01, -1.8031e+00, -1.3167e+00,  ...,  1.5703e+00,\n",
       "              -7.4182e-01,  2.3385e-01],\n",
       "             [ 3.9244e-01, -1.6642e+00, -7.6806e-01,  ...,  1.1689e+00,\n",
       "               4.5848e-02, -7.8316e-01],\n",
       "             ...,\n",
       "             [-1.3631e+00, -2.9359e+00,  9.6263e-01,  ..., -3.9056e-01,\n",
       "               5.3526e+00, -2.1709e+00],\n",
       "             [-1.0622e+00, -4.5258e+00,  6.2185e-01,  ...,  7.7345e-02,\n",
       "               4.8630e+00, -1.9212e+00],\n",
       "             [-1.3845e+00, -2.9444e+00,  1.0412e+00,  ..., -4.3046e-01,\n",
       "               5.4469e+00, -2.1500e+00]],\n",
       "  \n",
       "            [[-7.6571e-02, -2.1862e-02,  3.1526e-01,  ...,  4.8836e-01,\n",
       "               5.6518e-02, -2.0227e+00],\n",
       "             [-8.5462e-01,  7.1048e-01, -1.4530e+00,  ...,  5.4351e-01,\n",
       "              -3.4837e-01,  3.8102e+00],\n",
       "             [ 1.6520e+00, -7.6158e-01, -7.2883e-01,  ...,  3.7667e-01,\n",
       "              -8.7408e-02,  2.4233e+00],\n",
       "             ...,\n",
       "             [ 2.1690e+00, -1.6134e+00, -1.6237e+00,  ..., -1.6497e+00,\n",
       "              -8.1387e-02, -1.5496e+00],\n",
       "             [ 2.0887e+00, -3.2867e-01, -2.6876e+00,  ..., -1.8665e+00,\n",
       "               1.4759e-01, -6.2026e-01],\n",
       "             [ 2.1724e+00, -1.6319e+00, -1.6827e+00,  ..., -1.6649e+00,\n",
       "              -1.2454e-01, -1.6946e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-4.9549e-01, -9.6223e-01, -1.5773e+00,  ..., -8.9447e-01,\n",
       "              -1.6752e+00,  7.5425e-01],\n",
       "             [ 1.8472e+00,  1.4729e+00, -1.0867e+00,  ...,  1.3627e+00,\n",
       "              -1.8153e+00,  1.2332e+00],\n",
       "             [ 1.2498e+00,  2.6905e-01, -5.3599e-01,  ...,  7.9664e-01,\n",
       "              -2.8643e+00,  9.6770e-01],\n",
       "             ...,\n",
       "             [-4.3781e-01,  7.7877e-03, -5.2059e-01,  ..., -2.9902e+00,\n",
       "              -7.1555e+00, -3.6680e+00],\n",
       "             [ 7.1942e-01,  8.8740e-01, -8.9328e-01,  ..., -1.6693e+00,\n",
       "              -6.5624e+00, -2.7816e+00],\n",
       "             [-4.9307e-01, -4.5331e-02, -5.8609e-01,  ..., -3.1443e+00,\n",
       "              -7.1759e+00, -3.7997e+00]],\n",
       "  \n",
       "            [[-4.7654e-01, -3.9439e-01, -3.6832e-02,  ..., -7.6317e-02,\n",
       "              -2.8420e-01,  1.1539e+00],\n",
       "             [-1.6546e-01, -3.9656e-01, -2.0579e-01,  ..., -8.3005e-01,\n",
       "               1.1140e-01, -2.6167e+00],\n",
       "             [-1.1353e-01, -3.3726e-01, -6.2611e-01,  ...,  6.2819e-01,\n",
       "              -2.3303e-02, -9.2785e-01],\n",
       "             ...,\n",
       "             [-6.1267e+00,  1.4838e-01,  3.1597e+00,  ...,  8.6293e+00,\n",
       "               3.6374e-01,  4.1481e+00],\n",
       "             [-6.3406e+00,  5.6825e-01,  4.0743e+00,  ...,  7.0162e+00,\n",
       "               4.4845e-01,  3.9556e+00],\n",
       "             [-6.2856e+00,  1.6311e-01,  3.2931e+00,  ...,  8.7034e+00,\n",
       "               3.0326e-01,  4.2933e+00]],\n",
       "  \n",
       "            [[ 7.7467e-02, -2.6303e-01, -5.2029e-01,  ...,  2.1443e+00,\n",
       "               2.2853e+00,  4.4737e-01],\n",
       "             [ 1.7687e+00, -1.7020e-01, -2.1599e+00,  ..., -3.6347e-01,\n",
       "              -2.1461e+00, -8.9899e-01],\n",
       "             [-1.1482e-02, -1.4947e+00, -5.2686e-01,  ...,  1.7061e+00,\n",
       "              -1.1591e+00,  2.6533e-01],\n",
       "             ...,\n",
       "             [ 3.1832e+00, -2.3699e+00,  7.2115e-01,  ...,  1.0088e+01,\n",
       "               6.8950e+00,  1.1676e+00],\n",
       "             [ 4.6007e+00, -1.9037e+00,  1.9926e-01,  ...,  9.2790e+00,\n",
       "               6.8701e+00,  1.2638e+00],\n",
       "             [ 3.2946e+00, -2.3977e+00,  7.5841e-01,  ...,  1.0198e+01,\n",
       "               7.1239e+00,  1.1269e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-1.6332e-01,  1.4786e-01, -1.1422e-01,  ...,  3.8804e-02,\n",
       "               1.7195e-01, -1.0548e-01],\n",
       "             [-1.6032e-01,  5.5673e-01,  1.0013e+00,  ..., -8.0335e-01,\n",
       "               1.1870e+00, -3.0828e-01],\n",
       "             [ 1.3168e-01,  8.9118e-01,  9.7710e-01,  ..., -2.6267e-01,\n",
       "               9.1514e-01,  4.1614e-01],\n",
       "             ...,\n",
       "             [ 1.2225e-01,  5.3133e-01, -2.5466e-01,  ..., -9.4634e-01,\n",
       "               5.2956e-01,  1.7132e-01],\n",
       "             [-5.8868e-01,  4.4613e-01,  1.3970e-01,  ..., -5.0140e-01,\n",
       "               3.5854e-01,  5.0789e-02],\n",
       "             [ 8.7806e-02,  5.1828e-01, -2.8527e-01,  ..., -9.0446e-01,\n",
       "               4.7059e-01,  2.1138e-01]],\n",
       "  \n",
       "            [[ 1.1702e-01, -6.4699e-02,  3.1676e-02,  ..., -5.7561e-02,\n",
       "               1.1061e-01,  6.0477e-02],\n",
       "             [-1.8498e+00, -1.4362e+00,  2.8411e+00,  ..., -8.4311e-01,\n",
       "               4.4487e-02,  2.3439e+00],\n",
       "             [-9.9192e-01, -1.3068e+00,  1.3063e+00,  ..., -2.5021e-01,\n",
       "               1.6060e+00,  9.6410e-01],\n",
       "             ...,\n",
       "             [-3.5784e-01, -9.1374e-01,  6.8249e-01,  ...,  9.1868e-01,\n",
       "               6.2099e-01,  4.3478e-01],\n",
       "             [-9.0948e-01, -1.7387e+00,  2.6020e+00,  ...,  4.5886e-01,\n",
       "               8.4674e-01,  1.6683e+00],\n",
       "             [-3.9481e-01, -8.7358e-01,  6.8256e-01,  ...,  9.3054e-01,\n",
       "               6.1870e-01,  4.1275e-01]],\n",
       "  \n",
       "            [[ 1.0699e-01, -8.6197e-02, -1.5626e-01,  ..., -3.6686e-02,\n",
       "              -2.8451e-02, -1.8200e-01],\n",
       "             [ 2.0919e+00, -5.7443e-01,  5.0261e-01,  ...,  5.0879e-01,\n",
       "              -4.4538e-01,  1.1994e+00],\n",
       "             [ 1.1740e+00,  1.1131e+00, -1.0803e-01,  ...,  1.5733e+00,\n",
       "               5.6335e-01,  4.2996e-01],\n",
       "             ...,\n",
       "             [ 7.4576e-01,  3.7459e-01,  5.1748e-02,  ...,  5.9791e-01,\n",
       "               6.2805e-01, -2.7986e-03],\n",
       "             [ 1.1941e+00, -6.3472e-01,  4.1449e-01,  ...,  4.0538e-02,\n",
       "              -2.7813e-01,  5.5665e-01],\n",
       "             [ 7.5119e-01,  3.0822e-01,  4.3934e-02,  ...,  6.2068e-01,\n",
       "               6.9509e-01, -1.4949e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 7.0213e-02, -4.6690e-03, -1.3244e-02,  ..., -1.9181e-01,\n",
       "              -5.8565e-02, -1.1433e-01],\n",
       "             [-1.9068e-01,  6.4706e-01,  1.7624e+00,  ..., -1.2610e+00,\n",
       "              -6.0742e-01, -1.6792e-01],\n",
       "             [-2.5184e-01, -1.1926e-01,  3.5495e-02,  ..., -1.5271e+00,\n",
       "              -4.9171e-01,  1.1398e+00],\n",
       "             ...,\n",
       "             [ 1.5809e-01, -1.4623e+00,  2.1308e-01,  ..., -3.6758e-01,\n",
       "              -4.1551e-01, -2.6188e-02],\n",
       "             [-2.4769e-01, -1.0237e+00,  1.2874e+00,  ..., -4.7997e-01,\n",
       "               2.9049e-02, -4.7527e-01],\n",
       "             [ 1.6794e-01, -1.4549e+00,  1.7829e-01,  ..., -3.6329e-01,\n",
       "              -4.1272e-01, -3.8104e-02]],\n",
       "  \n",
       "            [[-1.8465e-01, -1.0772e-01, -6.3472e-02,  ..., -4.8459e-02,\n",
       "              -9.3733e-02,  7.8508e-03],\n",
       "             [-6.5891e-01, -7.6872e-01,  2.0198e+00,  ...,  4.1450e-01,\n",
       "              -1.1445e+00, -4.7673e-01],\n",
       "             [-1.2836e-01, -1.2783e+00,  9.8167e-01,  ..., -5.0068e-01,\n",
       "              -9.8449e-01,  1.7445e+00],\n",
       "             ...,\n",
       "             [-2.9627e-01, -1.3340e+00,  4.8251e-01,  ..., -5.8301e-01,\n",
       "              -6.2550e-01,  1.6966e+00],\n",
       "             [-1.3742e+00, -8.0175e-01,  5.1382e-01,  ..., -1.7461e-01,\n",
       "              -9.7751e-01,  1.4339e-01],\n",
       "             [-2.8782e-01, -1.3493e+00,  4.9810e-01,  ..., -5.6421e-01,\n",
       "              -5.4099e-01,  1.7088e+00]],\n",
       "  \n",
       "            [[ 1.5993e-02, -8.2934e-02, -1.0345e-01,  ...,  6.5070e-02,\n",
       "               2.6188e-02,  3.8959e-02],\n",
       "             [ 4.2417e-01, -1.5099e+00,  1.4649e-02,  ...,  1.4629e-01,\n",
       "              -1.0355e+00, -1.3556e+00],\n",
       "             [ 7.3402e-01, -2.0612e-01, -7.6275e-01,  ...,  1.2663e-01,\n",
       "              -6.5220e-01, -1.2646e+00],\n",
       "             ...,\n",
       "             [ 2.8900e-01, -1.4638e-01, -4.8337e-01,  ...,  6.8739e-01,\n",
       "              -1.4610e+00,  1.6385e-02],\n",
       "             [ 1.2888e-01, -1.1009e+00,  3.7273e-01,  ...,  9.4664e-01,\n",
       "              -1.8511e+00,  3.8415e-01],\n",
       "             [ 2.8724e-01, -1.4473e-01, -4.4987e-01,  ...,  6.7078e-01,\n",
       "              -1.4032e+00,  7.5681e-02]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-1.7468e-01,  1.0717e-01, -7.6040e-01,  ...,  3.0821e-01,\n",
       "               2.8741e-01,  2.9048e-02],\n",
       "             [ 4.2890e-01,  5.4284e-01,  4.4602e+00,  ...,  1.9793e-01,\n",
       "               1.5736e+00, -2.1584e+00],\n",
       "             [-1.5036e+00,  7.6909e-01,  1.0537e+00,  ..., -8.4576e-02,\n",
       "              -3.2861e-02, -7.8621e-01],\n",
       "             ...,\n",
       "             [ 2.4971e-01, -2.6549e-01, -1.8247e+01,  ...,  6.3037e-01,\n",
       "              -3.0734e-01,  1.1628e-01],\n",
       "             [ 1.6194e+00,  2.1027e-01, -1.7676e+01,  ...,  6.1824e-01,\n",
       "               3.2983e-01, -1.0848e+00],\n",
       "             [ 3.0269e-01, -3.2883e-01, -1.8575e+01,  ...,  5.9535e-01,\n",
       "              -2.6633e-01,  1.9165e-01]],\n",
       "  \n",
       "            [[ 5.6240e-01, -6.3630e-01,  1.1905e-02,  ...,  2.0326e-01,\n",
       "               1.9875e-01, -7.6223e-02],\n",
       "             [ 1.6348e+00, -1.4077e-01,  1.3047e-01,  ...,  8.6383e-01,\n",
       "               1.0218e+00,  2.5608e-01],\n",
       "             [ 6.7032e-01, -7.2540e-01, -9.9685e-01,  ...,  7.3853e-01,\n",
       "               4.6462e-02, -3.0084e-01],\n",
       "             ...,\n",
       "             [ 1.0973e+00, -3.2247e-01, -1.3315e+00,  ..., -2.5628e+00,\n",
       "               1.9588e+00,  1.6561e-01],\n",
       "             [ 2.2223e+00,  5.5571e-01, -2.0431e+00,  ..., -2.3944e+00,\n",
       "               1.9723e+00,  4.7768e-01],\n",
       "             [ 1.1568e+00, -3.1824e-01, -1.3313e+00,  ..., -2.6206e+00,\n",
       "               1.9909e+00,  1.3412e-01]],\n",
       "  \n",
       "            [[-1.7560e+00,  1.5315e+00,  1.0415e+00,  ...,  3.5970e-01,\n",
       "               1.6668e+00, -1.1792e+00],\n",
       "             [-1.1854e+00,  1.8038e-01,  8.0481e-01,  ...,  2.2626e-01,\n",
       "              -4.3112e-01, -1.7047e+00],\n",
       "             [ 5.1965e-01,  4.9645e-01, -2.7751e-02,  ...,  1.6743e+00,\n",
       "              -7.3849e-01, -1.1344e+00],\n",
       "             ...,\n",
       "             [ 1.1806e+00, -1.1148e+00, -1.1892e+00,  ...,  3.7135e+00,\n",
       "              -2.4685e+00,  8.7146e-01],\n",
       "             [ 2.7237e-02, -3.8074e-01, -4.3562e-01,  ...,  3.4326e+00,\n",
       "              -1.9947e+00,  7.7859e-01],\n",
       "             [ 1.1604e+00, -1.1115e+00, -1.1292e+00,  ...,  3.6589e+00,\n",
       "              -2.4985e+00,  9.7956e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 2.3085e-01,  5.2260e-02, -2.6409e-01,  ...,  1.0213e-01,\n",
       "               9.6440e-01,  3.3343e-01],\n",
       "             [-5.5340e-01,  4.6812e-02, -3.1486e-01,  ...,  4.3484e-02,\n",
       "              -9.7888e-01,  1.2114e+00],\n",
       "             [-8.7279e-01, -7.8645e-01, -7.8899e-01,  ..., -1.8146e+00,\n",
       "              -1.5181e+00,  2.3942e+00],\n",
       "             ...,\n",
       "             [ 1.1691e+00, -1.0570e+00,  1.2454e+00,  ..., -2.0210e+00,\n",
       "               1.8224e-01,  2.4187e+00],\n",
       "             [ 6.9002e-01, -1.3524e+00,  2.5004e+00,  ..., -1.9812e+00,\n",
       "               7.3888e-01,  2.0485e+00],\n",
       "             [ 1.2013e+00, -1.1386e+00,  1.3503e+00,  ..., -2.0165e+00,\n",
       "               2.3174e-01,  2.4157e+00]],\n",
       "  \n",
       "            [[ 2.1764e-01,  9.4057e-01,  5.6683e-01,  ...,  2.8146e-01,\n",
       "              -6.7593e-02,  9.5767e-01],\n",
       "             [ 5.8316e-01,  1.9273e+00,  2.3725e+00,  ...,  3.3895e-01,\n",
       "              -5.4782e-01,  2.5123e+00],\n",
       "             [ 1.6383e+00,  1.2497e-01,  1.7635e+00,  ..., -6.8835e-01,\n",
       "               1.2507e+00,  1.0928e+00],\n",
       "             ...,\n",
       "             [ 2.7377e+00, -2.4009e+00,  3.7117e+00,  ...,  1.6957e-01,\n",
       "               2.4561e+00, -1.6149e+00],\n",
       "             [ 1.6675e+00, -2.6092e+00,  4.3211e+00,  ...,  1.0568e+00,\n",
       "               1.3674e+00, -6.8008e-01],\n",
       "             [ 2.7063e+00, -2.4350e+00,  3.7069e+00,  ...,  1.7959e-01,\n",
       "               2.4409e+00, -1.6400e+00]],\n",
       "  \n",
       "            [[-1.1778e-01, -2.5513e-01, -1.5417e-01,  ...,  2.8240e-01,\n",
       "               5.9684e-01, -2.1728e-01],\n",
       "             [-8.0560e-01,  5.4791e-02,  5.7660e-01,  ..., -1.6531e+00,\n",
       "               2.5050e-01,  1.4666e+00],\n",
       "             [ 8.1632e-01,  1.5964e+00, -1.2875e+00,  ...,  2.1482e-01,\n",
       "              -1.6551e+00, -1.0226e-01],\n",
       "             ...,\n",
       "             [ 1.9715e+00,  2.5392e+00, -2.0742e+00,  ..., -2.0154e+00,\n",
       "              -1.0214e+00, -3.0341e+00],\n",
       "             [ 6.0310e-01,  2.2108e+00, -2.0675e+00,  ..., -2.1962e+00,\n",
       "              -5.8974e-01, -2.5049e+00],\n",
       "             [ 2.0665e+00,  2.5266e+00, -2.0451e+00,  ..., -2.1100e+00,\n",
       "              -1.0499e+00, -3.0856e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-4.5566e-02, -1.7480e-02, -3.8891e-02,  ...,  3.6865e-02,\n",
       "              -5.5307e-02, -7.1500e-02],\n",
       "             [ 6.2637e-01,  2.5595e-01,  6.6446e-01,  ..., -1.7559e+00,\n",
       "              -1.8878e-01,  5.3419e-01],\n",
       "             [-7.7922e-01,  1.3548e-01,  8.6319e-01,  ..., -6.4873e-01,\n",
       "              -2.8578e-01, -3.3432e-01],\n",
       "             ...,\n",
       "             [-1.0011e+00,  1.6430e-01, -2.5699e-01,  ..., -3.6857e-01,\n",
       "              -3.2400e-01, -4.7606e-01],\n",
       "             [ 9.6577e-02,  5.4291e-01,  2.0008e-01,  ..., -8.8246e-01,\n",
       "               5.1083e-01, -2.2739e-01],\n",
       "             [-9.7745e-01,  1.7431e-01, -2.9558e-01,  ..., -3.2513e-01,\n",
       "              -2.4526e-01, -4.8195e-01]],\n",
       "  \n",
       "            [[-1.2290e-01,  1.1081e-01, -4.3613e-02,  ...,  1.5664e-01,\n",
       "              -1.3631e-01,  3.4497e-02],\n",
       "             [-5.2788e-01, -7.9282e-01,  1.1273e+00,  ..., -1.0951e+00,\n",
       "               3.4573e-01, -6.2041e-01],\n",
       "             [ 2.0018e-02, -1.5128e+00,  1.2531e+00,  ...,  3.4519e-01,\n",
       "               2.3076e+00, -1.0036e+00],\n",
       "             ...,\n",
       "             [ 7.1103e-01, -1.4989e+00,  3.9438e-01,  ..., -8.2604e-02,\n",
       "               1.0878e+00, -1.1895e+00],\n",
       "             [-4.7197e-01, -6.9099e-01,  8.4547e-01,  ..., -1.1704e+00,\n",
       "               2.9250e-01, -9.6097e-01],\n",
       "             [ 6.9305e-01, -1.4659e+00,  4.1487e-01,  ..., -1.2832e-01,\n",
       "               9.6341e-01, -1.1758e+00]],\n",
       "  \n",
       "            [[-4.0915e-02, -2.5827e-02, -3.8340e-02,  ...,  6.5657e-02,\n",
       "              -6.5066e-02, -7.1902e-02],\n",
       "             [ 9.2164e-01, -1.4168e+00, -4.8031e-01,  ...,  6.4627e-01,\n",
       "              -9.3983e-01,  1.5531e+00],\n",
       "             [ 3.2302e-01,  6.6319e-01,  5.3836e-02,  ..., -9.4950e-01,\n",
       "               7.1307e-01, -2.3200e-01],\n",
       "             ...,\n",
       "             [ 9.8054e-02,  4.0679e-01,  4.1275e-01,  ..., -1.6196e-01,\n",
       "               9.0045e-01,  8.5608e-02],\n",
       "             [ 9.1775e-01, -1.7935e-01, -4.4064e-01,  ...,  1.0792e+00,\n",
       "              -2.4329e-01,  2.4545e-01],\n",
       "             [ 1.1315e-01,  4.2112e-01,  4.0122e-01,  ..., -1.0363e-01,\n",
       "               9.1427e-01,  1.2911e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-2.0744e-02,  6.6681e-02,  4.2876e-02,  ...,  6.3183e-02,\n",
       "               7.4517e-02, -2.1804e-02],\n",
       "             [ 1.2956e+00, -6.0599e-01, -7.3705e-01,  ..., -1.3554e+00,\n",
       "               1.1951e+00,  7.5260e-01],\n",
       "             [ 5.7595e-01, -6.3402e-01, -1.0337e+00,  ..., -1.4194e+00,\n",
       "              -3.7618e-01, -3.3896e-01],\n",
       "             ...,\n",
       "             [ 1.8955e-01, -8.5672e-01, -1.2462e+00,  ..., -7.5994e-01,\n",
       "              -4.3447e-01, -6.0162e-01],\n",
       "             [ 1.1331e+00, -3.4510e-01, -1.1935e+00,  ..., -1.7561e+00,\n",
       "               7.0557e-01,  2.1363e-01],\n",
       "             [ 2.3032e-01, -8.7259e-01, -1.2628e+00,  ..., -7.7210e-01,\n",
       "              -4.1047e-01, -5.7558e-01]],\n",
       "  \n",
       "            [[ 3.2518e-04,  3.7154e-02,  1.2617e-01,  ..., -3.2091e-02,\n",
       "               7.7207e-02,  1.2766e-01],\n",
       "             [ 9.3514e-01, -1.7814e-01, -2.8198e+00,  ...,  7.3082e-01,\n",
       "              -3.8584e-01, -1.1399e+00],\n",
       "             [ 3.0442e-01,  9.3584e-01,  8.9384e-02,  ...,  4.6791e-01,\n",
       "               4.5719e-01, -2.7641e-01],\n",
       "             ...,\n",
       "             [ 1.9988e-01,  9.2302e-01,  2.7621e-01,  ..., -4.0096e-01,\n",
       "               1.0985e+00,  2.7633e-01],\n",
       "             [ 1.3093e+00,  3.3545e-01, -1.9330e+00,  ..., -3.2209e-01,\n",
       "               2.5876e-01, -2.9921e-01],\n",
       "             [ 2.4153e-01,  9.2163e-01,  2.8477e-01,  ..., -3.7422e-01,\n",
       "               1.1135e+00,  2.6796e-01]],\n",
       "  \n",
       "            [[-1.4468e-02,  1.3328e-01,  2.9911e-02,  ..., -4.7124e-03,\n",
       "              -4.0540e-02, -2.9560e-02],\n",
       "             [-1.0652e+00,  1.2824e+00,  4.1322e-01,  ...,  8.1307e-01,\n",
       "               2.6433e-01, -4.2133e-01],\n",
       "             [-7.7034e-01,  5.4305e-01, -5.1909e-01,  ...,  1.3205e+00,\n",
       "               1.2389e+00, -1.8216e-01],\n",
       "             ...,\n",
       "             [-3.3312e-01,  6.6690e-01,  1.0034e-01,  ...,  7.7545e-01,\n",
       "               1.4768e-01,  1.3035e-01],\n",
       "             [-1.3296e+00,  1.1827e+00,  8.3366e-01,  ..., -7.6596e-02,\n",
       "               5.9457e-01,  1.6917e-01],\n",
       "             [-4.0216e-01,  6.6990e-01,  1.4997e-01,  ...,  7.6484e-01,\n",
       "               1.2988e-01,  9.3710e-02]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-6.8363e-02,  2.3473e-01, -1.8097e-01,  ...,  2.3013e-01,\n",
       "              -1.4967e-01, -5.4951e-02],\n",
       "             [ 3.0134e-02, -2.0588e-01,  8.5890e-01,  ..., -5.5731e-01,\n",
       "              -2.0551e+00, -4.1907e-01],\n",
       "             [-6.3715e-01,  6.3762e-03,  1.2009e+00,  ...,  1.3735e+00,\n",
       "               3.4252e-02,  2.2617e-01],\n",
       "             ...,\n",
       "             [-5.0931e-01,  1.2689e+00,  6.6942e-01,  ...,  1.5946e+00,\n",
       "              -1.1770e+00, -5.1137e-01],\n",
       "             [-4.7522e-01,  1.2451e+00,  7.3215e-01,  ...,  3.7181e-01,\n",
       "              -2.7636e+00, -1.2014e+00],\n",
       "             [-6.0890e-01,  1.2834e+00,  6.5942e-01,  ...,  1.5900e+00,\n",
       "              -1.2238e+00, -4.8055e-01]],\n",
       "  \n",
       "            [[-2.0398e-01, -4.6946e-02, -6.9316e-02,  ..., -1.3456e-01,\n",
       "              -2.3902e-01,  6.6880e-01],\n",
       "             [-1.2970e+00, -2.2308e-01, -4.1568e-01,  ..., -1.7620e-01,\n",
       "              -6.2372e-01, -1.8557e+00],\n",
       "             [-1.0259e+00,  3.8145e-02, -5.1221e-02,  ..., -3.6907e+00,\n",
       "              -1.4507e+00, -1.5562e+00],\n",
       "             ...,\n",
       "             [-3.7403e+00, -1.2280e+00, -1.4350e+00,  ..., -3.2937e+00,\n",
       "              -1.3203e+00, -2.4295e+00],\n",
       "             [-4.4497e+00, -8.6610e-01, -2.1866e+00,  ..., -1.6010e+00,\n",
       "              -6.3435e-01, -3.3133e+00],\n",
       "             [-3.7603e+00, -1.2771e+00, -1.4538e+00,  ..., -3.1839e+00,\n",
       "              -1.3174e+00, -2.4027e+00]],\n",
       "  \n",
       "            [[ 1.2101e-01, -3.3775e-01, -7.5342e-01,  ...,  1.4288e-01,\n",
       "              -5.7473e-02, -2.2517e-01],\n",
       "             [ 7.1858e-01,  1.0671e+00,  2.5207e+00,  ..., -7.8073e-01,\n",
       "               1.3278e+00,  1.7128e+00],\n",
       "             [-5.5515e-01,  4.5155e-01,  1.2876e+00,  ...,  9.0296e-02,\n",
       "               1.1994e+00,  1.3098e+00],\n",
       "             ...,\n",
       "             [-7.2239e-01,  1.0309e+00, -2.5305e+00,  ...,  3.5632e-01,\n",
       "               1.8742e+00,  2.6916e+00],\n",
       "             [ 9.3721e-03,  2.0874e+00, -2.2025e+00,  ..., -1.8252e-01,\n",
       "               3.1287e+00,  3.2005e+00],\n",
       "             [-6.3847e-01,  1.0943e+00, -2.6169e+00,  ...,  3.9455e-01,\n",
       "               1.9324e+00,  2.7169e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-3.1846e-01, -2.2966e-01, -4.1549e-01,  ...,  2.8118e-01,\n",
       "              -9.1376e-01, -1.3885e-01],\n",
       "             [ 2.2987e+00, -8.6459e-01,  2.8000e+00,  ..., -8.0268e-01,\n",
       "               2.8388e+00,  5.8557e-01],\n",
       "             [ 1.5848e-01, -8.9317e-01,  1.2131e+00,  ...,  3.5174e-02,\n",
       "               1.9402e+00,  7.1297e-01],\n",
       "             ...,\n",
       "             [-3.7386e+00,  3.4965e+00, -6.2242e+00,  ...,  7.6135e-01,\n",
       "              -1.0431e+00,  1.0505e+00],\n",
       "             [-2.5247e+00,  4.0209e+00, -5.4806e+00,  ...,  3.6015e-01,\n",
       "              -3.2000e-01,  7.0123e-01],\n",
       "             [-3.8224e+00,  3.5692e+00, -6.4203e+00,  ...,  7.5984e-01,\n",
       "              -1.2126e+00,  1.0081e+00]],\n",
       "  \n",
       "            [[ 8.2211e-01,  9.1275e-02,  4.2697e-02,  ..., -6.7359e-01,\n",
       "               1.0167e-01,  1.2876e+00],\n",
       "             [-8.9089e-01,  8.4156e-01,  1.2123e+00,  ..., -8.7723e-01,\n",
       "               5.6888e-01,  2.3910e-01],\n",
       "             [-5.4239e-01,  9.3615e-01,  1.0040e-01,  ..., -2.1924e+00,\n",
       "               4.9205e-02, -5.1112e-01],\n",
       "             ...,\n",
       "             [ 3.7273e+00,  1.4250e-01, -5.9669e+00,  ..., -1.0485e+00,\n",
       "               9.4365e-01, -1.1212e-01],\n",
       "             [ 3.9569e+00, -4.9953e-01, -6.4824e+00,  ..., -1.4632e+00,\n",
       "               1.2777e+00, -3.2002e-01],\n",
       "             [ 3.8552e+00,  1.0124e-01, -6.0035e+00,  ..., -1.0735e+00,\n",
       "               9.6882e-01, -1.1187e-01]],\n",
       "  \n",
       "            [[-3.7398e-01,  2.5305e-02,  9.7201e-01,  ...,  2.3895e-01,\n",
       "              -5.1968e-01,  6.8140e-01],\n",
       "             [ 1.1778e-01,  4.4421e-01, -2.7052e-01,  ...,  2.0774e+00,\n",
       "              -7.1972e-01,  1.1630e-01],\n",
       "             [ 1.0886e-01, -1.1758e+00, -1.5168e-01,  ...,  1.3644e+00,\n",
       "              -1.7472e+00,  4.0572e-01],\n",
       "             ...,\n",
       "             [ 5.7748e+00,  6.8894e+00,  9.0955e+00,  ..., -3.7789e+00,\n",
       "              -3.3661e+00, -6.2968e-01],\n",
       "             [ 6.8790e+00,  8.0774e+00,  1.0206e+01,  ..., -3.0138e+00,\n",
       "              -3.8751e+00, -9.1797e-01],\n",
       "             [ 5.9551e+00,  7.1900e+00,  9.2815e+00,  ..., -4.0185e+00,\n",
       "              -3.4386e+00, -6.5133e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-4.2885e-02, -1.2107e-01, -1.5820e-01,  ...,  2.0636e-01,\n",
       "               2.8759e-01,  7.4576e-02],\n",
       "             [ 1.1971e-01, -6.6895e-01, -1.8416e+00,  ...,  5.7289e-01,\n",
       "              -2.0339e-01,  1.3873e-02],\n",
       "             [-1.2138e+00,  3.6421e-01, -1.3385e+00,  ...,  4.9920e-01,\n",
       "               7.7398e-01, -7.5725e-01],\n",
       "             ...,\n",
       "             [-7.6551e-01,  1.4908e+00, -1.0417e+00,  ...,  9.3507e-01,\n",
       "               1.6497e+00,  4.3018e-02],\n",
       "             [-7.0337e-01,  1.1566e+00, -1.0160e+00,  ...,  1.4911e+00,\n",
       "               1.0859e+00,  6.0557e-01],\n",
       "             [-7.1998e-01,  1.4543e+00, -9.9271e-01,  ...,  9.9563e-01,\n",
       "               1.6108e+00,  3.3358e-02]],\n",
       "  \n",
       "            [[ 4.7294e-02,  1.2589e-02,  1.3605e-01,  ...,  7.8787e-03,\n",
       "               2.9382e-02, -2.9296e-02],\n",
       "             [-2.4300e-01,  3.3368e-01,  1.4025e-01,  ..., -1.6096e+00,\n",
       "               8.4775e-01,  1.0206e+00],\n",
       "             [ 2.5166e-01, -2.8205e-01, -4.3210e-01,  ..., -1.9428e+00,\n",
       "               2.0017e+00, -1.9764e-01],\n",
       "             ...,\n",
       "             [ 8.7309e-01,  1.3881e-01, -2.1979e-01,  ..., -1.5756e+00,\n",
       "               8.6083e-01, -7.9376e-01],\n",
       "             [-4.2063e-01,  3.9164e-01,  1.1048e-01,  ..., -1.2903e+00,\n",
       "               6.9103e-01,  8.6336e-01],\n",
       "             [ 8.5124e-01,  1.1466e-01, -1.6801e-01,  ..., -1.5125e+00,\n",
       "               8.3840e-01, -7.4542e-01]],\n",
       "  \n",
       "            [[ 1.2611e-01,  3.1437e-03, -7.3591e-02,  ...,  1.3742e-01,\n",
       "              -3.6719e-02, -9.5853e-02],\n",
       "             [ 1.2321e+00, -1.1449e+00, -3.8663e-01,  ...,  8.7898e-01,\n",
       "              -1.2717e+00,  1.0307e+00],\n",
       "             [ 8.4025e-01,  3.0522e-01, -7.7022e-01,  ...,  3.0005e-01,\n",
       "              -5.1376e-01,  1.0424e-01],\n",
       "             ...,\n",
       "             [ 7.1224e-01,  1.7528e-01, -3.9135e-01,  ...,  3.4713e-01,\n",
       "              -3.3522e-01,  2.7083e-01],\n",
       "             [ 4.6691e-01, -7.9861e-01, -5.7115e-01,  ...,  5.0911e-01,\n",
       "               1.2945e-01,  1.1278e+00],\n",
       "             [ 7.2042e-01,  1.2726e-01, -4.5077e-01,  ...,  4.2308e-01,\n",
       "              -2.6290e-01,  2.8119e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 4.6765e-02, -5.6109e-02, -1.3412e-01,  ..., -9.7240e-02,\n",
       "              -5.3328e-02, -3.9770e-02],\n",
       "             [-2.9042e-01,  1.0353e+00,  3.4273e-01,  ..., -6.2273e-01,\n",
       "               7.4122e-01, -1.0529e+00],\n",
       "             [ 3.9952e-02,  1.3833e-01,  6.6181e-02,  ...,  2.2065e-01,\n",
       "              -7.6734e-01, -3.8463e-01],\n",
       "             ...,\n",
       "             [-1.6303e-01,  9.8397e-02,  8.5066e-02,  ...,  8.9085e-01,\n",
       "              -1.0727e-01, -7.3128e-01],\n",
       "             [ 4.0338e-01,  1.1190e+00, -7.2316e-01,  ..., -4.1705e-01,\n",
       "              -4.1264e-01, -7.4593e-01],\n",
       "             [-1.4650e-01,  1.2129e-01,  7.5668e-02,  ...,  8.2638e-01,\n",
       "              -1.1043e-01, -7.1499e-01]],\n",
       "  \n",
       "            [[-5.0889e-02, -4.9232e-03,  3.7780e-02,  ..., -3.1797e-01,\n",
       "              -2.4316e-01,  1.4987e-02],\n",
       "             [-2.5397e-01,  6.5921e-01,  1.3070e+00,  ...,  4.6241e-01,\n",
       "               9.0882e-02,  2.8398e-02],\n",
       "             [-1.3102e+00,  1.0008e+00,  5.2654e-01,  ..., -1.5409e-01,\n",
       "              -1.7296e+00, -2.4269e-01],\n",
       "             ...,\n",
       "             [-7.4463e-01,  8.6569e-02,  3.1618e-02,  ..., -7.7343e-01,\n",
       "              -3.2690e-01, -2.0179e-02],\n",
       "             [ 5.9176e-01,  3.0533e-01,  5.2555e-01,  ..., -3.2570e-01,\n",
       "               1.1459e-01,  2.0796e-01],\n",
       "             [-7.0952e-01,  3.7049e-02,  2.2360e-02,  ..., -7.7559e-01,\n",
       "              -3.0874e-01,  1.1578e-02]],\n",
       "  \n",
       "            [[ 2.9618e-02,  3.8360e-02,  8.3317e-03,  ..., -2.7286e-01,\n",
       "              -4.2066e-02, -3.7188e-02],\n",
       "             [ 5.3378e-01,  1.7853e+00, -8.6738e-01,  ...,  2.3777e-01,\n",
       "               1.1519e+00, -2.4273e-02],\n",
       "             [ 3.9251e-01,  2.1904e-01, -1.6148e-01,  ...,  5.8887e-02,\n",
       "              -3.5234e-01,  1.0769e+00],\n",
       "             ...,\n",
       "             [ 4.3078e-01,  1.1279e-01, -1.4636e-01,  ...,  3.1902e-01,\n",
       "              -2.0811e-01,  8.7018e-01],\n",
       "             [ 1.5142e-01,  5.7969e-01, -1.6673e-01,  ...,  4.2616e-01,\n",
       "               8.8396e-01,  1.0264e+00],\n",
       "             [ 4.7118e-01,  5.4779e-02, -1.4001e-01,  ...,  3.0940e-01,\n",
       "              -1.9915e-01,  8.0427e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-2.4955e-02,  3.8577e-01,  2.8015e-01,  ...,  3.6721e-01,\n",
       "               1.9064e-01, -2.3876e-01],\n",
       "             [ 2.5125e-01, -1.2733e-01, -8.0235e-01,  ..., -4.6622e-01,\n",
       "               1.9686e+00,  1.0569e+00],\n",
       "             [ 2.8510e-01,  5.1489e-02, -1.5954e+00,  ..., -1.7207e-01,\n",
       "               1.2692e+00,  1.1976e-01],\n",
       "             ...,\n",
       "             [-2.5456e-01,  5.9838e-01, -5.5159e-01,  ..., -1.2533e+00,\n",
       "              -7.3917e-02,  4.2924e+00],\n",
       "             [-1.4242e-01,  1.3811e+00, -5.4571e-01,  ..., -8.9925e-01,\n",
       "              -3.9785e-01,  5.4730e+00],\n",
       "             [-2.4730e-01,  6.3387e-01, -4.7063e-01,  ..., -1.2184e+00,\n",
       "              -3.8992e-02,  4.4595e+00]],\n",
       "  \n",
       "            [[-1.9131e-01, -1.1398e-01, -1.6757e-01,  ...,  4.0200e-02,\n",
       "              -1.8549e-01,  9.0757e-02],\n",
       "             [ 4.1804e-01,  9.7727e-01,  7.1425e-01,  ..., -5.6789e-01,\n",
       "              -5.7342e-01,  9.7889e-01],\n",
       "             [-9.6520e-01, -4.1238e-01,  8.1408e-01,  ..., -1.3623e-01,\n",
       "               7.9233e-01,  9.6695e-01],\n",
       "             ...,\n",
       "             [ 7.2722e-03, -1.4795e+00,  1.7848e+00,  ...,  1.2893e+00,\n",
       "               7.8675e-01,  4.8717e-02],\n",
       "             [ 1.4567e-02, -1.5633e+00,  1.6778e+00,  ...,  7.0404e-01,\n",
       "               7.2698e-02,  1.8317e-01],\n",
       "             [ 4.7297e-02, -1.5337e+00,  1.7674e+00,  ...,  1.2655e+00,\n",
       "               8.3135e-01,  5.1514e-02]],\n",
       "  \n",
       "            [[ 3.2747e-01, -8.6707e-02, -1.4488e-01,  ...,  4.4925e-01,\n",
       "              -3.4146e-01,  3.6310e-02],\n",
       "             [ 8.1195e-01,  4.8323e-01, -7.2059e-01,  ..., -4.6087e-01,\n",
       "               1.4376e+00,  2.2213e-01],\n",
       "             [ 3.6785e-01,  3.3558e-01, -3.1029e-01,  ..., -4.0883e-01,\n",
       "               2.5137e+00,  3.6423e-01],\n",
       "             ...,\n",
       "             [ 1.8962e+00,  3.7996e+00, -1.4433e-01,  ...,  3.7024e+00,\n",
       "              -2.5669e+00,  2.5543e+00],\n",
       "             [ 2.4319e+00,  4.4623e+00, -6.1962e-01,  ...,  3.6510e+00,\n",
       "              -3.5980e+00,  3.2528e+00],\n",
       "             [ 1.9951e+00,  3.8396e+00, -2.1976e-01,  ...,  3.8463e+00,\n",
       "              -2.8782e+00,  2.5936e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.1725e-01, -2.1779e-01, -1.8960e-01,  ...,  3.1861e-01,\n",
       "               3.1671e-01,  1.5524e-02],\n",
       "             [-1.2444e+00,  2.4231e-01,  5.5073e-01,  ..., -5.8774e-01,\n",
       "              -8.5408e-01, -1.7633e+00],\n",
       "             [-4.7743e-01, -3.8582e-01,  1.0976e+00,  ..., -2.4307e-01,\n",
       "              -5.0690e-01, -9.9650e-01],\n",
       "             ...,\n",
       "             [-1.0695e+00,  1.0245e+00,  1.7452e+00,  ...,  5.2326e-01,\n",
       "              -8.4779e-01, -1.8184e+00],\n",
       "             [-1.1765e+00,  1.4705e+00,  1.6807e+00,  ..., -7.6910e-01,\n",
       "              -1.3749e+00, -2.8614e+00],\n",
       "             [-1.1527e+00,  1.0473e+00,  1.7507e+00,  ...,  5.2847e-01,\n",
       "              -7.8102e-01, -1.8488e+00]],\n",
       "  \n",
       "            [[-2.1762e-01,  3.3568e-01, -1.6233e-01,  ..., -6.5765e-01,\n",
       "               4.2922e-01, -7.0302e-01],\n",
       "             [-1.7727e+00, -5.9688e-01, -1.4605e+00,  ..., -2.4516e+00,\n",
       "               7.4791e-01,  9.4136e-02],\n",
       "             [-1.1862e+00,  1.9440e+00, -3.6245e-01,  ..., -1.5809e+00,\n",
       "               4.9827e-01,  6.7005e-01],\n",
       "             ...,\n",
       "             [-2.5242e+00,  3.4597e+00,  4.7155e-01,  ..., -8.3066e-01,\n",
       "               1.2267e+00,  1.3677e+00],\n",
       "             [-1.9195e+00,  1.7770e+00, -9.2155e-01,  ..., -1.9264e+00,\n",
       "               1.6124e+00,  9.0136e-01],\n",
       "             [-2.4809e+00,  3.4660e+00,  4.7123e-01,  ..., -8.4369e-01,\n",
       "               1.2412e+00,  1.3494e+00]],\n",
       "  \n",
       "            [[-3.1883e-01, -1.7511e-01, -1.6445e-01,  ...,  9.8513e-02,\n",
       "               2.0126e-01,  3.1801e-01],\n",
       "             [ 1.1070e+00, -1.5935e+00, -2.4602e-01,  ..., -8.2626e-01,\n",
       "               8.4036e-01, -1.6332e+00],\n",
       "             [ 2.8565e-01, -4.2543e-01, -9.1148e-01,  ...,  3.5443e-01,\n",
       "               9.5425e-01,  3.4728e-01],\n",
       "             ...,\n",
       "             [ 1.2397e-01, -6.4325e-01,  2.9638e-01,  ..., -2.2534e-01,\n",
       "               2.2002e+00,  5.0860e+00],\n",
       "             [ 7.1293e-01, -2.1270e+00,  2.6220e-01,  ...,  1.7734e-02,\n",
       "               1.7432e+00,  4.6476e+00],\n",
       "             [ 9.4910e-02, -6.2242e-01,  3.6751e-01,  ..., -1.8507e-01,\n",
       "               2.1614e+00,  5.2033e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 4.2250e-02,  5.0168e-02,  1.8583e-01,  ..., -1.2491e-01,\n",
       "              -1.2075e-01, -9.9351e-02],\n",
       "             [ 8.9135e-01, -3.7804e-01, -7.4224e-02,  ...,  1.8145e+00,\n",
       "              -9.0233e-02,  3.3136e-01],\n",
       "             [ 8.3002e-01, -1.5926e-01,  8.7375e-01,  ..., -2.1030e-02,\n",
       "              -6.1785e-01, -2.1124e-01],\n",
       "             ...,\n",
       "             [ 9.0745e-01, -8.4917e-01,  7.9817e-01,  ..., -3.3964e-01,\n",
       "              -4.9278e-01, -6.9301e-01],\n",
       "             [ 8.6314e-01, -5.3071e-01,  7.0246e-01,  ...,  1.5704e+00,\n",
       "              -6.0517e-01,  4.2361e-01],\n",
       "             [ 9.0642e-01, -8.4196e-01,  8.4113e-01,  ..., -3.4844e-01,\n",
       "              -4.5786e-01, -6.5748e-01]],\n",
       "  \n",
       "            [[ 6.8843e-02, -3.4312e-02,  1.1653e-01,  ...,  2.8305e-03,\n",
       "               1.8918e-01,  1.6803e-01],\n",
       "             [ 7.9128e-01,  5.6126e-01,  3.2284e-01,  ...,  1.2917e+00,\n",
       "               1.0787e+00, -5.7739e-01],\n",
       "             [ 1.7619e-01,  1.7284e-01,  5.3417e-02,  ..., -8.9633e-01,\n",
       "               1.2142e+00,  1.5870e-02],\n",
       "             ...,\n",
       "             [ 6.3668e-01,  9.8275e-01, -1.7126e-01,  ..., -1.4113e-01,\n",
       "               1.7819e+00,  2.6439e-02],\n",
       "             [-1.7387e-01,  1.0414e+00, -2.5323e-01,  ...,  7.7625e-01,\n",
       "               1.6442e+00, -3.3496e-01],\n",
       "             [ 6.5011e-01,  9.4777e-01, -1.7463e-01,  ..., -1.3352e-01,\n",
       "               1.7757e+00,  6.0550e-02]],\n",
       "  \n",
       "            [[ 4.8726e-02, -7.4730e-03,  1.2723e-01,  ..., -4.8515e-02,\n",
       "               8.2777e-02, -9.4125e-02],\n",
       "             [-7.5361e-01,  5.9208e-01,  3.8427e-01,  ...,  8.1735e-01,\n",
       "              -6.6898e-01, -1.0737e-01],\n",
       "             [-7.0942e-01,  7.7838e-01,  5.4821e-01,  ...,  1.1840e-01,\n",
       "               4.2942e-01,  3.2433e-01],\n",
       "             ...,\n",
       "             [-4.4041e-01, -3.7385e-01,  5.3134e-02,  ...,  1.3888e-01,\n",
       "               2.8167e-01,  9.5426e-01],\n",
       "             [-6.0928e-01, -4.4753e-01, -4.8937e-02,  ...,  9.0892e-02,\n",
       "              -5.0845e-01,  4.8503e-01],\n",
       "             [-4.2209e-01, -3.9506e-01,  5.1358e-02,  ...,  1.1986e-01,\n",
       "               2.4777e-01,  9.8372e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-5.2235e-03, -1.2553e-01,  8.3080e-03,  ..., -5.1718e-02,\n",
       "              -1.1923e-02,  2.9005e-02],\n",
       "             [-1.1340e+00, -2.3374e-01, -2.1158e+00,  ...,  1.3871e-01,\n",
       "              -2.7347e+00, -1.5221e+00],\n",
       "             [-1.0410e+00, -5.7328e-01,  4.0415e-01,  ...,  1.9808e-01,\n",
       "               5.3024e-01, -9.3553e-01],\n",
       "             ...,\n",
       "             [-1.5851e-02, -6.9414e-01,  1.0032e+00,  ..., -1.0471e-01,\n",
       "               7.6869e-01,  1.7194e-01],\n",
       "             [-2.5266e-01, -3.1722e-02, -6.0867e-01,  ..., -3.0899e-01,\n",
       "              -2.3465e+00,  1.1645e-01],\n",
       "             [-5.2057e-02, -7.3095e-01,  1.0170e+00,  ..., -1.3583e-01,\n",
       "               7.5799e-01,  2.1820e-01]],\n",
       "  \n",
       "            [[-9.5237e-02,  1.0166e-01, -4.5621e-02,  ..., -1.3682e-01,\n",
       "              -4.5252e-02, -7.4323e-02],\n",
       "             [-1.0420e+00, -2.3487e+00, -2.4052e+00,  ..., -2.1206e+00,\n",
       "               1.6993e+00, -5.3463e-01],\n",
       "             [ 7.7381e-01, -1.5026e+00, -2.3097e-01,  ..., -1.1356e+00,\n",
       "               7.5040e-01, -6.3723e-01],\n",
       "             ...,\n",
       "             [-6.7290e-02, -4.9291e-01,  5.6384e-01,  ...,  5.7656e-03,\n",
       "               3.1572e-01, -1.4450e+00],\n",
       "             [ 4.6952e-01, -1.1270e+00, -1.1987e+00,  ..., -1.0248e+00,\n",
       "               1.3965e+00, -8.3618e-01],\n",
       "             [-2.6378e-02, -4.3790e-01,  5.4692e-01,  ..., -5.2463e-03,\n",
       "               2.5328e-01, -1.4716e+00]],\n",
       "  \n",
       "            [[ 1.4156e-01,  1.0304e-01,  5.9696e-02,  ..., -4.3525e-02,\n",
       "               2.4092e-02, -3.3962e-02],\n",
       "             [ 6.8187e-02,  5.5612e-01, -7.5028e-01,  ..., -3.4026e-01,\n",
       "               2.9871e-01, -1.4040e+00],\n",
       "             [ 4.5204e-01,  3.0772e-01, -1.2906e-01,  ..., -1.0082e+00,\n",
       "              -2.6068e-01, -8.4508e-01],\n",
       "             ...,\n",
       "             [ 8.5006e-01,  1.0708e-01, -4.5580e-01,  ..., -5.0894e-01,\n",
       "              -8.9236e-02, -7.4427e-01],\n",
       "             [-6.1613e-01,  3.1678e-01, -7.9129e-01,  ..., -3.8556e-01,\n",
       "               8.8199e-04, -1.3910e+00],\n",
       "             [ 8.0914e-01,  1.1730e-01, -4.7282e-01,  ..., -5.1120e-01,\n",
       "              -1.0140e-01, -7.3145e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 1.6141e-02,  4.0214e-01, -2.0166e-01,  ...,  1.6615e-01,\n",
       "               2.5353e-01, -6.4479e-01],\n",
       "             [ 8.0857e-01, -3.6461e-02, -1.1835e+00,  ..., -1.9934e-01,\n",
       "              -1.7127e+00,  4.0425e-01],\n",
       "             [ 9.6605e-01, -8.8255e-01,  4.7908e-01,  ...,  5.2032e-01,\n",
       "              -1.9165e+00, -1.3385e+00],\n",
       "             ...,\n",
       "             [ 4.2081e+00, -4.3530e-01,  1.4359e+00,  ...,  3.9579e+00,\n",
       "               4.7798e+00, -1.0570e+00],\n",
       "             [ 4.5705e+00,  4.0026e-01,  7.7374e-01,  ...,  3.6120e+00,\n",
       "               5.2667e+00, -1.3971e-01],\n",
       "             [ 4.3024e+00, -3.5679e-01,  1.4794e+00,  ...,  3.9988e+00,\n",
       "               5.0204e+00, -1.1465e+00]],\n",
       "  \n",
       "            [[-1.1129e-01, -5.0358e-01,  3.5406e-01,  ..., -2.7672e-01,\n",
       "              -4.6886e-01,  1.9271e-01],\n",
       "             [-3.0500e-01,  5.2277e+00,  5.3263e-01,  ...,  1.9336e+00,\n",
       "               1.1235e+00,  7.0536e-01],\n",
       "             [ 3.4710e-01,  2.8968e+00,  1.6316e+00,  ...,  6.5483e-01,\n",
       "               8.1731e-01,  3.8100e-03],\n",
       "             ...,\n",
       "             [-2.4028e+00, -5.6856e+00,  5.9516e+00,  ..., -9.8585e-02,\n",
       "               3.7800e+00, -6.2792e-01],\n",
       "             [-3.3216e+00, -4.8009e+00,  6.5393e+00,  ...,  1.1598e+00,\n",
       "               3.9386e+00,  1.3036e-01],\n",
       "             [-2.5365e+00, -5.9112e+00,  5.9930e+00,  ..., -1.2175e-01,\n",
       "               3.7828e+00, -5.7390e-01]],\n",
       "  \n",
       "            [[ 1.5179e-01, -4.6314e-02, -1.6434e-01,  ..., -1.9313e-01,\n",
       "              -5.5654e-01, -3.4469e-01],\n",
       "             [-1.1374e-02, -2.2429e+00,  1.1760e-01,  ...,  3.0295e-01,\n",
       "              -6.4599e-01, -1.5921e+00],\n",
       "             [-7.9938e-01, -1.3096e+00,  8.3456e-02,  ..., -1.8451e+00,\n",
       "              -1.5635e+00, -1.3109e+00],\n",
       "             ...,\n",
       "             [-7.1534e-01, -3.3939e+00, -2.6312e-01,  ..., -1.7791e+00,\n",
       "              -3.6649e+00, -2.8745e+00],\n",
       "             [ 1.4202e-02, -4.1120e+00,  4.3337e-01,  ..., -1.2172e+00,\n",
       "              -2.8369e+00, -2.2511e+00],\n",
       "             [-6.5960e-01, -3.4302e+00, -2.5048e-01,  ..., -1.7801e+00,\n",
       "              -3.7233e+00, -2.8359e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 9.5145e-02, -1.4019e-02,  1.6208e-01,  ...,  1.4838e-01,\n",
       "              -6.9859e-02, -2.1973e-01],\n",
       "             [ 5.1006e-01, -1.0189e+00,  1.6536e+00,  ..., -2.2259e-02,\n",
       "              -5.9242e-01,  2.5183e-01],\n",
       "             [ 7.2814e-01, -5.6520e-01,  1.0891e+00,  ..., -3.2481e-01,\n",
       "              -7.0000e-01,  5.2532e-01],\n",
       "             ...,\n",
       "             [ 3.6993e+00, -2.8455e+00, -3.3613e-02,  ..., -4.1317e-01,\n",
       "              -5.0132e-01, -1.8119e+00],\n",
       "             [ 5.1440e+00, -3.5246e+00, -4.3941e-01,  ...,  3.9153e-01,\n",
       "              -7.9707e-01, -2.3939e+00],\n",
       "             [ 3.8572e+00, -2.8988e+00, -2.0321e-02,  ..., -4.0340e-01,\n",
       "              -5.2071e-01, -1.8964e+00]],\n",
       "  \n",
       "            [[ 2.2533e-01, -1.1276e-01,  2.4039e-01,  ...,  2.4186e-01,\n",
       "               3.9230e-03,  1.3308e-01],\n",
       "             [ 1.8568e+00, -7.0363e-01,  5.6584e-01,  ..., -9.5466e-01,\n",
       "              -7.0394e-01,  7.9250e-01],\n",
       "             [ 7.9223e-01, -5.1518e-01,  1.4123e-01,  ...,  3.0214e-01,\n",
       "              -2.0003e+00, -1.3828e-01],\n",
       "             ...,\n",
       "             [ 4.9774e+00, -1.0434e+00, -6.5068e-01,  ..., -2.6522e-01,\n",
       "              -1.9447e+00,  1.3932e+00],\n",
       "             [ 7.0939e+00, -4.5766e-01, -5.8808e-01,  ..., -1.1504e+00,\n",
       "              -1.2180e+00,  1.2815e+00],\n",
       "             [ 5.1196e+00, -1.0972e+00, -6.2979e-01,  ..., -4.3470e-01,\n",
       "              -1.9564e+00,  1.4618e+00]],\n",
       "  \n",
       "            [[ 1.9628e-01, -2.1743e-01, -5.4668e-01,  ..., -1.5320e-01,\n",
       "              -2.7920e-01,  2.3376e-01],\n",
       "             [ 1.2563e+00,  3.3390e+00, -7.0119e-01,  ..., -6.6875e-01,\n",
       "              -9.8511e-01, -3.8920e-01],\n",
       "             [ 5.3804e-01,  2.1276e+00, -8.1373e-01,  ..., -7.7831e-01,\n",
       "               8.3867e-01,  1.0519e+00],\n",
       "             ...,\n",
       "             [-1.4767e+00,  6.1488e-01, -1.5700e+00,  ...,  1.3528e+00,\n",
       "               1.8489e-01,  2.0690e+00],\n",
       "             [-1.2938e+00,  1.5394e+00, -9.2237e-01,  ...,  4.5599e-01,\n",
       "              -3.9254e-01,  6.7409e-01],\n",
       "             [-1.5100e+00,  5.1334e-01, -1.5493e+00,  ...,  1.3725e+00,\n",
       "               2.2794e-01,  2.0244e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-1.4793e-02,  7.3524e-02,  4.5829e-02,  ..., -1.1554e-02,\n",
       "              -5.7882e-02,  5.2907e-02],\n",
       "             [-1.2772e+00, -3.3786e-01, -1.1363e+00,  ...,  1.0080e+00,\n",
       "              -6.9762e-01, -1.7991e-01],\n",
       "             [-9.8204e-01, -3.0691e-01, -3.8778e-01,  ...,  1.8543e-01,\n",
       "              -1.0096e-01,  2.8559e-01],\n",
       "             ...,\n",
       "             [-7.6848e-01, -8.7848e-01, -7.2359e-01,  ..., -7.1192e-01,\n",
       "              -1.1538e-01, -2.3633e-01],\n",
       "             [-1.3124e+00, -9.9143e-01, -1.5553e+00,  ...,  3.7022e-01,\n",
       "              -5.0282e-01, -3.2339e-01],\n",
       "             [-7.5191e-01, -8.9060e-01, -7.4888e-01,  ..., -7.5845e-01,\n",
       "              -9.6685e-02, -2.3739e-01]],\n",
       "  \n",
       "            [[-1.1834e-01, -5.5375e-02,  8.5329e-02,  ..., -1.4903e-01,\n",
       "               2.6376e-02,  1.6399e-01],\n",
       "             [ 6.3696e-01,  1.5603e+00, -6.8002e-01,  ...,  4.9880e-01,\n",
       "              -1.2674e+00, -6.9325e-01],\n",
       "             [ 6.1338e-02,  6.6575e-01, -3.0962e-01,  ...,  1.4828e-01,\n",
       "               8.7651e-01,  1.8279e-01],\n",
       "             ...,\n",
       "             [ 1.6080e-01, -5.2305e-01, -3.2931e-01,  ...,  1.4840e-01,\n",
       "               1.4074e-01,  4.7458e-03],\n",
       "             [-6.3594e-01,  1.3913e-02, -3.1687e-03,  ...,  7.6394e-02,\n",
       "              -3.9832e-01, -3.6468e-01],\n",
       "             [ 4.6908e-02, -5.3697e-01, -3.0798e-01,  ...,  1.4127e-01,\n",
       "               1.6072e-01, -4.2524e-02]],\n",
       "  \n",
       "            [[-9.6596e-02, -2.2499e-01,  2.0475e-01,  ...,  6.6423e-02,\n",
       "              -3.0584e-02,  2.2477e-01],\n",
       "             [-4.7936e-01, -1.1253e+00, -1.0839e+00,  ..., -4.5152e-01,\n",
       "              -9.4806e-02,  5.9853e-02],\n",
       "             [ 6.1400e-01, -6.7186e-01,  4.9245e-01,  ..., -6.7523e-01,\n",
       "               2.2928e-01,  1.5224e+00],\n",
       "             ...,\n",
       "             [ 6.1487e-01, -8.2942e-01,  4.3423e-01,  ..., -3.0143e-01,\n",
       "               4.6173e-01,  7.8557e-01],\n",
       "             [-1.4434e+00, -1.7917e-01, -5.9628e-01,  ..., -1.2410e-01,\n",
       "               3.3434e-01, -2.2182e-02],\n",
       "             [ 5.7284e-01, -7.9262e-01,  3.9897e-01,  ..., -3.0192e-01,\n",
       "               5.2859e-01,  7.6397e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-1.2443e-03,  8.5336e-02, -1.9872e-01,  ...,  4.0273e-02,\n",
       "              -1.5836e-01, -2.1388e-01],\n",
       "             [-4.1405e-01, -7.2114e-01, -3.8220e-01,  ..., -1.9422e-01,\n",
       "               5.5711e-01, -1.1581e-01],\n",
       "             [-5.1784e-01, -1.0053e-01, -2.1552e+00,  ...,  1.0807e+00,\n",
       "              -1.6824e+00, -1.3177e+00],\n",
       "             ...,\n",
       "             [-6.9235e-01, -6.1264e-01, -9.7350e-01,  ..., -6.7548e-03,\n",
       "              -1.1592e+00, -1.8305e+00],\n",
       "             [-7.5757e-01, -3.3887e-01, -6.8633e-01,  ..., -3.2268e-01,\n",
       "               1.2986e+00,  1.8509e-01],\n",
       "             [-6.9719e-01, -5.5009e-01, -9.4186e-01,  ..., -1.0114e-02,\n",
       "              -1.1904e+00, -1.7452e+00]],\n",
       "  \n",
       "            [[-8.8972e-03,  4.0978e-02, -1.3444e-01,  ...,  7.5752e-02,\n",
       "              -2.2658e-01,  3.1502e-03],\n",
       "             [ 4.1815e-01, -5.8090e-01, -2.9797e-01,  ...,  7.1130e-01,\n",
       "               7.8117e-01,  1.2556e+00],\n",
       "             [ 8.4496e-01, -7.0487e-01, -5.1064e-01,  ..., -7.1248e-01,\n",
       "               1.2750e+00,  6.7227e-01],\n",
       "             ...,\n",
       "             [ 5.5512e-01, -1.8391e+00, -9.0667e-01,  ..., -5.9074e-01,\n",
       "               8.6677e-01,  1.5167e-01],\n",
       "             [-1.0410e-01, -9.6551e-01, -7.8450e-01,  ...,  1.7369e-01,\n",
       "               2.4163e-01,  5.1076e-01],\n",
       "             [ 5.6207e-01, -1.8085e+00, -8.5340e-01,  ..., -5.7552e-01,\n",
       "               7.6965e-01,  1.8303e-01]],\n",
       "  \n",
       "            [[-5.1970e-02, -4.6209e-02,  1.6716e-01,  ...,  1.0012e-01,\n",
       "              -2.4189e-02, -3.5561e-02],\n",
       "             [ 1.2953e+00,  4.1384e-01, -3.6409e-01,  ...,  1.2637e-01,\n",
       "              -1.8758e+00,  3.0507e-01],\n",
       "             [ 3.4642e-01, -6.4915e-01,  8.6135e-01,  ..., -7.9244e-01,\n",
       "               8.5907e-01, -1.3510e+00],\n",
       "             ...,\n",
       "             [-1.3831e-01,  8.3662e-02,  7.0042e-01,  ...,  3.8570e-01,\n",
       "               7.4740e-01, -1.0022e+00],\n",
       "             [ 9.8044e-01,  1.2741e+00,  3.7253e-02,  ...,  8.5164e-01,\n",
       "              -1.0858e+00, -1.0824e-01],\n",
       "             [-1.2115e-01,  1.0846e-01,  7.2400e-01,  ...,  3.6335e-01,\n",
       "               7.2299e-01, -9.2417e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-1.7704e-01,  2.5098e-01, -1.3918e-01,  ...,  1.0624e-01,\n",
       "              -3.9388e-01, -3.4023e-01],\n",
       "             [-1.6987e-01,  1.4030e-01,  1.0399e-01,  ...,  3.1858e-01,\n",
       "              -3.9741e-01,  1.9997e+00],\n",
       "             [-8.0378e-01,  2.6730e-01, -2.9187e-01,  ...,  9.7025e-01,\n",
       "              -5.4433e-01,  2.3530e-01],\n",
       "             ...,\n",
       "             [ 2.3442e-01,  2.5107e+00, -3.5300e+00,  ...,  1.4958e+00,\n",
       "               1.1240e+00, -2.7029e-01],\n",
       "             [ 7.3228e-01,  2.2478e+00, -3.0260e+00,  ...,  1.3465e+00,\n",
       "               1.1869e+00,  1.4907e+00],\n",
       "             [ 3.0713e-01,  2.5970e+00, -3.7267e+00,  ...,  1.4601e+00,\n",
       "               1.1870e+00, -3.0428e-01]],\n",
       "  \n",
       "            [[-4.5692e-01, -4.5598e-01,  4.6643e-01,  ..., -6.5904e-01,\n",
       "              -1.9306e-01, -5.6085e-01],\n",
       "             [ 3.4918e-01, -5.3679e-02, -1.2179e+00,  ..., -4.9807e-01,\n",
       "              -2.1204e-01,  9.1124e-01],\n",
       "             [ 7.9751e-01,  5.2607e-01, -3.4294e-01,  ..., -3.7007e-01,\n",
       "               2.8444e-01, -9.3577e-01],\n",
       "             ...,\n",
       "             [ 2.4496e+00,  1.1858e+00, -3.1055e-01,  ...,  1.0241e+00,\n",
       "              -2.8110e+00, -5.0356e+00],\n",
       "             [ 2.2789e+00,  4.5656e-02, -4.5705e-01,  ...,  1.0229e+00,\n",
       "              -1.8912e+00, -4.4007e+00],\n",
       "             [ 2.4304e+00,  1.1487e+00, -3.6415e-01,  ...,  9.8561e-01,\n",
       "              -2.8631e+00, -5.1554e+00]],\n",
       "  \n",
       "            [[ 1.8130e-01,  2.0221e+00,  1.3832e+00,  ...,  5.1169e-01,\n",
       "               2.4475e-01,  7.3984e-02],\n",
       "             [-7.8292e-01, -2.1030e+00, -1.4412e+00,  ...,  1.3658e+00,\n",
       "              -2.3080e+00,  1.2692e+00],\n",
       "             [-3.3811e-01, -5.7479e-01, -9.9980e-01,  ...,  3.0638e-01,\n",
       "              -1.5366e+00,  1.5604e+00],\n",
       "             ...,\n",
       "             [-2.7012e+00,  1.1177e+01, -6.1279e-02,  ...,  9.2151e-01,\n",
       "              -1.1434e+00,  1.0019e-01],\n",
       "             [-4.1771e+00,  1.0677e+01, -5.1034e-01,  ...,  1.9042e+00,\n",
       "              -2.4349e+00,  1.3552e-01],\n",
       "             [-2.8427e+00,  1.1470e+01, -1.4490e-02,  ...,  9.7658e-01,\n",
       "              -1.1251e+00,  2.8623e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 3.7110e-02,  3.0914e-01,  4.5760e-01,  ...,  1.7316e-01,\n",
       "               8.9155e-03,  7.0790e-02],\n",
       "             [-2.8040e-01,  8.4698e-02,  1.1572e+00,  ...,  3.7928e-01,\n",
       "               7.9117e-01,  1.3659e+00],\n",
       "             [-4.2214e-01,  5.1926e-01,  8.5266e-01,  ...,  1.1217e+00,\n",
       "               4.2534e-01,  1.9072e+00],\n",
       "             ...,\n",
       "             [-3.4201e+00,  3.6615e-01,  3.6774e+00,  ...,  1.3953e+00,\n",
       "              -1.9797e+00,  2.0195e+00],\n",
       "             [-3.7788e+00,  2.4771e-01,  4.0974e+00,  ...,  1.1594e+00,\n",
       "              -2.7271e+00,  2.7430e+00],\n",
       "             [-3.4812e+00,  3.9650e-01,  3.7584e+00,  ...,  1.4194e+00,\n",
       "              -2.0235e+00,  2.0380e+00]],\n",
       "  \n",
       "            [[ 1.8872e-01,  4.7173e-02, -4.8206e-02,  ...,  9.8904e-02,\n",
       "              -9.0675e-03,  3.8710e-02],\n",
       "             [-7.4598e-01,  2.2388e+00,  4.8148e-01,  ...,  1.0424e+00,\n",
       "               1.7593e-01, -5.6965e-01],\n",
       "             [ 1.3636e+00,  2.3075e+00,  1.1324e+00,  ...,  1.4751e+00,\n",
       "               1.5005e+00, -1.4745e-01],\n",
       "             ...,\n",
       "             [ 1.0379e+00,  4.3442e-01,  9.4968e-01,  ...,  6.0330e-01,\n",
       "               1.1024e+00, -1.0083e+00],\n",
       "             [-7.8777e-01,  1.1156e+00,  1.2716e+00,  ...,  5.6302e-02,\n",
       "              -1.4833e-01, -9.6032e-01],\n",
       "             [ 1.0270e+00,  4.1547e-01,  1.0022e+00,  ...,  5.2029e-01,\n",
       "               1.0537e+00, -1.0053e+00]],\n",
       "  \n",
       "            [[ 7.9454e-02,  7.0977e-02,  3.5175e-02,  ...,  5.5455e-02,\n",
       "               3.2942e-01,  2.7173e-01],\n",
       "             [-1.5891e+00,  6.4638e-01,  3.0005e-01,  ..., -1.8071e-01,\n",
       "               1.1370e+00, -1.0484e+00],\n",
       "             [-1.3361e+00,  1.3833e+00, -9.3750e-01,  ...,  5.2912e-02,\n",
       "               2.0330e+00, -1.2787e+00],\n",
       "             ...,\n",
       "             [-4.0762e+00,  8.7766e-01, -2.0543e-01,  ...,  8.0790e-01,\n",
       "               2.6155e+00,  7.3779e-02],\n",
       "             [-4.4705e+00,  1.4958e+00, -2.5387e-01,  ...,  4.4849e-01,\n",
       "               1.4788e+00,  6.9613e-01],\n",
       "             [-4.1150e+00,  9.0303e-01, -2.1573e-01,  ...,  8.5942e-01,\n",
       "               2.6103e+00,  1.9477e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-2.4876e-02,  6.3550e-02,  2.7732e-02,  ...,  1.2231e-03,\n",
       "               2.5960e-01,  8.1726e-03],\n",
       "             [ 1.0898e+00,  2.0889e+00,  9.9452e-01,  ...,  5.2285e-01,\n",
       "              -1.7416e+00,  6.1093e-01],\n",
       "             [ 1.8767e-01,  9.9919e-01, -1.4214e+00,  ..., -8.6920e-01,\n",
       "               2.8190e-01,  4.3446e-01],\n",
       "             ...,\n",
       "             [-3.2562e-01,  1.2323e+00, -1.5686e+00,  ..., -1.9024e-01,\n",
       "               6.0371e-01,  9.4988e-02],\n",
       "             [-3.4477e-01,  2.4342e+00,  1.4290e+00,  ...,  6.3836e-01,\n",
       "              -1.7070e+00,  7.1635e-02],\n",
       "             [-2.8222e-01,  1.2455e+00, -1.5270e+00,  ..., -1.3148e-01,\n",
       "               5.9510e-01,  9.1886e-02]],\n",
       "  \n",
       "            [[ 1.5006e-01,  8.6397e-02, -2.8647e-01,  ..., -7.7012e-02,\n",
       "               9.6815e-02, -1.4891e-01],\n",
       "             [ 1.7671e+00,  1.8874e-02,  4.0821e-01,  ...,  7.5981e-01,\n",
       "               6.7134e-01,  1.3102e+00],\n",
       "             [ 1.0771e+00, -3.3804e-01, -1.2949e+00,  ...,  3.2510e-01,\n",
       "               1.1067e+00, -1.0542e+00],\n",
       "             ...,\n",
       "             [ 1.9251e+00, -1.4540e+00, -6.8589e-01,  ...,  4.9009e-01,\n",
       "               2.9731e-01, -1.1636e+00],\n",
       "             [ 2.6139e+00, -1.1586e+00, -9.8465e-02,  ...,  9.2550e-01,\n",
       "               4.5506e-01,  1.0919e+00],\n",
       "             [ 2.0306e+00, -1.4759e+00, -7.1596e-01,  ...,  5.8672e-01,\n",
       "               3.2487e-01, -1.1108e+00]],\n",
       "  \n",
       "            [[ 1.7615e-01, -7.7676e-02,  7.6348e-03,  ..., -7.3023e-02,\n",
       "               1.6742e-03,  9.7209e-03],\n",
       "             [-1.2699e+00,  2.3622e+00,  2.3304e-01,  ...,  1.2093e+00,\n",
       "              -1.1019e+00,  2.9163e-02],\n",
       "             [-1.5788e-01,  2.2549e+00,  6.3066e-01,  ..., -1.9863e+00,\n",
       "              -4.3600e-01, -7.6353e-02],\n",
       "             ...,\n",
       "             [-6.0184e-01,  8.1608e-01,  4.3826e-01,  ..., -1.9377e+00,\n",
       "              -8.2105e-01,  2.5787e-01],\n",
       "             [-1.8052e+00,  3.0268e+00,  3.5748e-02,  ..., -6.3478e-01,\n",
       "              -1.2507e-01,  2.0221e-01],\n",
       "             [-6.6504e-01,  7.2733e-01,  5.0961e-01,  ..., -1.9192e+00,\n",
       "              -7.7049e-01,  2.6718e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-9.1487e-02, -1.1333e-01,  1.7586e-01,  ..., -1.1592e-01,\n",
       "               1.1683e-01,  1.9515e-01],\n",
       "             [-1.7625e+00, -6.8811e-01, -1.1842e+00,  ...,  1.1516e+00,\n",
       "               5.2354e-01,  2.1580e+00],\n",
       "             [-1.5248e+00,  2.5224e-01, -4.5799e-01,  ..., -7.6002e-01,\n",
       "              -7.3545e-01,  1.8108e+00],\n",
       "             ...,\n",
       "             [-1.3440e+00,  7.2174e-01,  5.9245e-01,  ..., -6.7168e-01,\n",
       "              -3.8330e-01,  5.0656e-01],\n",
       "             [-1.8809e+00, -2.2718e-01, -6.3792e-01,  ...,  5.5448e-01,\n",
       "               8.4136e-01,  1.6922e+00],\n",
       "             [-1.3130e+00,  6.7752e-01,  5.9598e-01,  ..., -7.0757e-01,\n",
       "              -3.8928e-01,  5.3699e-01]],\n",
       "  \n",
       "            [[ 1.5977e-01, -2.4339e-03,  1.7304e-01,  ...,  1.5837e-01,\n",
       "              -5.6212e-02,  7.2709e-02],\n",
       "             [ 7.0068e-01, -2.7764e-01,  1.3608e+00,  ...,  1.6692e+00,\n",
       "               1.2503e+00, -1.0444e+00],\n",
       "             [ 1.6338e-01,  9.1919e-01,  1.7184e+00,  ...,  4.8779e-01,\n",
       "               1.6966e+00, -1.6504e-01],\n",
       "             ...,\n",
       "             [ 5.4991e-01,  6.3887e-01,  1.7825e-01,  ...,  1.0669e+00,\n",
       "               6.7639e-01, -3.3838e-01],\n",
       "             [ 7.1695e-01,  3.6230e-01,  1.4782e+00,  ...,  1.4439e+00,\n",
       "               2.8586e-01, -8.2015e-01],\n",
       "             [ 5.8005e-01,  5.8963e-01,  1.7634e-01,  ...,  1.1358e+00,\n",
       "               6.3790e-01, -3.1256e-01]],\n",
       "  \n",
       "            [[-1.3735e-01, -5.0504e-02,  7.0306e-02,  ...,  5.5682e-02,\n",
       "               8.5924e-02, -1.5147e-02],\n",
       "             [-1.1011e+00,  1.2676e-01, -1.0486e+00,  ..., -7.0738e-03,\n",
       "               1.1977e+00, -7.8149e-01],\n",
       "             [-4.9343e-01, -1.2560e+00, -5.2777e-02,  ...,  7.2532e-01,\n",
       "               7.7925e-01,  3.7413e-01],\n",
       "             ...,\n",
       "             [ 1.4137e-01, -2.3489e-01,  3.3385e-01,  ..., -2.7325e-02,\n",
       "               1.3163e-01,  9.3181e-02],\n",
       "             [-5.4350e-01, -9.1463e-01, -7.8626e-01,  ...,  4.5531e-01,\n",
       "               1.3796e-01, -3.2892e-01],\n",
       "             [ 1.1929e-01, -2.4079e-01,  3.7288e-01,  ..., -2.8434e-02,\n",
       "               4.0973e-02,  8.1618e-02]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-5.4882e-01, -4.6021e-01,  2.0816e-01,  ...,  3.2185e-02,\n",
       "               1.9677e-01, -4.0632e-01],\n",
       "             [-1.0208e+00,  3.4483e-01, -5.4078e+00,  ...,  1.0608e+00,\n",
       "              -8.1824e-01,  1.1844e-01],\n",
       "             [-1.2334e+00,  4.0289e-01, -4.1039e+00,  ...,  2.7255e-01,\n",
       "              -9.4773e-01, -7.6409e-01],\n",
       "             ...,\n",
       "             [-9.0423e-01,  1.4264e+00, -2.3019e-01,  ..., -8.1958e-01,\n",
       "               1.2976e-01, -1.0081e+00],\n",
       "             [ 2.7279e-01,  1.6665e+00, -1.7681e+00,  ..., -5.6377e-01,\n",
       "              -1.2213e-02, -4.4814e-01],\n",
       "             [-8.7353e-01,  1.4625e+00, -7.5520e-02,  ..., -8.5398e-01,\n",
       "               1.3444e-01, -1.0402e+00]],\n",
       "  \n",
       "            [[ 1.7843e+00,  1.7307e-01,  1.8474e+00,  ...,  1.3269e+00,\n",
       "              -8.6536e-01, -1.1275e+00],\n",
       "             [-1.7347e+00, -4.2126e-01,  1.8031e+00,  ..., -9.7250e-01,\n",
       "              -1.2086e-02, -8.2845e-03],\n",
       "             [ 1.2622e+00, -8.0226e-01,  1.9230e+00,  ...,  1.0388e+00,\n",
       "               9.7922e-01, -3.4361e-01],\n",
       "             ...,\n",
       "             [ 6.4515e+00, -2.5691e+00,  7.3559e+00,  ...,  7.6330e+00,\n",
       "              -3.2111e+00, -3.2274e+00],\n",
       "             [ 5.9927e+00, -2.9345e+00,  8.1582e+00,  ...,  6.6971e+00,\n",
       "              -4.7485e+00, -2.6457e+00],\n",
       "             [ 6.6104e+00, -2.6805e+00,  7.5273e+00,  ...,  7.8273e+00,\n",
       "              -3.3536e+00, -3.2021e+00]],\n",
       "  \n",
       "            [[ 1.3848e-01,  1.3382e-01, -1.0545e-01,  ...,  1.8984e-01,\n",
       "              -3.4012e-01, -2.5155e-02],\n",
       "             [-9.5574e-01, -2.9557e-01,  1.5038e+00,  ...,  4.4021e-01,\n",
       "              -1.8121e-01,  1.2305e+00],\n",
       "             [-1.1769e-01,  1.5981e-01, -2.7390e-01,  ...,  5.8114e-02,\n",
       "               8.0108e-01, -7.8723e-02],\n",
       "             ...,\n",
       "             [ 2.5359e+00, -1.1967e+00,  4.8737e-01,  ...,  1.0169e+00,\n",
       "               3.1231e+00,  1.3402e-01],\n",
       "             [ 2.0860e+00, -1.3404e+00,  1.4930e+00,  ...,  1.6625e+00,\n",
       "               3.3391e+00,  6.2623e-01],\n",
       "             [ 2.5595e+00, -1.2567e+00,  4.8819e-01,  ...,  1.1011e+00,\n",
       "               3.1878e+00,  1.1695e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-1.4330e-01, -1.1125e+00,  1.2784e-01,  ..., -9.6856e-02,\n",
       "               3.9662e-02,  7.5653e-02],\n",
       "             [-4.6940e-01,  3.0301e+00, -1.8223e+00,  ..., -5.6182e-01,\n",
       "              -1.9719e+00, -4.3371e-01],\n",
       "             [-1.3592e+00,  7.8946e-01, -6.9511e-01,  ...,  8.3361e-01,\n",
       "              -1.0995e+00,  1.5486e-01],\n",
       "             ...,\n",
       "             [-1.4273e+00, -3.0889e+00, -1.7460e-01,  ...,  2.0498e+00,\n",
       "              -2.1883e+00, -1.8069e+00],\n",
       "             [-1.3390e+00, -2.2423e+00, -3.2350e-01,  ...,  1.6105e+00,\n",
       "              -3.9416e+00, -2.6857e+00],\n",
       "             [-1.3849e+00, -3.2182e+00, -1.4738e-01,  ...,  2.0622e+00,\n",
       "              -2.2773e+00, -1.8409e+00]],\n",
       "  \n",
       "            [[ 1.3482e-01, -2.9507e-01,  5.6703e-01,  ...,  1.0898e+00,\n",
       "               3.2482e-01, -2.3599e-02],\n",
       "             [-5.6551e-01, -6.4587e-01, -1.2605e+00,  ..., -4.7155e+00,\n",
       "              -2.9846e-01, -8.6159e-01],\n",
       "             [-1.1860e-02, -1.5711e+00, -1.3479e+00,  ..., -1.2150e+00,\n",
       "               4.4109e-01, -1.5147e+00],\n",
       "             ...,\n",
       "             [ 1.4615e+00, -1.3878e+00,  1.8757e+00,  ...,  3.5135e+00,\n",
       "              -6.5171e-01, -4.9758e-01],\n",
       "             [ 4.3581e-01, -8.9862e-01,  1.8466e+00,  ...,  2.4336e+00,\n",
       "              -9.1073e-01, -8.0294e-02],\n",
       "             [ 1.5089e+00, -1.4183e+00,  1.9243e+00,  ...,  3.7652e+00,\n",
       "              -6.5864e-01, -3.8773e-01]],\n",
       "  \n",
       "            [[ 9.2106e-01, -6.0047e-01,  8.7529e-02,  ...,  4.1618e-01,\n",
       "               4.3836e-01,  1.6913e-01],\n",
       "             [ 1.4848e+00, -1.8421e+00,  1.2808e+00,  ...,  1.0000e+00,\n",
       "              -1.1346e+00,  1.2150e+00],\n",
       "             [ 1.4640e+00, -3.7169e-01, -1.2154e-02,  ...,  1.1695e+00,\n",
       "               4.9029e-01,  5.4550e-01],\n",
       "             ...,\n",
       "             [ 1.2602e+00, -1.8831e+00, -1.4602e+00,  ...,  2.8330e+00,\n",
       "               1.6484e+00, -1.4097e+00],\n",
       "             [ 1.7661e+00, -2.6651e+00, -1.0273e+00,  ...,  2.1976e+00,\n",
       "              -5.0749e-02, -1.7200e+00],\n",
       "             [ 1.2591e+00, -1.8880e+00, -1.4761e+00,  ...,  2.8063e+00,\n",
       "               1.5843e+00, -1.4621e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-1.4193e-01,  2.1976e-01,  1.2551e-01,  ..., -9.5327e-02,\n",
       "              -1.0484e-01, -8.7670e-02],\n",
       "             [ 1.7174e+00,  2.0334e+00, -9.2068e-01,  ...,  1.2619e-01,\n",
       "               1.9880e+00, -1.7541e+00],\n",
       "             [ 3.2213e-01,  9.6757e-01,  4.8397e-01,  ...,  6.8499e-01,\n",
       "               4.6540e-01,  1.1371e-01],\n",
       "             ...,\n",
       "             [ 3.8857e-01,  6.7228e-01,  1.9031e+00,  ...,  8.2911e-01,\n",
       "              -6.1086e-02,  3.0047e-01],\n",
       "             [ 1.4374e+00,  1.8598e+00,  3.8459e-01,  ...,  1.3647e+00,\n",
       "               1.0759e+00, -1.5020e+00],\n",
       "             [ 4.4441e-01,  6.4935e-01,  1.8172e+00,  ...,  7.1834e-01,\n",
       "              -9.0189e-02,  2.0138e-01]],\n",
       "  \n",
       "            [[-1.6054e-01, -2.9684e-01, -1.8202e-01,  ..., -8.9942e-02,\n",
       "              -4.9850e-02, -7.4437e-02],\n",
       "             [ 1.2724e+00,  2.4158e+00,  8.4902e-01,  ..., -7.9608e-03,\n",
       "              -7.3562e-01, -2.3337e-01],\n",
       "             [ 2.9652e-01, -2.7902e-01, -4.7435e-01,  ..., -1.2875e+00,\n",
       "               5.5154e-01, -1.1449e-02],\n",
       "             ...,\n",
       "             [-7.9831e-01, -1.1952e-01, -7.7449e-01,  ..., -1.9535e+00,\n",
       "              -1.6713e-01, -4.9686e-01],\n",
       "             [-2.8111e-01,  8.3772e-01, -9.8066e-01,  ..., -1.9979e+00,\n",
       "              -7.4465e-01, -1.7014e-01],\n",
       "             [-8.3804e-01, -1.5263e-01, -7.6826e-01,  ..., -1.9346e+00,\n",
       "              -1.5150e-01, -4.9915e-01]],\n",
       "  \n",
       "            [[-1.7407e-01, -3.7663e-02,  1.0152e-01,  ...,  6.1656e-02,\n",
       "               1.8936e-01,  1.3889e-01],\n",
       "             [ 2.5385e-01,  5.5175e-01,  3.8710e-01,  ..., -4.9093e-01,\n",
       "              -7.3466e-01,  1.0957e+00],\n",
       "             [ 1.1181e-01, -2.1531e-01, -8.0064e-01,  ...,  2.3635e-02,\n",
       "              -2.8929e-01, -6.5819e-01],\n",
       "             ...,\n",
       "             [-8.4514e-02,  2.8505e-01, -5.6647e-01,  ...,  9.9958e-02,\n",
       "               1.1791e+00,  9.2019e-02],\n",
       "             [ 1.9857e-01,  9.6409e-01, -6.8282e-01,  ..., -3.1281e-01,\n",
       "               3.3856e-01,  1.5680e+00],\n",
       "             [-4.2736e-02,  3.1813e-01, -5.7075e-01,  ...,  9.7042e-02,\n",
       "               1.1830e+00,  9.7430e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-6.1854e-02,  1.5219e-01,  3.8176e-02,  ...,  1.1879e-01,\n",
       "              -1.0173e-01,  2.5631e-01],\n",
       "             [ 2.5597e-01, -1.5963e+00, -3.8434e-01,  ..., -9.2291e-01,\n",
       "              -1.6733e-01, -1.0866e+00],\n",
       "             [ 4.3841e-01,  6.9563e-01,  7.5351e-01,  ...,  1.5627e-01,\n",
       "               1.3347e+00,  1.0270e+00],\n",
       "             ...,\n",
       "             [ 1.2977e+00, -6.3770e-01,  1.2925e+00,  ...,  4.2165e-01,\n",
       "               6.1576e-01,  1.3251e+00],\n",
       "             [ 4.9071e-01, -2.2035e+00,  8.9428e-02,  ..., -1.0772e-01,\n",
       "              -8.1806e-01, -1.1161e-01],\n",
       "             [ 1.3188e+00, -6.5788e-01,  1.2972e+00,  ...,  4.6899e-01,\n",
       "               5.2662e-01,  1.3232e+00]],\n",
       "  \n",
       "            [[ 8.0342e-02,  3.1236e-02,  1.5771e-01,  ...,  1.8027e-01,\n",
       "               2.7733e-02,  1.5062e-01],\n",
       "             [-4.8490e-01, -2.1014e-01,  1.5100e-01,  ...,  1.6893e+00,\n",
       "               1.5314e+00, -3.8153e-01],\n",
       "             [-1.4744e+00,  3.7450e-01,  8.2778e-01,  ...,  1.0125e+00,\n",
       "               1.3094e+00,  1.6447e-01],\n",
       "             ...,\n",
       "             [-6.0433e-01,  1.4829e-01,  3.6789e-01,  ...,  1.0004e+00,\n",
       "               2.1375e+00, -6.7600e-01],\n",
       "             [-5.0285e-01,  1.5441e-01,  9.8772e-01,  ...,  1.7075e+00,\n",
       "               1.1775e+00, -3.1753e-01],\n",
       "             [-5.2505e-01,  1.3058e-01,  4.1900e-01,  ...,  9.7927e-01,\n",
       "               2.0930e+00, -6.5377e-01]],\n",
       "  \n",
       "            [[-1.0647e-01, -1.0394e-01, -1.2336e-01,  ...,  2.2990e-01,\n",
       "              -7.5569e-02, -8.2221e-02],\n",
       "             [-1.5905e-01,  3.1457e-01,  3.1631e-01,  ..., -5.9599e-01,\n",
       "              -6.2033e-01,  1.9923e-01],\n",
       "             [-6.1743e-01, -6.5234e-02, -1.3838e-01,  ..., -2.1462e-01,\n",
       "              -9.4081e-01, -1.0362e+00],\n",
       "             ...,\n",
       "             [-5.5370e-01,  2.1646e-01, -1.1931e-01,  ...,  3.3600e-01,\n",
       "              -9.0798e-01, -3.7384e-01],\n",
       "             [-1.5450e-01, -3.3756e-01,  1.2874e+00,  ..., -3.5643e-01,\n",
       "               1.3048e-01,  5.9807e-01],\n",
       "             [-5.1705e-01,  2.3171e-01, -6.3643e-02,  ...,  3.3825e-01,\n",
       "              -8.8192e-01, -3.9846e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 1.8027e-01,  7.5461e-04,  2.3850e-01,  ...,  1.3871e+00,\n",
       "               2.9911e-01,  2.1594e-01],\n",
       "             [-7.8824e-01,  4.3092e-01,  1.8194e+00,  ..., -2.6896e+00,\n",
       "              -2.5671e+00, -1.7007e-01],\n",
       "             [-2.8774e-01,  1.2918e+00,  6.2638e-01,  ..., -2.5072e+00,\n",
       "              -1.9606e-01, -7.8911e-01],\n",
       "             ...,\n",
       "             [-8.7987e-01,  1.4322e-01,  7.0794e-01,  ..., -3.2964e-02,\n",
       "              -5.0382e-01, -6.6921e-01],\n",
       "             [-1.0716e+00, -3.4049e-01,  1.6845e+00,  ...,  5.7890e-01,\n",
       "              -2.2676e+00, -2.7539e-01],\n",
       "             [-8.6890e-01,  4.8599e-02,  7.3749e-01,  ...,  1.3483e-01,\n",
       "              -5.3672e-01, -6.7282e-01]],\n",
       "  \n",
       "            [[ 5.4798e-02, -2.1423e-01,  7.4569e-01,  ...,  2.2229e-01,\n",
       "              -8.7351e-02,  2.0732e-01],\n",
       "             [ 1.8219e+00,  5.8821e-01, -2.0124e+00,  ...,  5.6362e-01,\n",
       "               2.5628e-01,  1.3906e+00],\n",
       "             [ 2.5506e-01, -1.2909e+00, -2.3909e+00,  ..., -2.7489e-01,\n",
       "               1.6423e+00,  1.2155e+00],\n",
       "             ...,\n",
       "             [ 1.7150e+00, -1.7978e+00, -4.7017e-01,  ...,  3.4562e-01,\n",
       "               2.1228e+00,  2.1023e+00],\n",
       "             [ 2.7706e+00, -1.7052e+00,  3.2495e-01,  ...,  5.4775e-01,\n",
       "               2.3870e+00,  2.2331e+00],\n",
       "             [ 1.7369e+00, -1.8217e+00, -3.1760e-01,  ...,  3.6189e-01,\n",
       "               2.1813e+00,  2.1321e+00]],\n",
       "  \n",
       "            [[ 8.7318e-02,  3.3052e-01,  6.2967e-01,  ...,  1.4984e-01,\n",
       "               1.8454e-01,  1.2409e-01],\n",
       "             [-1.2808e+00,  6.0221e-01, -2.0623e-01,  ...,  3.7366e-01,\n",
       "               1.2254e+00, -7.6948e-01],\n",
       "             [-1.7421e-01,  7.9473e-01,  3.3923e-01,  ..., -1.2332e+00,\n",
       "               7.6260e-01, -1.6328e-01],\n",
       "             ...,\n",
       "             [-1.6184e-01,  8.6737e-01,  5.7822e-01,  ..., -4.6709e-01,\n",
       "               6.6804e-01,  8.0891e-02],\n",
       "             [-9.6585e-01,  9.0728e-01, -5.2769e-01,  ...,  2.7699e-01,\n",
       "              -2.2544e-01,  3.0169e-01],\n",
       "             [-1.9067e-01,  8.4514e-01,  5.8380e-01,  ..., -4.6943e-01,\n",
       "               6.6226e-01,  5.1599e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 2.6856e-01,  8.8726e-02,  3.2200e-01,  ..., -5.2333e-01,\n",
       "               1.2339e-02, -1.2732e-01],\n",
       "             [-4.1138e-01,  8.1252e-01, -9.5610e-01,  ...,  2.4976e-01,\n",
       "              -1.6647e+00,  4.8598e-01],\n",
       "             [ 9.8685e-01, -1.7914e+00, -9.0734e-01,  ..., -9.0234e-01,\n",
       "               1.2767e+00,  4.7515e-01],\n",
       "             ...,\n",
       "             [ 2.7181e-01, -3.6437e+00,  7.1021e-01,  ..., -1.3013e+00,\n",
       "               7.3182e-01, -2.0994e-01],\n",
       "             [ 1.7475e-01, -2.7206e+00,  4.8408e-01,  ..., -1.1127e+00,\n",
       "              -8.7733e-01,  2.6706e-01],\n",
       "             [ 2.0927e-01, -3.6462e+00,  7.6479e-01,  ..., -1.2971e+00,\n",
       "               7.2158e-01, -2.3324e-01]],\n",
       "  \n",
       "            [[-2.9320e-01,  6.2091e-02,  8.8453e-02,  ..., -3.6115e-01,\n",
       "              -3.5452e-02,  7.3534e-03],\n",
       "             [ 1.4868e+00,  2.2568e-01, -8.0983e-01,  ...,  1.8122e+00,\n",
       "               1.6197e+00,  2.5910e+00],\n",
       "             [ 8.1656e-01,  7.0976e-01, -6.3347e-02,  ...,  5.5968e-01,\n",
       "               1.3582e-01,  2.0041e-01],\n",
       "             ...,\n",
       "             [ 5.2555e-01,  2.6536e-01,  8.2882e-02,  ..., -2.1166e+00,\n",
       "               8.9868e-01, -2.3135e+00],\n",
       "             [ 2.1523e+00, -1.3651e-01, -1.6191e-01,  ..., -1.6805e+00,\n",
       "               1.3857e+00, -1.2237e+00],\n",
       "             [ 5.6323e-01,  2.4931e-01,  1.1835e-01,  ..., -2.2247e+00,\n",
       "               9.1748e-01, -2.4350e+00]],\n",
       "  \n",
       "            [[ 2.1357e-02, -2.3502e-01,  5.3432e-02,  ...,  1.2665e-01,\n",
       "               1.0988e-01, -1.8252e-01],\n",
       "             [ 1.1956e+00,  7.2363e-01,  6.2911e-02,  ..., -6.1963e-01,\n",
       "               1.1160e-01, -1.3090e-01],\n",
       "             [-3.1462e-01,  1.1621e+00,  2.3795e-01,  ...,  1.1620e+00,\n",
       "              -3.5170e-01,  5.7834e-01],\n",
       "             ...,\n",
       "             [-2.3509e+00,  1.0813e+00,  7.5645e-01,  ...,  8.9207e-01,\n",
       "              -4.9020e-01,  5.0270e-01],\n",
       "             [-2.1497e+00,  4.0503e-01,  4.1496e-01,  ..., -3.4130e-02,\n",
       "               5.1769e-01, -8.0151e-01],\n",
       "             [-2.4305e+00,  1.0686e+00,  7.1002e-01,  ...,  8.6243e-01,\n",
       "              -4.3973e-01,  5.0854e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 1.6860e-01,  9.3417e-02, -1.3401e-01,  ...,  1.6013e-01,\n",
       "              -2.1127e-01,  6.0922e-02],\n",
       "             [-6.9370e-01, -1.1658e+00, -6.6914e-01,  ...,  7.5296e-01,\n",
       "              -1.2386e-01,  3.1232e-01],\n",
       "             [ 1.6283e+00, -3.0457e-01,  6.4167e-01,  ...,  9.8505e-01,\n",
       "               7.1228e-01,  3.8351e-01],\n",
       "             ...,\n",
       "             [ 1.5811e+00, -6.7422e-01,  3.0336e-01,  ...,  9.9743e-01,\n",
       "               7.3817e-01, -8.3021e-01],\n",
       "             [ 7.1862e-01, -7.5777e-01, -6.8733e-01,  ...,  9.8309e-01,\n",
       "               6.5858e-01, -1.0394e+00],\n",
       "             [ 1.6077e+00, -6.2839e-01,  2.7645e-01,  ...,  9.4632e-01,\n",
       "               8.1335e-01, -9.0022e-01]],\n",
       "  \n",
       "            [[-1.1954e-02, -1.1543e-01, -3.0793e-01,  ...,  5.6343e-02,\n",
       "              -3.3205e-01,  1.9663e-01],\n",
       "             [ 1.1990e+00, -7.0437e-01,  1.4361e+00,  ..., -1.3195e+00,\n",
       "              -1.2408e-01, -1.5006e+00],\n",
       "             [ 4.9311e-01,  5.4443e-01,  1.0154e+00,  ...,  8.1392e-01,\n",
       "              -6.9691e-01, -1.3208e-01],\n",
       "             ...,\n",
       "             [ 6.0459e-01,  3.8829e-03, -2.1965e-01,  ...,  9.2987e-01,\n",
       "              -4.0680e-01, -3.6630e-01],\n",
       "             [ 1.6519e+00, -6.8339e-01,  6.8179e-02,  ..., -9.6331e-01,\n",
       "               9.5155e-01, -1.5415e+00],\n",
       "             [ 5.4861e-01, -5.9308e-02, -2.2522e-01,  ...,  9.2706e-01,\n",
       "              -3.3390e-01, -3.9962e-01]],\n",
       "  \n",
       "            [[-5.1941e-02, -2.2396e-01, -2.0413e-01,  ..., -8.7052e-02,\n",
       "               5.6029e-02, -1.4984e-01],\n",
       "             [ 3.3276e-01,  1.6351e-01,  1.9216e-01,  ...,  1.4765e+00,\n",
       "              -9.2817e-01,  1.8998e+00],\n",
       "             [-1.7456e+00,  3.6567e-01, -1.2842e+00,  ..., -4.3035e-01,\n",
       "              -8.0880e-01,  5.2751e+00],\n",
       "             ...,\n",
       "             [-1.1211e+00,  6.4422e-01, -1.6814e+00,  ...,  2.0298e-01,\n",
       "              -1.7585e-01,  3.6025e+00],\n",
       "             [ 1.2902e+00, -4.3128e-01, -8.1505e-02,  ...,  1.0756e+00,\n",
       "              -3.6664e-01,  7.2480e-02],\n",
       "             [-1.0806e+00,  6.2305e-01, -1.6903e+00,  ...,  2.0071e-01,\n",
       "              -1.8220e-01,  3.4788e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-3.8027e-01, -4.0654e-01,  3.1190e-01,  ...,  4.7975e-01,\n",
       "               2.1680e-01, -3.0893e-01],\n",
       "             [-1.2741e-01,  1.9662e-01,  1.2317e-02,  ..., -6.7248e-01,\n",
       "              -1.8503e+00, -9.4523e-01],\n",
       "             [-2.2417e-01, -2.0096e-01,  1.0359e+00,  ...,  3.3461e-01,\n",
       "              -3.9151e-01, -4.2620e-01],\n",
       "             ...,\n",
       "             [ 3.2101e-01, -2.0099e-01,  4.2574e-01,  ...,  1.8685e-01,\n",
       "              -6.1960e-01,  1.8035e-01],\n",
       "             [ 1.6138e-01,  4.5141e-01, -2.6567e-01,  ..., -4.4470e-01,\n",
       "              -2.2374e+00, -3.6056e-02],\n",
       "             [ 2.7775e-01, -1.6329e-01,  3.8470e-01,  ...,  1.4558e-01,\n",
       "              -6.3315e-01,  1.4248e-01]],\n",
       "  \n",
       "            [[ 1.1950e-01, -6.1611e-02, -1.8663e-01,  ...,  6.6358e-02,\n",
       "               7.8154e-02, -1.0299e-01],\n",
       "             [-2.8338e+00, -1.1946e+00,  5.1238e-01,  ..., -1.5235e+00,\n",
       "               5.0046e-02,  2.6793e-01],\n",
       "             [-6.8270e-01,  4.2171e-01, -2.9975e-01,  ...,  2.3492e-01,\n",
       "               9.1121e-01, -1.8729e-01],\n",
       "             ...,\n",
       "             [ 7.9067e-01,  4.7576e-01,  3.3231e-02,  ...,  6.5831e-01,\n",
       "               8.6210e-01,  1.8020e-01],\n",
       "             [-1.3089e+00, -1.7519e+00,  5.0876e-01,  ..., -5.0022e-02,\n",
       "               5.2008e-01,  7.4356e-01],\n",
       "             [ 8.2782e-01,  4.4182e-01,  8.7032e-03,  ...,  6.9404e-01,\n",
       "               9.0272e-01,  1.4671e-01]],\n",
       "  \n",
       "            [[-7.7069e-02,  1.0782e-01,  1.1658e-02,  ...,  6.1299e-02,\n",
       "              -9.2770e-02, -1.4468e-01],\n",
       "             [ 5.9941e-01,  1.4788e-01, -4.8524e-01,  ..., -1.5537e+00,\n",
       "              -8.7137e-01,  1.2707e-02],\n",
       "             [ 5.9306e-01,  2.6453e-02,  5.0147e-01,  ...,  2.2775e-01,\n",
       "               6.1684e-01,  3.9077e-01],\n",
       "             ...,\n",
       "             [ 4.6960e-01,  9.4719e-01,  2.9457e-01,  ..., -6.7474e-01,\n",
       "               8.0134e-01,  7.1939e-01],\n",
       "             [ 8.7411e-01,  6.4449e-01, -1.1855e+00,  ..., -2.1342e+00,\n",
       "              -7.4908e-01, -9.9778e-02],\n",
       "             [ 4.9481e-01,  9.4906e-01,  3.2488e-01,  ..., -6.4252e-01,\n",
       "               7.3193e-01,  7.3225e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 6.6615e-02,  6.0832e-01, -4.7964e-01,  ...,  1.3632e-01,\n",
       "              -9.9073e-02, -9.0911e-01],\n",
       "             [ 3.6406e-01, -4.9409e-01, -2.9283e-01,  ...,  5.3283e-01,\n",
       "               1.8498e-02,  5.7925e-03],\n",
       "             [ 7.3907e-01, -1.7766e+00, -8.5893e-01,  ..., -1.2064e-02,\n",
       "              -3.8492e-02, -7.3187e-01],\n",
       "             ...,\n",
       "             [ 1.1354e+00, -2.1696e+00, -9.5136e-01,  ...,  1.1017e+00,\n",
       "               4.1215e-01, -8.2494e-01],\n",
       "             [ 1.2213e+00, -1.1367e+00, -2.2582e-01,  ...,  2.1398e+00,\n",
       "               5.6479e-01, -1.1099e+00],\n",
       "             [ 1.1778e+00, -2.1122e+00, -9.5004e-01,  ...,  1.1893e+00,\n",
       "               3.7140e-01, -8.9399e-01]],\n",
       "  \n",
       "            [[ 1.2187e-02,  7.8845e-01,  4.9964e-01,  ...,  5.8986e-02,\n",
       "               2.7161e-01, -4.4144e-01],\n",
       "             [ 7.8673e-01,  1.1204e+00,  3.5772e-01,  ..., -4.7365e-01,\n",
       "               3.2460e-01, -1.1671e+00],\n",
       "             [ 5.4521e-01,  1.2924e+00, -1.9031e-01,  ...,  4.7603e-02,\n",
       "               1.7241e+00, -1.0002e+00],\n",
       "             ...,\n",
       "             [ 4.9768e-02,  1.6369e+00, -1.8572e-01,  ..., -7.5291e-02,\n",
       "               3.6897e+00, -1.1238e+00],\n",
       "             [-9.6341e-02,  1.3252e+00, -1.1345e-01,  ..., -9.0504e-01,\n",
       "               3.9211e+00, -1.0618e+00],\n",
       "             [ 3.9368e-02,  1.6338e+00, -1.7518e-01,  ..., -3.9679e-02,\n",
       "               3.7395e+00, -1.1432e+00]],\n",
       "  \n",
       "            [[-8.5318e-02, -5.0549e-03,  1.8199e+00,  ...,  3.3701e-01,\n",
       "              -1.2126e+00, -7.9893e-02],\n",
       "             [ 5.0266e-01, -4.3630e-01, -1.7187e+00,  ...,  1.6472e-01,\n",
       "               4.6963e-01,  1.2524e+00],\n",
       "             [ 1.9592e-01,  1.5726e-03,  1.1578e+00,  ...,  6.4496e-01,\n",
       "              -6.1938e-01,  1.2030e-01],\n",
       "             ...,\n",
       "             [ 6.2853e-01, -7.6749e-01,  3.8992e+00,  ...,  7.2322e-01,\n",
       "              -2.1454e+00,  2.1041e-01],\n",
       "             [ 9.9720e-01, -1.0300e+00,  2.0191e+00,  ...,  3.2456e-01,\n",
       "              -1.1709e+00,  6.0874e-02],\n",
       "             [ 6.5969e-01, -8.0843e-01,  4.0804e+00,  ...,  7.6266e-01,\n",
       "              -2.2430e+00,  1.7994e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 3.6574e-01,  1.1903e+00,  9.8930e-01,  ..., -3.6188e-01,\n",
       "              -6.4226e-01,  4.6684e-01],\n",
       "             [ 2.4320e+00, -4.2003e-01,  5.5116e-01,  ..., -2.1063e+00,\n",
       "              -2.5617e-01, -8.3429e-01],\n",
       "             [ 1.0009e+00,  4.1185e-01,  1.8455e+00,  ..., -2.2559e-01,\n",
       "              -3.6371e-01,  4.9991e-01],\n",
       "             ...,\n",
       "             [ 1.3961e+00,  5.7823e-01,  3.3300e+00,  ...,  5.7331e+00,\n",
       "               1.2755e-01, -2.1673e+00],\n",
       "             [ 2.7557e+00,  2.1018e-01,  3.1832e+00,  ...,  5.2571e+00,\n",
       "              -4.7276e-01, -2.9170e+00],\n",
       "             [ 1.3755e+00,  5.1930e-01,  3.3370e+00,  ...,  5.9363e+00,\n",
       "               1.1298e-01, -2.2010e+00]],\n",
       "  \n",
       "            [[ 5.2429e-02, -1.4332e-01,  8.8792e-02,  ..., -2.7514e-01,\n",
       "              -2.6421e-01, -3.7105e-01],\n",
       "             [ 3.1488e-01, -6.8475e-01, -2.4824e-01,  ..., -9.7153e-01,\n",
       "              -9.3588e-01, -1.6398e+00],\n",
       "             [ 7.3513e-01,  4.5702e-01, -9.7447e-01,  ..., -7.1012e-01,\n",
       "              -9.8893e-01, -3.9196e-01],\n",
       "             ...,\n",
       "             [-2.2036e-01,  7.8090e-01, -9.4945e-01,  ..., -1.1649e+00,\n",
       "              -4.0440e+00, -5.5184e-01],\n",
       "             [-1.1075e+00,  2.4947e-01, -6.5430e-01,  ..., -1.7041e+00,\n",
       "              -4.9874e+00, -1.0262e+00],\n",
       "             [-2.7656e-01,  8.4039e-01, -9.3745e-01,  ..., -1.2238e+00,\n",
       "              -4.1358e+00, -5.8057e-01]],\n",
       "  \n",
       "            [[-1.9041e-01, -1.0983e+00, -1.3844e-01,  ..., -1.7704e-01,\n",
       "              -1.6084e+00, -1.6774e+00],\n",
       "             [-8.3762e-01, -1.4886e+00, -4.6962e-01,  ...,  1.7815e-01,\n",
       "              -1.3161e+00, -6.0492e-01],\n",
       "             [-1.0768e-01, -8.0412e-01, -1.3242e+00,  ..., -4.3617e-01,\n",
       "              -2.0145e+00, -8.9839e-01],\n",
       "             ...,\n",
       "             [ 7.9296e-01, -3.6966e-01,  1.2761e-01,  ..., -5.1042e-01,\n",
       "              -1.4039e+00, -1.4803e+00],\n",
       "             [ 1.5778e-01, -9.9760e-01,  3.2951e-01,  ...,  2.6292e-01,\n",
       "              -1.2221e+00, -1.6887e+00],\n",
       "             [ 8.4111e-01, -3.8233e-01,  2.3182e-01,  ..., -5.1156e-01,\n",
       "              -1.3926e+00, -1.5235e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-4.0222e-01, -1.3492e-01,  2.3634e-01,  ...,  6.6472e-02,\n",
       "               3.5365e-01,  1.7240e-02],\n",
       "             [ 9.9387e-01,  1.4693e+00,  1.7769e+00,  ..., -1.1020e+00,\n",
       "               5.5229e-01, -2.3767e-01],\n",
       "             [-6.0168e-01, -6.5283e-02,  2.0450e-01,  ..., -4.3109e-02,\n",
       "               3.8749e-01,  5.2931e-01],\n",
       "             ...,\n",
       "             [-9.7931e-01, -5.7004e-01,  9.1523e-01,  ..., -5.6859e-02,\n",
       "               8.1573e-01,  1.2169e+00],\n",
       "             [ 2.3384e-01,  9.2300e-01,  2.3591e+00,  ..., -1.4308e+00,\n",
       "               8.9592e-01,  8.2579e-01],\n",
       "             [-9.9361e-01, -5.7807e-01,  9.2539e-01,  ..., -8.7061e-02,\n",
       "               7.7379e-01,  1.1757e+00]],\n",
       "  \n",
       "            [[-3.2203e-01,  2.2255e-01, -3.0696e-02,  ..., -1.0983e-01,\n",
       "              -1.5164e-01, -1.3908e-01],\n",
       "             [-5.7625e-01,  1.3223e-01,  5.5197e-01,  ...,  1.2983e+00,\n",
       "               1.6463e-01,  1.6416e+00],\n",
       "             [-8.8620e-04,  1.3972e-01, -1.0652e+00,  ...,  7.9735e-01,\n",
       "              -4.8895e-01, -5.7817e-01],\n",
       "             ...,\n",
       "             [-2.9517e-01,  3.9489e-01, -4.7385e-01,  ...,  1.5720e-01,\n",
       "              -4.0356e-01, -8.1057e-01],\n",
       "             [-7.6714e-02,  4.5322e-01,  8.6148e-01,  ...,  6.5187e-01,\n",
       "              -3.1327e-01,  7.1030e-01],\n",
       "             [-2.3619e-01,  3.7388e-01, -4.7233e-01,  ...,  1.4813e-01,\n",
       "              -4.2750e-01, -8.3750e-01]],\n",
       "  \n",
       "            [[-3.4737e-01, -6.6538e-01, -1.4102e-01,  ..., -3.7922e-01,\n",
       "               7.7607e-02, -3.1144e-01],\n",
       "             [ 4.1450e-01, -1.2975e+00,  8.3287e-02,  ...,  1.1104e+00,\n",
       "               1.5150e+00,  1.5179e+00],\n",
       "             [-4.3630e-02, -1.9480e+00, -3.4169e-01,  ..., -1.9637e-01,\n",
       "               7.5193e-02, -1.2153e-01],\n",
       "             ...,\n",
       "             [-9.5914e-02, -4.2690e-01, -4.3369e-01,  ..., -7.2699e-02,\n",
       "               9.6740e-02, -3.9675e-01],\n",
       "             [-3.1023e-02, -5.7642e-01,  3.6555e-01,  ...,  6.7368e-01,\n",
       "               1.3400e+00,  8.5771e-01],\n",
       "             [-5.4967e-02, -3.6839e-01, -4.1169e-01,  ..., -6.9794e-02,\n",
       "               1.0722e-01, -4.1704e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-4.7797e-02, -2.0053e-01,  1.6933e-01,  ..., -3.2388e-01,\n",
       "              -1.0444e-01, -3.7925e-02],\n",
       "             [-2.8723e-01, -7.0007e-02,  5.4297e-01,  ..., -7.1863e-01,\n",
       "               6.3360e-02,  1.5822e-01],\n",
       "             [-6.5045e-01, -2.5599e-01,  9.9382e-01,  ..., -4.8836e-01,\n",
       "               8.2334e-01,  4.3500e-02],\n",
       "             ...,\n",
       "             [-1.7043e-01, -6.8230e-01,  7.3170e-01,  ...,  2.6212e-01,\n",
       "               8.2788e-01, -3.0503e-01],\n",
       "             [-4.2758e-01, -1.6562e-01,  3.4579e-01,  ..., -1.8357e-01,\n",
       "               4.3252e-01, -3.0873e-01],\n",
       "             [-1.6493e-01, -6.7484e-01,  6.8031e-01,  ...,  2.5228e-01,\n",
       "               8.2553e-01, -3.1823e-01]],\n",
       "  \n",
       "            [[ 8.3323e-02,  3.0383e-01, -6.9427e-02,  ...,  4.5188e-02,\n",
       "              -6.4996e-02,  7.5110e-02],\n",
       "             [ 2.9699e-01, -9.5230e-01, -7.1017e-01,  ..., -7.6028e-01,\n",
       "              -9.2141e-01, -1.4632e+00],\n",
       "             [-2.8251e-01, -3.9354e-01, -8.7663e-01,  ...,  2.3163e-01,\n",
       "              -2.4260e-01,  2.0104e-01],\n",
       "             ...,\n",
       "             [-3.9468e-01, -1.3828e-01,  6.3829e-01,  ...,  8.4154e-01,\n",
       "              -8.1455e-01,  4.0797e-01],\n",
       "             [-6.9137e-02, -5.6917e-01,  4.7718e-02,  ..., -3.1876e-01,\n",
       "              -1.0225e+00, -4.3917e-01],\n",
       "             [-4.0118e-01, -1.0799e-01,  6.8332e-01,  ...,  8.2821e-01,\n",
       "              -8.1812e-01,  3.9673e-01]],\n",
       "  \n",
       "            [[-1.1143e-01, -3.3076e-02, -6.2504e-02,  ..., -5.3652e-02,\n",
       "               1.4412e-01, -1.1772e-01],\n",
       "             [ 6.3368e-01,  5.2622e-01, -8.3218e-01,  ..., -3.6903e-01,\n",
       "              -2.9545e-01, -2.1842e-01],\n",
       "             [ 1.0448e-01,  8.1367e-01,  5.6495e-01,  ...,  2.1679e-01,\n",
       "              -1.7677e+00, -6.6624e-02],\n",
       "             ...,\n",
       "             [ 1.9913e-01,  4.0581e-01,  9.2101e-01,  ...,  6.9523e-01,\n",
       "              -9.0900e-01,  5.6784e-01],\n",
       "             [ 5.1563e-01,  6.7310e-01,  4.6081e-01,  ...,  1.4200e-01,\n",
       "              -2.2363e-01,  8.0402e-01],\n",
       "             [ 1.6800e-01,  4.1659e-01,  9.6482e-01,  ...,  7.0922e-01,\n",
       "              -8.6311e-01,  5.3320e-01]]]]], grad_fn=<StackBackward>)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.forward(chat_history_ids)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model(chat_history_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(q[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = answer - context\n",
    "if len(answer) < 2:\n",
    "    r2 = 0\n",
    "else:\n",
    "    vec_a, vec_b = answer, answer\n",
    "    r2 = sum(vec_a*vec_b) / sum(abs(vec_a)*abs(vec_b))\n",
    "    r2 = -F.logsigmoid(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Adam' object has no attribute 'zerograd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-67e55e3af91e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzerograd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mr2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Adam' object has no attribute 'zerograd'"
     ]
    }
   ],
   "source": [
    "optimizer.zerograd()\n",
    "r2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = q\n",
    "v2 = a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50256, 15496, 50256, 15496, 50256, 15496,  5145, 50256, 15496,  5145,\n",
       "         50256, 17250,  5145, 50256]),\n",
       " tensor([15496,  5145,  1058,    35, 50256]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1,v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0057,  0.0238, -0.0812,  ..., -0.0227, -0.0378, -0.0031],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings()(v2).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(size=(3,tokenizer.vocab_size))\n",
    "x[:,-1] = 1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[103.3320,  96.6074,  96.4836,  ..., 109.7336, 112.1754, 125.7009],\n",
       "         [107.1582,  98.9198,  98.5507,  ..., 115.2873, 114.8348, 128.2074],\n",
       "         [116.1806, 104.2458, 106.2476,  ..., 122.1931, 125.4122, 139.5822]],\n",
       "        grad_fn=<MmBackward>),\n",
       " tensor([[ -11.4564,  -17.4578,  -19.1987,  ...,  -16.9864,  -16.7421,\n",
       "            -7.3800],\n",
       "         [-206.5140, -198.9877, -200.6138,  ..., -230.4619, -229.1432,\n",
       "          -207.2097],\n",
       "         [-180.3867, -184.7284, -181.1404,  ..., -203.2211, -201.3414,\n",
       "          -167.4208],\n",
       "         [-157.9265, -160.9077, -163.2302,  ..., -188.6371, -188.0802,\n",
       "          -162.3012],\n",
       "         [-200.9722, -208.1657, -210.0039,  ..., -236.2414, -233.3469,\n",
       "          -213.2841],\n",
       "         [-179.1016, -196.8122, -194.7751,  ..., -215.5321, -212.9505,\n",
       "          -185.1048]], grad_fn=<MmBackward>))"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = model(v1)[0]\n",
    "B = model(v2)[0]\n",
    "A,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.1308, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "A = model(v1)[0]\n",
    "B = model(v2)[0]\n",
    "max_len = np.max([len(A.detach().numpy()),len(B.detach().numpy())])\n",
    "print(max_len)\n",
    "extra = torch.zeros(size=(max_len-len(A.detach().numpy()),tokenizer.vocab_size))\n",
    "extra[:,-1] = 1\n",
    "# A = torch.cat([A , extra ])\n",
    "B = torch.cat((B , torch.tensor([torch.ones(tokenizer.vocab_size)]*(max_len-len(B.detach().numpy()))) ))\n",
    "A, B\n",
    "\n",
    "A , B = torch.cat([torch.softmax(A, dim=-1), extra]), torch.softmax(B, dim=-1)\n",
    "A , B\n",
    "# loss = F.cosine_similarity(A,B, dim=-2)\n",
    "loss = -torch.mm(A, B.transpose(1, 0)).sum()\n",
    "\n",
    "# loss = -F.cosine_similarity(torch.softmax(A, dim=-1),torch.softmax(B, dim=-1))\n",
    "# loss = 0\n",
    "# for i,j in zip(v1,v2):\n",
    "#     loss += i*j\n",
    "# # loss = -model(v1, past=None)[0].sum()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' shore Term marine'"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "logits = model(v1)[0]\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "prev_input = torch.multinomial(probs, num_samples=1)\n",
    "decode(prev_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[75.3702, 73.6392, 72.8180,  ..., 81.0872, 82.3673, 87.0727],\n",
       "        [74.3895, 72.5374, 71.8523,  ..., 80.4320, 81.0682, 86.7291],\n",
       "        [73.9969, 72.2922, 71.6807,  ..., 80.3298, 80.4573, 86.1744]],\n",
       "       grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! :D'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9997, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.get_output_embeddings()(model.get_input_embeddings()(v1)).mean(dim=0)\n",
    "y = model.get_output_embeddings()(model.get_input_embeddings()(v2)).mean(dim=0)\n",
    "-torch.log(F.cosine_similarity(x,y, dim=-1))\n",
    "F.cosine_similarity(x,y, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-122.8433, -150.5232, -149.6123,  ..., -168.9333, -161.6223,\n",
       "         -133.7793],\n",
       "        [-204.0837, -205.4568, -208.4964,  ..., -240.3041, -233.0705,\n",
       "         -217.4363],\n",
       "        [-205.8837, -210.4453, -214.1883,  ..., -246.4192, -239.3104,\n",
       "         -219.0015]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(v1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-242-6ee6c6e0f6f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-10d376e69163>\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(token_ids)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces)\u001b[0m\n\u001b[0;32m    753\u001b[0m                 \u001b[0mcurrent_sub_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcurrent_sub_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m             \u001b[0msub_texts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_tokens_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_sub_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\tokenization_gpt2.py\u001b[0m in \u001b[0;36mconvert_tokens_to_string\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconvert_tokens_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;34m\"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyte_decoder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, NoneType found"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['<pad>'])\n",
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check learned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-e5569e72ff68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# generated a response while limiting the total chat history to 1000 tokens,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     chat_history_ids = B.generate(input_ids, max_length=1000, \n\u001b[1;32m---> 24\u001b[1;33m                                       \u001b[0mpad_token_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m#                                       num_beams=3,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#                                       early_stopping=True,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_kwargs)\u001b[0m\n\u001b[0;32m    492\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m             )\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36m_generate_no_beam_search\u001b[1;34m(self, input_ids, cur_len, max_length, min_length, do_sample, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id, batch_size, attention_mask, use_cache, model_kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             )\n\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    729\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 731\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    732\u001b[0m         )\n\u001b[0;32m    733\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m             )\n\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcross_attn_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# add cross attentions if we output attention weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m         \u001b[0mfeed_forward_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         \u001b[1;31m# residual connection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfeed_forward_hidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[0mh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1017\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1019\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1020\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msize_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "A = model\n",
    "B = model\n",
    "chat_history_ids = tokenizer.encode(tokenizer.bos_token + \"Hello\" + tokenizer.eos_token, return_tensors='pt')\n",
    "for frame in range(15):\n",
    "    epsilon = epsilon_by_frame(frame)\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "#     new_user_input_ids = tokenizer.encode(tokenizer.eos_token, return_tensors='pt')\n",
    "    input_ids = chat_history_ids\n",
    "    chat_history_ids = A.generate(input_ids, max_length=1000, \n",
    "                             pad_token_id=tokenizer.eos_token_id, \n",
    "#                              num_beams=3,\n",
    "# #                              num_return_sequences=1,\n",
    "#                              early_stopping=True,\n",
    "#                              no_repeat_ngram_size=3\n",
    "                            ) if frame > 0 else input_ids\n",
    "    question = chat_history_ids[:, input_ids.shape[-1]:][0] if frame > 0 else input_ids[0]\n",
    "    print(\"User: {}\".format(decode(question)))\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    input_ids = chat_history_ids # if step > 0 else new_user_input_ids\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = B.generate(input_ids, max_length=1000, \n",
    "                                      pad_token_id=tokenizer.eos_token_id,\n",
    "#                                       num_beams=3,\n",
    "#                                       early_stopping=True,\n",
    "#                                       num_return_sequences=3,\n",
    "#                                       no_repeat_ngram_size=3\n",
    "                                     )\n",
    "\n",
    "    # pretty print last output tokens from bot\n",
    "    answer = chat_history_ids[:, input_ids.shape[-1]:][0]\n",
    "    print(\"DialoGPT: {}\".format(decode(answer)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292, 292)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.base_model.parameters())), len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-5168ea7c7dbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "model.get_head_mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
