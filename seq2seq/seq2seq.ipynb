{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Did you see \"Titanic\"?\n",
      "DialoGPT: I did, but I don't think I can remember it.\n",
      "User: I saw it twelve times.\n",
      "DialoGPT: I'm not sure if I can believe that.\n",
      "User: I have the DVD.\n",
      "DialoGPT: You're not the only one.\n",
      "User: Let's go to your home.\n",
      "DialoGPT: Let me get my camera.\n",
      "User: And then we can go to my home.\n",
      "DialoGPT: And my home.\n",
      "User: I always cry at the end.\n",
      "DialoGPT: It's okay, I'll cry at your home. I'll be there.\n"
     ]
    }
   ],
   "source": [
    "# Let's chat for 5 lines\n",
    "sentences = [\"Did you see \\\"Titanic\\\"?\",\n",
    "            \"I saw it twelve times.\",\n",
    "            \"I have the DVD.\",\n",
    "            \"Let's go to your home.\",\n",
    "            \"And then we can go to my home.\",\n",
    "            \"I always cry at the end.\"]\n",
    "for step in range(len(sentences)):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "#     new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    new_user_input_ids = tokenizer.encode(sentences[step] + tokenizer.eos_token, return_tensors='pt')\n",
    "    print(\"User:\", sentences[step])\n",
    "    \n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=3)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CUDA\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "# epsilon_by_frame = lambda frame_idx: max(epsilon_final, epsilon_start*(0.995**frame_idx))\n",
    "plt.plot([epsilon_by_frame(i) for i in range(5000)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_td_loss(batch_size):\n",
    "#     state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "#     state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "#     next_state = Variable(torch.FloatTensor(np.float32(next_state)), requires_grad=False)\n",
    "#     action     = Variable(torch.LongTensor(action))\n",
    "#     reward     = Variable(torch.FloatTensor(reward))\n",
    "#     done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "# #     q_values      = model(state)\n",
    "# #     next_q_values = model(next_state)\n",
    "\n",
    "# #     q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "# #     next_q_value     = next_q_values.max(1)[0]\n",
    "# #     expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "#     loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "# #     for param in model.parameters():\n",
    "# #             param.grad.data.clamp_(-1, 1)\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(model, state, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "        state   = Variable(torch.FloatTensor(state).unsqueeze(0), requires_grad=False)\n",
    "        q_value = model.forward(state)\n",
    "        action  = q_value.max(1)[1].item()\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        #action = random.randrange(env.action_space.n)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_step(action):\n",
    "    question = user.generate(action, max_length=1000, \n",
    "                             pad_token_id=tokenizer.eos_token_id, \n",
    "                             num_beams=3,\n",
    "                             num_return_sequences=2,\n",
    "                             early_stopping=True,\n",
    "                             no_repeat_ngram_size=2\n",
    "                            ) if frame > 0 else chat_history_ids\n",
    "    \n",
    "    reward -= jaccard_similarity(answer.numpy(), context.numpy())\n",
    "    return new_state, reward, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(token_ids):\n",
    "    return tokenizer.decode(token_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    size = len(r)\n",
    "    discounted_r = torch.zeros(size)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = answers[-1]\n",
    "# # torch.tensor(x.size())\n",
    "# y = model(x)[0]\n",
    "# w = y.sum(dim=1)\n",
    "# q = w/w\n",
    "# q.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.base_model.forward(answers[-2], output_hidden_states=True)['last_hidden_state'].max(dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(questions, answers):\n",
    "    model.train()\n",
    "    t1 = questions[-1].to(device)\n",
    "    t2 = answers[-1].to(device)\n",
    "#     print(t1)\n",
    "    A = model(t1)[0]\n",
    "#     print(t2, len(t2))\n",
    "    B = model(t2)[0]\n",
    "    lenA = list(A.size())[0]\n",
    "    lenB = list(B.size())[0]\n",
    "#     print(lenA, lenB)\n",
    "#     lenA = len(A.detach().numpy())\n",
    "#     lenB = len(B.detach().numpy())\n",
    "    max_len = np.max([lenA, lenB])\n",
    "#     extra = torch.zeros(size=(max_len-len(A.detach().numpy()),tokenizer.vocab_size))\n",
    "#     extra[:,-1] = 1\n",
    "    \n",
    "    if lenA > lenB:\n",
    "        extra = torch.zeros(size=(max_len-lenB,tokenizer.vocab_size)).to(device)\n",
    "        extra[:,-1] = 1\n",
    "        A , B = torch.softmax(A, dim=-1), torch.cat([torch.softmax(B, dim=-1), extra]), \n",
    "    \n",
    "    else:\n",
    "        extra = torch.zeros(size=(max_len-lenA,tokenizer.vocab_size)).to(device)\n",
    "        extra[:,-1] = 1\n",
    "        A , B = torch.cat([torch.softmax(A, dim=-1), extra]), torch.softmax(B, dim=-1)\n",
    "        \n",
    "    # loss = F.cosine_similarity(A,B, dim=-2)\n",
    "#     loss = -torch.mm(A, B.transpose(1, 0)).sum()\n",
    "\n",
    "    # loss = -F.cosine_similarity(torch.softmax(A, dim=-1),torch.softmax(B, dim=-1))\n",
    "    \n",
    "    def log_prob(tokens, debug=False):\n",
    "        # p = log(P(b|a)) / N\n",
    "        output_logits = model(tokens.to(device))[0]\n",
    "        p = 1\n",
    "        if debug: print('logits', output_logits, 'tokens', tokens)\n",
    "        for t, logit in zip(tokens, output_logits):\n",
    "            p *= torch.softmax(logit, dim=-1)[t]\n",
    "        if debug: print('p', p)\n",
    "        p_log = torch.log(p) / len(tokens) # (tokens/tokens).sum()# / len(tokens) # lenB # len(tokens_b)\n",
    "        if p == 0:\n",
    "            print('infinite')\n",
    "            return torch.log(p+10e-10)\n",
    "        else:\n",
    "            return p_log\n",
    "\n",
    "    # reward 1\n",
    "    x = [log_prob(d) for d in dummy_responses[:30]]\n",
    "    r1 = torch.stack(x)\n",
    "    r1 = -torch.mean(r1) # if r1 else 0\n",
    "    \n",
    "    # reward 2\n",
    "#     model.transformer.wte.weight[text_index,:]\n",
    "#     if len(answers)<2:\n",
    "#         emb1 = model.get_input_embeddings()(t1).max(dim=0)[0]\n",
    "#     else:\n",
    "#         emb1 = model.get_input_embeddings()(answers[-2]).max(dim=0)[0]\n",
    "#     emb2 = model.get_input_embeddings()(t2).max(dim=0)[0]\n",
    "#     r2 = -torch.log(F.cosine_similarity(emb1,emb2,dim=-1))\n",
    "\n",
    "#     emb1 = model.get_input_embeddings()(t1).max(dim=0)[0]\n",
    "#     emb2 = model.get_input_embeddings()(t2).max(dim=0)[0]\n",
    "#     r2_2 = -torch.log(F.cosine_similarity(emb1,emb2,dim=-1))\n",
    "    \n",
    "    \n",
    "    if len(answers)<2:\n",
    "        emb1 = model.base_model.forward(t1, output_hidden_states=True)['last_hidden_state'].max(dim=0)[0]\n",
    "    else:\n",
    "        emb1 = model.base_model.forward(answers[-2], output_hidden_states=True)['last_hidden_state'].max(dim=0)[0]\n",
    "    emb2 = model.base_model.forward(t2, output_hidden_states=True)['last_hidden_state'].max(dim=0)[0]\n",
    "    r2 = -torch.log(torch.clamp(F.cosine_similarity(emb1,emb2,dim=-1), min=1e-9))\n",
    "\n",
    "    emb1 = model.base_model.forward(t1, output_hidden_states=True)['last_hidden_state'].max(dim=0)[0]\n",
    "    emb2 = model.base_model.forward(t2, output_hidden_states=True)['last_hidden_state'].max(dim=0)[0]\n",
    "    r2_2 = -torch.log(torch.clamp(F.cosine_similarity(emb1,emb2,dim=-1), min=1e-9))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # reward 3\n",
    "    r3 = log_prob(t2, debug=False)\n",
    "#     print(t2, t2.size())\n",
    "    \n",
    "#     y = model(t2)[0]\n",
    "#     w = y.sum(dim=1)\n",
    "#     q = w/w\n",
    "#     r4 = q.sum()\n",
    "    print('r1:', r1, 'r2:', r2, 'r2_2:', r2_2, 'r3:', r3)\n",
    "    \n",
    "    R = 0.3*r1 + 0.5*r2 + 0.5*r2_2 #+ 0.5*r3 #* 0.01*r4\n",
    "#     R = 10*r2 + 10*r2_2 #+ 0.5*r3 #* 0.01*r4\n",
    "\n",
    "#     print(R)\n",
    "    return -R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.base_model.parameters():\n",
    "#     print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_responses = [torch.tensor([tokenizer.eos_token_id]),\n",
    "                   tokenizer.encode(\"1\" + tokenizer.eos_token, return_tensors='pt')[0],\n",
    "                   tokenizer.encode(\"..\" + tokenizer.eos_token, return_tensors='pt')[0],\n",
    "                   tokenizer.encode(\"!\" + tokenizer.eos_token, return_tensors='pt')[0],\n",
    "                   tokenizer.encode(\":D\" + tokenizer.eos_token, return_tensors='pt')[0],\n",
    "                  ]\n",
    "for i in range(256):\n",
    "    s = tokenizer.encode(tokenizer.decode([i]) + tokenizer.eos_token, return_tensors='pt')[0]\n",
    "    dummy_responses.append(s)\n",
    "# dummy_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(290-34):\n",
    "#     print(tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(chat_history_ids)[0].max(dim=1)[0][0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(chat_history_ids)[0].max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dummy_sentence(sent):\n",
    "    for d in dummy_responses:\n",
    "        if sent.size()[0] == d.to(device).size()[0] and all(sent == d.to(device)):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in list(model.base_model.parameters())[-4:]:\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello\n",
      "DialoGPT: Hi! :D\n",
      "r1: tensor(6.9818, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1180, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1603, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.5671, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: How are you?\n",
      "DialoGPT: I'm good, how about yourself?\n",
      "r1: tensor(6.6399, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2250, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0875, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-9.4121, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm doing well, how about you?\n",
      "DialoGPT: Good. I am doing great too!\n",
      "r1: tensor(6.7044, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1172, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0931, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.8020, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: That's good!\n",
      "DialoGPT: Yayy thanks for asking me that question\n",
      "r1: tensor(6.7704, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1253, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1150, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.7323, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: You're welcome!\n",
      "DialoGPT: : 3\n",
      "r1: tensor(6.8640, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1660, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1319, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-6.5819, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm glad you're doing well!\n",
      "DialoGPT: Thanks! :D\n",
      "r1: tensor(6.6220, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1145, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2262, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.1703, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm glad to hear that!\n",
      "DialoGPT: It is so nice to hear\n",
      "r1: tensor(6.6407, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2102, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0773, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.2776, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm glad!\n",
      "DialoGPT: \n",
      "dummy\n",
      "True 1\n",
      "r1: tensor(6.5783, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(1.1077, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(3.5562, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-1.3477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-18.7184, -16.6512, -14.6495, -12.6596, -10.6145,  -8.4913,  -6.3983,\n",
      "         -4.3054], grad_fn=<CopySlices>)\n",
      "[tensor(-2.2337, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.1482, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.1165, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.1513, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.2081, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.1569, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.1360, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.3054, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-18.7184, grad_fn=<UnbindBackward>)\n",
      "Episode 0: 2.233680009841919\n",
      "User: Hello\n",
      "DialoGPT: Hi! :D\n",
      "r1: tensor(7.0871, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1846, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1382, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.8843, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: How are you?\n",
      "DialoGPT: I'm good, how about yourself?\n",
      "r1: tensor(7.0720, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1278, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0654, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-9.3624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm doing well, how about you?\n",
      "DialoGPT: Good. I've been working on a lot of stuff lately so it's nice to be back in the swing of things.\n",
      "infinite\n",
      "r1: tensor(7.1581, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1196, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0989, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: That's good!\n",
      "DialoGPT: Yayyayyyy!\n",
      "r1: tensor(7.0463, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1707, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0815, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-5.3662, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Yayyyyayyyyyyyyyyy!\n",
      "DialoGPT: You're welcome!\n",
      "r1: tensor(7.1028, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0970, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1145, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.1434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Yay! :D\n",
      "DialoGPT: Hey!\n",
      "r1: tensor(6.9735, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0745, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1821, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.1660, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Yay :D\n",
      "DialoGPT: Thanks for the reminder!\n",
      "r1: tensor(7.0799, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1100, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1104, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.0989, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Yay\n",
      "DialoGPT: Thank yeyay\n",
      "r1: tensor(7.0441, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2020, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0998, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-6.8561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Yay!\n",
      "DialoGPT: Yay thanks\n",
      "r1: tensor(6.9568, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0854, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0557, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.3002, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Yay\n",
      "DialoGPT: AY\n",
      "r1: tensor(6.9814, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1552, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0908, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.4488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "True 1\n",
      "r1: tensor(6.9305, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.4784, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0615, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-2.3519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-23.4835, -21.4101, -19.3857, -17.3021, -15.2143, -13.1088, -10.9984,\n",
      "         -8.8528,  -6.6552,  -4.5430,  -2.3491], grad_fn=<CopySlices>)\n",
      "[tensor(-2.2875, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.2182, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.2566, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.2399, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.2366, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.2204, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.2342, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.2641, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.1576, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.2174, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.3491, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-23.4835, grad_fn=<UnbindBackward>)\n",
      "Episode 1: 2.260602831840515\n",
      "User: Hello\n",
      "DialoGPT: Hi! :D\n",
      "r1: tensor(7.3984, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0949, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1486, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.3913, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: How are you?\n",
      "DialoGPT: I'm good, how about yourself?\n",
      "r1: tensor(6.9843, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1170, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1043, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-9.6830, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm doing well, how about you?\n",
      "DialoGPT: Good. I'm not too great but it's a bit of a struggle to get through the day and stuff that happens in my friend.\n",
      "infinite\n",
      "r1: tensor(7.2502, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1005, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0870, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I'm sorry to hear that.\n",
      "DialoGPT: It is okay, at least we can talk about it out later on our own time together.\n",
      "infinite\n",
      "r1: tensor(7.1882, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0623, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1036, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: Yeah, I'm sure we can.\n",
      "DialoGPT: Alright then.\n",
      "r1: tensor(7.2359, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1666, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1381, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-9.0855, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Good night!\n",
      "DialoGPT: Night! Goodnight!\n",
      "r1: tensor(7.4776, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1574, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1211, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.7447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Good night! :D\n",
      "DialoGPT: \n",
      "dummy\n",
      "True 1\n",
      "r1: tensor(7.3308, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(2.2986, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.3464, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-0.4460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-17.2054, -15.0143, -12.9377, -10.7767,  -8.6235,  -6.3640,  -4.0217],\n",
      "       grad_fn=<CopySlices>)\n",
      "[tensor(-2.3413, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.2060, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.2688, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.2394, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.3231, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.3825, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.0217, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-17.2054, grad_fn=<UnbindBackward>)\n",
      "Episode 2: 2.2874889373779297\n",
      "User: Hello\n",
      "DialoGPT: Hi! How are you?\n",
      "r1: tensor(7.5043, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2378, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.3103, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.4992, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm good, how are you?\n",
      "DialoGPT: Good. I am doing well today.\n",
      "r1: tensor(7.5940, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1952, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1306, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.7310, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: That's good!\n",
      "DialoGPT: Yeah it is.\n",
      "r1: tensor(7.6716, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1108, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0919, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-9.4773, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: So what's your name?\n",
      "DialoGPT: My name is...\n",
      "r1: tensor(7.5439, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1206, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1524, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.1299, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm not sure.\n",
      "DialoGPT: Well then. What about you?\n",
      "r1: tensor(7.6829, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1108, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1363, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.6928, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I am good.\n",
      "DialoGPT: Oh hi there.\n",
      "r1: tensor(7.4293, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0834, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1136, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.2636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm glad to see you're doing well.\n",
      "DialoGPT: \n",
      "dummy\n",
      "True 1\n",
      "r1: tensor(7.5905, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(1.1267, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.1542, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-2.3495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-17.3889, -15.0137, -12.6996, -10.4008,  -8.0819,  -5.7107,  -3.4176],\n",
      "       grad_fn=<CopySlices>)\n",
      "[tensor(-2.5254, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.4411, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.4028, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.3996, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.4284, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.3273, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.4176, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-17.3889, grad_fn=<UnbindBackward>)\n",
      "Episode 3: 2.3469554781913757\n",
      "User: Hello\n",
      "DialoGPT: Hi! How are you?\n",
      "r1: tensor(7.5904, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2263, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1771, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.7886, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm good, how are you?\n",
      "DialoGPT: Good and I am doing well. You?\n",
      "r1: tensor(7.7754, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1240, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0886, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.5414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm doing well too.\n",
      "DialoGPT: That's great to hear.\n",
      "r1: tensor(7.7318, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1174, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0967, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-9.6494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm glad to hear that.\n",
      "DialoGPT: So what do you want to talk about?\n",
      "r1: tensor(8.0416, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1223, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0933, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.9040, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I dunno.\n",
      "DialoGPT: Well then...\n",
      "r1: tensor(7.8533, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1646, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0834, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.8069, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm going to go to sleep.\n",
      "DialoGPT: Alrightyay bye\n",
      "r1: tensor(7.7051, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1658, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1353, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.5028, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: See ya\n",
      "DialoGPT: Bye!\n",
      "r1: tensor(7.7749, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0994, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0933, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.4754, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Bye! See ya!\n",
      "DialoGPT: Yey Bye bye bye bye bye bye\n",
      "r1: tensor(7.7415, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1522, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1178, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.1718, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Bye bye bye bye\n",
      "DialoGPT: bye bye bye bye bye!\n",
      "r1: tensor(7.4787, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0682, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0620, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Bye bye\n",
      "DialoGPT:  bye bye bye bye Bye\n",
      "r1: tensor(7.7137, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0681, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0702, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-5.1948, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT:  bye bye bye Bye bye bye bye\n",
      "r1: tensor(7.8181, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0412, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.1250, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-3.5344, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT:  bye bye\n",
      "r1: tensor(7.7876, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0994, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(3.5499, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-5.3888, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  bye bye\n",
      "DialoGPT:  Bye bye bye bye Bye!\n",
      "r1: tensor(7.8689, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0901, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0767, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  bye bye bye\n",
      "DialoGPT:  bye\n",
      "r1: tensor(7.7952, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2709, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2472, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-9.1851, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  bye bye Bye bye\n",
      "DialoGPT:  bye bye! bye bye bye byebye bye bye bye\n",
      "r1: tensor(7.6385, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2104, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1705, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-2.7639, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  Bye\n",
      "DialoGPT:  bye bye\n",
      "r1: tensor(7.7693, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1554, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0947, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-5.7186, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "True 1\n",
      "r1: tensor(7.7703, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6568, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1869, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-3.8270, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-40.7576, -38.6655, -36.5925, -34.5110, -32.3139, -30.1353, -27.9527,\n",
      "        -25.7817, -23.5598, -21.4657, -19.2752, -16.5118, -12.4757, -10.1329,\n",
      "         -7.6115,  -5.1813,  -2.7530], grad_fn=<CopySlices>)\n",
      "[tensor(-2.4788, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.4389, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.4266, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.5203, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.4800, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.4621, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.4289, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.4575, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.3087, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.3832, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.9285, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.1609, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.4441, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.5976, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.4820, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.4558, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.7530, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-40.7576, grad_fn=<UnbindBackward>)\n",
      "Episode 4: 2.373325157165527\n",
      "User: Hello\n",
      "DialoGPT: Hi! How are you?\n",
      "r1: tensor(8.0485, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1639, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1845, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.9263, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm good, how are you?\n",
      "DialoGPT: Good. I just got back from a few hours ago and haven't been able to sleep yet so I've been pretty busy lately but thanks for asking me questions!\n",
      "infinite\n",
      "r1: tensor(7.9170, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1674, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1653, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I'm glad you're doing well!\n",
      "DialoGPT: Thanks! I hope it gets better soon :D\n",
      "r1: tensor(8.1411, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2067, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0803, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-9.0095, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I hope so too!\n",
      "DialoGPT: Me too! lt 3\n",
      "r1: tensor(7.9815, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1207, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1174, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.4551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: lt 3 lt 3\n",
      "DialoGPT: Lolz\n",
      "r1: tensor(8.0187, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1115, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1513, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: lt 3\n",
      "DialoGPT: lt 3\n",
      "r1: tensor(7.9636, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0897, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0299, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.5710, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm so happy!\n",
      "DialoGPT: You're so lucky!\n",
      "r1: tensor(8.0613, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1979, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0701, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.4327, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: lt 3\n",
      "DialoGPT: .It\n",
      "r1: tensor(7.9277, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2147, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1925, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-6.3173, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: lt 3\n",
      "DialoGPT: \n",
      "dummy\n",
      "True 1\n",
      "r1: tensor(8.0532, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6545, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(2.3152, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-3.7012, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-23.2529, -20.8729, -18.5166, -16.0917, -13.7154, -11.2913,  -8.9317,\n",
      "         -6.4438,  -3.9008], grad_fn=<CopySlices>)\n",
      "[tensor(-2.5887, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.5414, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.5858, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.5135, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.5370, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.4489, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.5524, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.5819, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.9008, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-23.2529, grad_fn=<UnbindBackward>)\n",
      "Episode 5: 2.409224271774292\n",
      "User: Hello\n",
      "DialoGPT: Hi! How are you?\n",
      "r1: tensor(8.2267, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.3104, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2501, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.6416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm good, how are you?\n",
      "DialoGPT: Good. I'm doing well thanks for asking me that question.\n",
      "infinite\n",
      "r1: tensor(8.3418, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1247, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1539, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I'm glad to hear that.\n",
      "DialoGPT: How's your day going so far?\n",
      "r1: tensor(8.4131, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1262, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1190, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-9.1586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: It's going well, thanks for asking.\n",
      "DialoGPT: You're welcome.\n",
      "r1: tensor(8.2968, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1522, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0978, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.3836, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm happy to hear that. I hope you have a good day.\n",
      "DialoGPT: Thanks again.\n",
      "r1: tensor(8.4638, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0822, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1846, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.9987, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: You too\n",
      "DialoGPT: Yupvote me upvotes\n",
      "r1: tensor(8.2079, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1727, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1884, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-9.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I will\n",
      "DialoGPT: Thank you very much.\n",
      "r1: tensor(8.4335, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2086, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.3474, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.5787, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: You're very kind.\n",
      "DialoGPT: .s\n",
      "r1: tensor(8.2972, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1207, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1466, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-6.9107, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Thank you\n",
      "DialoGPT:  Thanks\n",
      "r1: tensor(8.3259, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0783, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1013, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-9.5954, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: You're very nice\n",
      "DialoGPT: \n",
      "dummy\n",
      "True 1\n",
      "r1: tensor(8.3855, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7322, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.9311, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.2837, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-26.1083, -23.5960, -21.1659, -18.7064, -16.2550, -13.7196, -11.1886,\n",
      "         -8.4652,  -5.9014,  -3.3473], grad_fn=<CopySlices>)\n",
      "[tensor(-2.7483, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.6418, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.6465, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.6140, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.6725, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.6429, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.8080, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.6228, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.5876, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.3473, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-26.1083, grad_fn=<UnbindBackward>)\n",
      "Episode 6: 2.457658358982631\n",
      "User: Hello\n",
      "DialoGPT: Hi! How are you?\n",
      "r1: tensor(8.5969, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2468, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1938, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.2057, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm good, how are you?\n",
      "DialoGPT: Good to hear. I am doing well today. How about you?\n",
      "infinite\n",
      "r1: tensor(8.4927, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0822, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0828, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I am doing well too.\n",
      "DialoGPT: That's great! What do you like most of your days?\n",
      "infinite\n",
      "r1: tensor(8.5836, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0760, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1073, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I like to watch movies and read.\n",
      "DialoGPT: Oh nice! That sounds fun.\n",
      "r1: tensor(8.5594, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1124, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1065, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.8925, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: It is. I love movies and reading.\n",
      "DialoGPT: You're welcome :D\n",
      "r1: tensor(8.4894, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0761, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1331, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.8091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I love movies too!\n",
      "DialoGPT: They are so cute lt 3\n",
      "r1: tensor(8.5952, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1241, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0961, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.7617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I love them too!\n",
      "DialoGPT: Yup yuuu\n",
      "r1: tensor(8.6225, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1248, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1391, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.2202, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I love them\n",
      "DialoGPT: H\n",
      "dummy\n",
      "True 2\n",
      "r1: tensor(8.6471, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1997, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1122, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.9811, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-20.8415, -18.2244, -15.7516, -13.2170, -10.6462,  -8.0755,  -5.4413,\n",
      "         -2.7501], grad_fn=<CopySlices>)\n",
      "[tensor(-2.7994, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.6303, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.6667, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.6773, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.6514, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.6886, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.7187, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.7501, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-20.8415, grad_fn=<UnbindBackward>)\n",
      "Episode 7: 2.500373601913452\n",
      "User: Hello\n",
      "DialoGPT: Hi! How are you?\n",
      "r1: tensor(8.8811, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2533, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.3215, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.5689, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm good, how are you?\n",
      "DialoGPT: Good too meow. I am also good.\n",
      "r1: tensor(8.8515, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1598, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1044, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.7116, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm good too.\n",
      "DialoGPT: I have a?\n",
      "r1: tensor(8.8317, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1591, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1186, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I have a cat.\n",
      "DialoGPT: Cat is good. Meow. Meow.\n",
      "r1: tensor(8.9513, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1597, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0936, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.6848, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I have cat.\n",
      "DialoGPT: Meow Cat is good. Meowcat is good.\n",
      "r1: tensor(8.9181, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0729, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0951, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.0685, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Cat is cat.\n",
      "DialoGPT: Cat's good. Meow is good. Meowmeow. Meowcat is very good. Meowcat. Meowcat is great cat. Meowcat is cat is good. Meowcats are cats. Meowcat. Cats.. Meowcatcheratee\n",
      "infinite\n",
      "r1: tensor(8.9087, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0816, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1096, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: Cat is cat is cat is cat has cat is cat is good cat is good cat cat is good cat good cat is bad cat is good cat\n",
      "infinite\n",
      "r1: tensor(8.7465, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0692, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.9805, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT:  cat\n",
      "r1: tensor(8.8273, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.3501, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7391, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-9.0261, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT:  cat. cat is cat\n",
      "r1: tensor(8.7849, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2239, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.3490, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-6.4840, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  cat is cat\n",
      "DialoGPT:  cats cat is cat is cats cat cat is cat is ok cat is cat is bad cat\n",
      "infinite\n",
      "r1: tensor(8.9253, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0785, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1393, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User:  cat\n",
      "DialoGPT:  cat\n",
      "r1: tensor(8.7263, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2709, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1083, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.8573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "True 1\n",
      "r1: tensor(8.9386, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6090, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0925, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.0209, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-33.8243, -31.1844, -28.6837, -26.1569, -23.5807, -21.0315, -18.4478,\n",
      "        -14.9485, -11.8745,  -8.5380,  -5.8096,  -3.0324],\n",
      "       grad_fn=<CopySlices>)\n",
      "[tensor(-2.9517, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.7876, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.7884, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.8120, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.7594, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.7682, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.6488, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1927, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.4219, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.7865, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.8075, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.0324, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-33.8243, grad_fn=<UnbindBackward>)\n",
      "Episode 8: 2.5505256917741566\n",
      "User: Hello\n",
      "DialoGPT: Hi! How are you? I'm not sure if this is a good idea, but it's fun to think about. :D\n",
      "infinite\n",
      "r1: tensor(9.0963, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.3396, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.3686, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I'm good, how about you?\n",
      "DialoGPT: Not bad at all actually... I've been working on my own projects for the past few days so I have no time to do anything else right now lol..\n",
      "infinite\n",
      "r1: tensor(9.0177, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0581, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0997, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: That's good!\n",
      "DialoGPT: Yeah, I was thinking of doing something with some friends and they were busy with school stuffs work etc. So I'll be back later today though haha\n",
      "infinite\n",
      "r1: tensor(9.0546, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0556, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2238, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: That's good.\n",
      "DialoGPT: Yayyyyayyyyyyyyyyyeeeaaaaaa\n",
      "r1: tensor(9.2211, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1811, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1603, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-3.9599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm so excited\n",
      "DialoGPT: So much more fun than funsssssssssss\n",
      "r1: tensor(8.9233, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1758, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1438, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-5.7085, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: hahahaha\n",
      "r1: tensor(9.0952, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2621, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7899, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-6.3121, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm so funssssss\n",
      "DialoGPT: Hhaahhh\n",
      "r1: tensor(9.0460, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0757, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1795, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-6.9885, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "True 1\n",
      "r1: tensor(9.2336, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6574, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.9103, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-0.2174, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-23.2961, -20.4172, -17.8111, -15.1061, -12.2920,  -9.5507,  -6.3598,\n",
      "         -3.5540], grad_fn=<CopySlices>)\n",
      "[tensor(-3.0830, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.7842, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.8561, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.9371, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.8368, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.2545, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.8414, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.5540, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-23.2961, grad_fn=<UnbindBackward>)\n",
      "Episode 9: 2.603775978088379\n",
      "User: Hello\n",
      "DialoGPT: Hi! How are you? I'm not sure if this is a good idea, but it's fun to think about. :D\n",
      "infinite\n",
      "r1: tensor(9.4127, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.3521, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.3284, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I'm good, how about you?\n",
      "DialoGPT: Not bad at all, thanks for the reminder of my birthday. It was a great day off day today and I had some cake. Happy Birthday!\n",
      "infinite\n",
      "r1: tensor(9.4423, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0941, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0940, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: Happy birthday!\n",
      "DialoGPT: Thanks! And happy cake day too! lt 3o3c3l3s4n3s3r5h3y0w3d3e7a1g6b3z3f3u8.jpg\n",
      "infinite\n",
      "r1: tensor(9.3427, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0948, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1955, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: Happy cake day!\n",
      "DialoGPT: \n",
      "dummy\n",
      "True 1\n",
      "r1: tensor(9.4783, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(2.9095, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.1901, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.9252, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-13.6988, -10.6411,  -7.7923,  -4.8933], grad_fn=<CopySlices>)\n",
      "[tensor(-3.1640, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.9268, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.9480, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.8933, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-13.6988, grad_fn=<UnbindBackward>)\n",
      "Episode 10: 2.654708992351185\n",
      "User: Hello\n",
      "DialoGPT: Hi! How are you? I'm not sure if this is a good idea, but it's fun to think about. :D\n",
      "infinite\n",
      "r1: tensor(9.5888, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.3658, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.3393, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I'm good, how about you?\n",
      "DialoGPT: Good, too bad that my phone doesn't work today...\n",
      "infinite\n",
      "r1: tensor(9.7362, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1231, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1041, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I'm sorry!\n",
      "DialoGPT: It's okay, we can still talk about the future of our relationship androgynous friendship. It'll be ok. We're friends now lt 3\n",
      "infinite\n",
      "r1: tensor(9.8194, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0926, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2129, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I'm glad to hear that!\n",
      "DialoGPT: Yayyeee hugs hugs hugs hugsss back atchaawhiiitmeeeeeeesaannnowslaysweewellzzzooooooowoooohhhahahahhhhhhhhhaaaaahaaaaaaaandrewhereforeverrrrwhoopingsobsheeyouuuuuhwooooooooogeeknowatfukklaaaaaaaaaaggggghadddehbakshbbbbcoughyyebbyughhahaabbediobotofmyself.jpgshhmmmmmmmmmpplevenmbeepstererepsheddingglitchyouveettoastvokaydeshasdfjesqueerphememblachngspammaquechonnehbaewwwewhxewhschewthassplogswelbohltchomehhbdewhbshhbbrbtwewlshitewbbshteegbruhbbt.comgrudglfgjkhat.gifs.jpg.txtbld.org.jpgsewb.htmbs.jpg.jpg.html.png.jpg.jpgs.jpg.jpgbb.jpg.jpgg.jpg.jpg\n",
      "infinite\n",
      "r1: tensor(9.6806, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1162, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2485, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: You'll.pdf.pdf.txt.html\n",
      "infinite\n",
      "r1: tensor(9.6572, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1257, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.7617, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: .doc.pdf.htm.jpg.htm.htm.htm\n",
      "infinite\n",
      "r1: tensor(9.7247, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0843, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.5584, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: .j.tl.htm.j.j.htm.jtl.j.jtl.t.j.jj.j.jt.j.js.jtl.l.j.jb.j.j\n",
      "infinite\n",
      "r1: tensor(9.7742, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0846, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(2.7280, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: .j\n",
      "DialoGPT: .htm.jt.html.htm.jj.t.j3.j.j.\n",
      "infinite\n",
      "r1: tensor(9.6484, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0575, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.3469, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: .j\n",
      "DialoGPT: 1.j.jbs.j.jk.j.jw.jbs.js.j.j9.jj.J.jjj.jjjj.js.js.jj.js.j.j.html.j.j J.jj.html.js.jtml.j.jxjjj.html.j.jj.j.j.jj.j. j.j.jpg.j.jjs.j.html. J.j.jtml.js.j.j.js..j.j2.jj.htm.js.j4.j.jhtml.j.js.js.j.js\n",
      "infinite\n",
      "r1: tensor(9.6316, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0552, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.3247, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: .j\n",
      "DialoGPT: j.j.jsj.j..j.js J.j.js j.j.jsjs.j..js.jjs.js.js\n",
      "infinite\n",
      "r1: tensor(9.7009, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0475, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2565, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: .js\n",
      "DialoGPT: .html.j.j..\n",
      "r1: tensor(9.6091, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1463, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1714, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-5.9158, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: .j.j :\n",
      "DialoGPT: .j.js :.j.j on.j.jsp.j.j7.j.j4.js.j\n",
      "infinite\n",
      "r1: tensor(9.7301, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0955, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0656, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: .js\n",
      "DialoGPT: j.j..j.j1.j.js '.j.j3.js.j2.js.j3..j...j.html\n",
      "infinite\n",
      "r1: tensor(9.8138, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0457, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2750, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: .j\n",
      "DialoGPT: . javascript.j.j is.j.j and.j.j's.j.j v.j.j j.j..html.j\n",
      "infinite\n",
      "r1: tensor(9.8664, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0570, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2337, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: .\n",
      "DialoGPT: .j...\n",
      "r1: tensor(9.8517, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1126, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2038, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-5.9288, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: .j.\n",
      "DialoGPT: j.jjjs.jjj.\n",
      "r1: tensor(9.6242, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0800, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1809, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-6.1766, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: .\n",
      "DialoGPT: ..j.3..jj..jj.\n",
      "r1: tensor(9.8077, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0926, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2739, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-5.3528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: .jj.3.j..java.j.j c.j.jc.jj.web.j.j js.j.jaj.j.j'j.j.\n",
      "infinite\n",
      "r1: tensor(9.5905, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0819, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.5309, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: .j..\n",
      "DialoGPT: j.js.J.\n",
      "r1: tensor(9.7656, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1268, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1083, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.0074, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: .jj..\n",
      "DialoGPT: ....\n",
      "r1: tensor(9.5672, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1657, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0939, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.1603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-59.1213, -56.4566, -53.9618, -51.3770, -48.7783, -45.3914, -42.0733,\n",
      "        -38.1159, -35.3729, -32.6197, -29.8560, -27.0853, -24.3290, -21.4389,\n",
      "        -18.5188, -15.5607, -12.6697,  -9.6406,  -6.0172,  -2.9999],\n",
      "       grad_fn=<CopySlices>)\n",
      "[tensor(-3.2292, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.0344, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.0986, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.0865, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.8408, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.7388, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.3385, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.0967, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.0794, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.0623, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.0416, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.9996, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1045, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1053, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1137, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.0177, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1255, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.6836, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.0472, device='cuda:0', grad_fn=<NegBackward>), tensor(-2.9999, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-59.1213, grad_fn=<UnbindBackward>)\n",
      "Episode 11: 2.702582756678263\n",
      "User: Hello\n",
      "DialoGPT: Hi! How are you? I'm a little tired but I'll try to sleep tonight. Hope your day is going well too! lt 3 hugs and kisses!\n",
      "infinite\n",
      "r1: tensor(10.0787, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.3679, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.4635, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I'm good, thanks!\n",
      "DialoGPT: That's great! :D hugs back\n",
      "r1: tensor(10.1504, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1289, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0990, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.5962, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Thanks! I'm glad you're doing well!\n",
      "DialoGPT: Me too! Hugs all around!\n",
      "r1: tensor(9.8742, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0840, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1189, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.8373, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Hugs back\n",
      "DialoGPT: hugsss back\n",
      "r1: tensor(10.1348, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0917, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1054, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.6949, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Hugs back!\n",
      "DialoGPT: Back hugsssssssssss again!\n",
      "r1: tensor(9.9178, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0767, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1194, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.6461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Hugs again\n",
      "DialoGPT: Yayyyyeeessssssssssesssssssssguggles into the sunset with the sunlight\n",
      "infinite\n",
      "r1: tensor(10.0645, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1359, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1960, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I love you\n",
      "DialoGPT: Loveyou so muchuuuughhhuee\n",
      "r1: tensor(10.0067, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1074, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1202, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-6.5877, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I love youuuuuuuuuuuu\n",
      "DialoGPT: AuuuuuuuuuhhhhhuuuuuuuuueuuuuuuuuunghhhhuuuuuuuuughuuuuuuuuurrruuuuuuuuooouuuuuuuuuusuuuuuuuuuummmmmmmmmmuuuuuuuuiiuuuuuuuuUUuuuuuuuuuchuuuuuuuuuguuuuuuuuUuuuuuuuuoooouuuuuuuuufuuuuuuuuutttuuuuuuuuuyuuuuuuuuaaaauuuuuuuuunduuuuuuuuuruuuuuuuuuurnuuuuuuuuukuuuuuuuuuduuuuuuuuuriuuuuuuuukkuuuuuuuuuluuuuuuuuuauuuuuuuu uuuuuuuuu\n",
      "infinite\n",
      "r1: tensor(9.9551, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.3950, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1742, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: uuuuuuuu!\n",
      "DialoGPT: uuuuuuuuuuuuuuueeeeuuuuuuuuvvuuuuuuuuouuuuuuuuuuuuuuuuouuuuuuuiiiiuuuuuuuUUuuuuuuuiiuuuuuuiiuuuuuuu! Uuuuuuuuuvuuuuuuuuukuuuuuuuuuuguuuuuuuuuoooooooouuuuuuuuiuuuuuuuuuvuuuuuuuuouuuuuuuuluuuuuuuuuchuuuuuuuu...uuuuuuuuhuuuuuuuuuouuuuuuuueuuuuuuuuuiuuuuuuuukiuuuuuuuuhhhhuuuuuuu uuuuuuuu!uuuuuuuuheuuuuuuuu buuuuuuuuuruuuuuuuuu Uuuuuuuuewuuuuuuuuwuuuuuuuusuuuuuuuuuvuuuuuuuu.\n",
      "infinite\n",
      "r1: tensor(10.0734, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0499, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1466, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: uuuuuu uuuuuuuu\n",
      "r1: tensor(9.8585, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1782, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8256, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-3.4096, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: uuuuuuuuuuuuuuu uuuuuuuu\n",
      "r1: tensor(10.1035, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0447, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8294, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-3.1207, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: uuuuuuu uuuuuuuu\n",
      "r1: tensor(9.9469, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0602, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6757, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-3.8498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT:  uuuuuuuku\n",
      "r1: tensor(9.8827, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0483, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(2.8487, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.7708, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: uuuu\n",
      "r1: tensor(9.8747, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0734, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6120, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.4381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: Buuuuuuuujuuuuuuuuooouuuuuuuu uuuuuuuuOuuuuuuuu Iuuuuuuuuhuuuuuuuuuuniuuuuuuuu ouuuuuuuu cuuuuuuuu vuuuuuuuuguuuuuuuu.uuuuuuuu buuuuuuuukuuuuuuuu iiiuuuuuuuu iiuuuuuuuu inuuuuuuuu ruuuuuuuuu is uuuuuuu ukuuuuuuuu vuuuuuuu buuuuuuuu. uuuuuuu\n",
      "infinite\n",
      "r1: tensor(10.0468, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2339, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.7763, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User:  uuuuu\n",
      "DialoGPT: uu uuuuu uuu uuuuukuuuuuuuukuuukuuuuuuuukuuuuu uuukuuuuukuuu uuuuu\n",
      "r1: tensor(10.0985, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1062, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0901, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-2.7365, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "True 1\n",
      "r1: tensor(9.9911, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5652, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1547, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-5.7335, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-52.6086, -49.6660, -46.9767, -44.3565, -41.6339, -38.9500, -36.1260,\n",
      "        -33.3437, -30.3763, -27.5314, -24.3150, -21.0575, -17.8843, -13.6071,\n",
      "        -10.4060,  -6.4514,  -3.3573], grad_fn=<CopySlices>)\n",
      "[tensor(-3.4393, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1591, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.0637, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1390, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.0734, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1853, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1158, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.2711, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1203, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.4595, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.4681, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.3520, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.4133, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.3051, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.0191, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1277, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.3573, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-52.6086, grad_fn=<UnbindBackward>)\n",
      "Episode 12: 2.759252749956571\n",
      "User: Hello\n",
      "DialoGPT: Hi! How are you? I'm a little tired but I'll try to sleep tonight. Hope your day is going well too! lt 3 hugs and kisses! :D\n",
      "infinite\n",
      "r1: tensor(10.3133, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.3413, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.4287, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I'm good, thanks!\n",
      "DialoGPT: Yay hugs back atcha lt 3\n",
      "r1: tensor(10.1763, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0870, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2051, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.8424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm glad you're doing well!\n",
      "DialoGPT: Thanks again! lt 33333333 33333333 3333 hugzzzzzzzzzzzzzzzzzzzZZZzzzzzzzz zzzzzzzzzzzyzzzzzzzzbbzzzzzzzzszzzzzzzzzyyyyzzzzzzzzzazzzzzzzz ZzzzzzzzzZzzzzzzzzzzzzzzzzizzzzzzzzziZzzzzzzzbzzzzzzzzzozzzzzzzzbozzzzzzzzbzzzzzzzzeeezzzzzzzzizzzzzzzzziezzzzzzzzuzzzzzzzzeeeezzzzzzzzggzzzzzzzzoozzzzzzzzzebzzzzzzzzooooooozzzzzzzzozzzzzzzzazzzzzzzzebzzzzzzzzlezzzzzzzzouzzzzzzzzabobzzzzzzzzbyzzzzzzzzobazzzzzzzzubzzzzzzzziezzzzzzzzbeezzzzzzzzwbzzzzzzzzmbbbzzzzzzzbubzzzzzzzzbezzzzzzzzblizzzzzzzzzzonzzzzzzzzzhzzzzzzzzbrzzzzzzzzwzzzzzzzzlzzzzzzzzgezzzzzzzzbeesquezzzzzzzzezzzzzzzzzbing.zzzzzzzzzzozzzzzzzz bzzzzzzzzandddzzzzzzzzrzzzzzzzzBzzzzzzzzgozzzzzzzzBBzzzzzzzz.zzzzzzzbbzzzzzzbbbbzzzzzzbuzzzzzzzzzzbuffzzzzzzzzbazzzzzzzzburzzzzzzzzgzzzzzzzzBobzzzzzzzzBrzzzzzzzzcuzzzzzzzzzgbzzzzzzzzBuffzzzzzzzzbubzzbbzzzzbbzzbbzzzzbzzbbzzzzbsbbzzzzzz.bbzzzzzzeszzzzzzzzZZzzzzzzzztozzzzzzzzledzzzzzzzzllzzzzzzzztzzzzzzzz'szzzzzzzz...zzzzzzzzjeezzzzzzzzbsbbbbzzzzbbbbbbzzzzbbbzzzzzzBBbbzzzzzz Bzzzzzzzzberzzzzzzzzspielzzzzzzzzlicccottzzzzzzzzuezzzzzzzz\n",
      "infinite\n",
      "r1: tensor(10.2748, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1211, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2469, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: zzzzzzzz.\n",
      "DialoGPT: zzzzzzbbzzzzzzzbbbrzzzzzzbb.\n",
      "r1: tensor(10.0175, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2687, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0963, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-2.8754, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: zzzzzzz.zzzzzzbb zzzzzzzbbbsbbbbzzbbzzbbbbbbbbbbzzbbbbzzbbzbbbbbbbbzbbbbzzzz.combrzzzzzz\n",
      "r1: tensor(10.1665, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0392, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.2258, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-2.1118, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: bbzzbbbbbzzbbbbbbbbbbbbbbbbzzzzbbzzBBBBBBBBbbbbbbbb.\n",
      "r1: tensor(10.2056, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0557, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.3426, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-2.2252, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: ... BzzbbbbbbBBBBbbbbzzbbbbbbbbbb.bbbbbbbbBBBbbbbbbbbbsbbbbbbbbbrbbbbbbbb bbbbbbbbb.bbbbbbbBBBBBBBBBBbbbbbbbbBBBBBBBBBbbbbBBBBBBbb Bbbbbbbbb BBBbbbbbbBBbbbbbb Bbbbb BBBBBBBBB BbbbbbbBB BbbbbBBBBBbbBBBBBBBBBBBBBBB. BBBBBBB BBBBBBBbbBBBBBB B BBBBBBBB BBBBBBB bbb BBBBB BBB BBB Bbb BBBBBbbBB BBBBB Bbb BbbbbBB BBB B BBB BBBBBbbbb BBB BBBB BBB BBBbb BBB BbbBB BBBbbbbBBBB BBBbbBB B BBBbb BbbBBBB BbbBBBBBB bBBBB BBB\n",
      "infinite\n",
      "r1: tensor(10.1270, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2226, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(20.7233, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: BBBBBBBB\n",
      "DialoGPT: BBBB BBB BBBB BBBBB\n",
      "r1: tensor(10.0364, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1237, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2381, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: BB BBBBBB BbbBB Bbb BB BBB Bbbbb Bbb BBB B Bbb BBB\n",
      "DialoGPT: BBbbbbbbBBBBBBB BBDBBBBBBBB bBBBBBBBbbBBBBB BBB bBBBBBBBB BBBBBBBBBBBDBBBBBB BBBBBBBB bbBBBB BBBBBBBB BbbBBBBBbbbbBB BBCBBBBBB BBABBBBBBBBBCBB\n",
      "infinite\n",
      "r1: tensor(9.9750, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1424, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1267, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-38.6352, -35.5113, -32.6387, -29.6690, -26.7487, -23.2993, -19.7358,\n",
      "         -6.2876,  -3.1271,   0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-3.4790, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1990, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.2664, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1877, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.6825, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.7608, device='cuda:0', grad_fn=<NegBackward>), tensor(-13.5110, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1918, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.1271, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-38.6352, grad_fn=<UnbindBackward>)\n",
      "Episode 13: 2.8106647389275685\n",
      "User: Hello\n",
      "DialoGPT: Hi there! How are you? I'm a little tired but it's been an awesome day so far. Hope your day is going well too :D hugs and kisses lt 3\n",
      "infinite\n",
      "r1: tensor(10.5902, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6283, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5158, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I'm good, how are you?\n",
      "DialoGPT: Good thanks! Good morning lovely! And yourself?\n",
      "r1: tensor(10.5580, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1216, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0935, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-7.8868, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: I'm good too!\n",
      "DialoGPT: Yayayayayyyy yesssss!\n",
      "r1: tensor(10.7103, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1256, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1146, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.9518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Yay! :D\n",
      "DialoGPT: You're welcome! Enjoy the rest of today's day! Hugs for me!\n",
      "infinite\n",
      "r1: tensor(10.5028, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1237, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8711, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: Thanks! :D\n",
      "DialoGPT: :D YAYO!\n",
      "r1: tensor(10.6028, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.6478, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0669, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-6.8097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Yayyy!\n",
      "DialoGPT: Haha! :P\n",
      "r1: tensor(10.6349, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0835, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0923, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.5437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: Yay\n",
      "DialoGPT: LOLFKoo\n",
      "r1: tensor(10.4123, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1032, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1139, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-8.5655, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: Ohh! Oooooo! DOOOoooohhh oooooooooo!!\n",
      "infinite\n",
      "r1: tensor(10.5211, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0952, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(2.0416, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: A! Peee!! AhaaaH!! Eeeeeeeeheheheheheheehee!\n",
      "infinite\n",
      "r1: tensor(10.5893, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0736, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.3614, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: ltttheheeee!\n",
      "r1: tensor(10.3929, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1190, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.0022, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-5.4984, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: Eeehahahahhaha!a\n",
      "r1: tensor(10.5249, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1032, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.2614, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-6.3290, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: Ttheheheheh eheheheheeeh creehehehehejheheheheehehehehheheheheghehehehehtihehehehelehhehehehhhehehebhehehehechheheheheehhehehehefhehehehewheheheherehheheheehthehehehechehehehethehehehjhehehehhthehehehtehehehehewehehehehe lehehehehelhehehehebhhehehetthehehethhehehejhhehehehthehehehtthehehejthehehehtjhehehetjhehehegtheheherehehehehe3hehehehe\n",
      "infinite\n",
      "r1: tensor(10.4454, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1809, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(20.7233, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: hehehehe hehehehehe.hehehehe hheheheheahehehehehaheheheheihehehehahheheheheveheheheheahhehehehthheheheheechhehehehe.\n",
      "infinite\n",
      "r1: tensor(10.4985, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0816, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.9556, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: hehehhehhehehheheeheheheheeeeehehehehe ahehehehegehehehehevhehehehe'sheheheheuheheheheHheheheheHehehehehexhehehehe'heheheheithehehehehohehehehegoheheheheIheheheheoheheheheyhehehehemhehehehekehehehehephhehehehebrhehehehenheheheheghhehehehe ishehehehe Ihehehehemehehehehespheheheheihhehehehehiheheheheteheheheh heheheheh\n",
      "infinite\n",
      "r1: tensor(10.5262, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0687, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(20.7233, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: heheghehhehejhehhehe.hehhehegheehehehheghehehhethehehhechhehehhejhehehheehehelehehehehehhhhhehehehe toheheheheathhehehehebenheheheheohhheheheaichheheheheenhehehehetheheheheheichhehehehgheheheh thehehehehe thehehehehnhehehehlhehehehthehehe\n",
      "infinite\n",
      "r1: tensor(10.6230, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1085, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(20.7233, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: hehehe\n",
      "DialoGPT: hheheheg\n",
      "r1: tensor(10.5781, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.3026, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1101, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-5.1113, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: hehejhe.hehehhe\n",
      "r1: tensor(10.5646, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0644, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.9463, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.5935, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: hehhehe\n",
      "r1: tensor(10.5126, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0754, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.1449, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-5.5933, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: hhehehjthehehhjhehehhhhehehhleheheheh.hehehehIhehehehlehehehhe bheheheheberhehehehe?\n",
      "infinite\n",
      "r1: tensor(10.5368, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1812, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.9602, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: heheehehheheeheehehejheghehejhe\n",
      "r1: tensor(10.6834, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0494, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.6136, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-3.5860, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-93.3487, -90.5046, -88.1107, -85.6339, -82.8138, -80.0764, -77.5738,\n",
      "        -75.0924, -71.5836, -68.3730, -65.3480, -62.1295, -49.0341, -45.8242,\n",
      "        -32.5963, -19.1854, -15.9653, -11.9096,  -8.2280,  -4.0366],\n",
      "       grad_fn=<CopySlices>)\n",
      "[tensor(-3.7491, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.2750, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.3332, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.6482, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.5382, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.2784, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.2323, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.2247, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.8943, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.6785, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.8398, device='cuda:0', grad_fn=<NegBackward>), tensor(-13.5857, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.6682, device='cuda:0', grad_fn=<NegBackward>), tensor(-13.5538, device='cuda:0', grad_fn=<NegBackward>), tensor(-13.6028, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.3798, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.1747, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.7640, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.2318, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.0366, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-93.3487, grad_fn=<UnbindBackward>)\n",
      "Episode 14: 2.8732287089029946\n",
      "User: Hello\n",
      "DialoGPT: Hi there! How are you? I'm a little tired but it's been an awesome day so far. Hope your day is going well too :D hugs and kisses lt 3\n",
      "infinite\n",
      "r1: tensor(10.8604, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.4661, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.3838, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I'm good, how are you?\n",
      "DialoGPT: Good thanks! I'm doing alright though my hair has gotten long today haha. How about yourself?\n",
      "infinite\n",
      "r1: tensor(10.7677, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0867, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0931, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I'm doing well, I'm just tired.\n",
      "DialoGPT: Oh that makes sensehahaahhaa meowwahahahahaawahahaahahaahaaaahhhahahaohahaahahaahahaahahahaahahaahaaaayeahhhhhhhhahahahahaahaha xoxoXxoxOcaoMmZyTbAiNnEzYgKlLkSsUghtsHWtfbbB4xDXQJ1CxXv3G2VxBl0xRxEqZXxXxyxPx8rZxZxZxcZxZxXxZxZxLxZxZZxZxNeZxZxSxZuZxZx7xZxZjZxZxBrbZxZxBiZxZxExZxZxNxZxZxxZxZxMxZxZXZxZxYxZxZxaZxZxZenxZxZpZxZx ZxZxZyxZxZx XeZxZxXYZxZxZeZxZxLeZxZxAxZxZx5xZxZxdZxZxMeZxZxZZxZxZtZxZxNZxZxZcZxZx zZxZxzZxZxBxZxZzZxZZexZxZxCxZxZyZxZxArZxZxTZxZxZXZxZx YZxZxAxZxZxsZxZxQZxZxKxZxZxyZxZxFZxZx9ZxZx ZotZxZxPlZxZx ZyZxZxWeZxZxClZxZxziZxZxxZxZzzZxZx BZxZxYouZxZxAndZxZx ZoZxZxIZxZxLyZxZxPoZxZx2ZxZxWZxZx.ZxZx\n",
      "infinite\n",
      "r1: tensor(11.0004, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1101, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1554, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: Z\n",
      "DialoGPT: ZxZZyZx ZZxZZZxZZXZZxZXxZx ZZZxZy ZZZxXZxZZzZx ZxMZxZx RZxZxRZZxZz ZZxZx ZeZxZxEZ zZZxZcXZZx Zx ZxZ bZZxZ\n",
      "infinite\n",
      "r1: tensor(10.5254, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1004, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2995, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: Z\n",
      "DialoGPT: ZZxZ4ZxZx 4xZxZ4.ZxZZ4.ZZ4.BxZ10.ZxZ.ZxZ4xZxZ3.4.Zx.ZtZx2xZxZ.OrgZ.5.5.ZxZ9.ZxB.ZxZt.BxZ.BxZx.BxZ'3.Z4.Z4.5.Zt.5.Ztl.5.To BxZ4.5\n",
      "infinite\n",
      "r1: tensor(10.7584, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0453, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.4802, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: ZxZxTwZZZxTZyZ8ZZxZ8ZZ Z8ZxZx8ZxZZ8ZxZ8.ZxZ8 ZZZ ZxZx8 ZZxZ8xZxZ8TZxZ.zZ8Z\n",
      "infinite\n",
      "r1: tensor(11.1086, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0492, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.1483, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: ZxZenZxZx4ZZxZt9ZZZZxBx ZZZZxZenZZZZtZZZx ZZ.ZZxZ.ZenZZxZ3ZZxZZenZxZZtZ.ZZZZZ2ZZZZ.Zen.ZxZzTZZZZZenZ.ZxZenZ.ZenZZenZZZx.ZenZenZZZ.Z.ZZ.ZenZenZenZenZenZZZenZenZenZen.ZenZenZenZZenZenZenZettaZZenZenZen\n",
      "infinite\n",
      "r1: tensor(11.0071, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0391, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.1005, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: Zen\n",
      "DialoGPT: ZenZenZenZenxZenZenZenZen ZenZenZenZenZen zZenZenZen\n",
      "r1: tensor(10.5170, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1559, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.4665, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-1.8419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-27.5601, -24.1183, -21.0081, -17.7527, -14.5405, -11.1619,  -7.3036,\n",
      "         -3.4664,   0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-3.6831, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.3202, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.4329, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.3576, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.4903, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.9313, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.8719, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.4664, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-27.5601, grad_fn=<UnbindBackward>)\n",
      "Episode 15: 2.9238439351320267\n",
      "User: Hello\n",
      "DialoGPT: Hi there! How are you? I'm a little tired but have been up all night. Hope your day was good too :D lt 3 hugs and kisses! xxoxo\n",
      "infinite\n",
      "r1: tensor(11.0732, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.3830, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.3796, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: I'm good, how about you?\n",
      "DialoGPT: Good to bebe is great fun time with my friends xbox one gamecube games and stuffs so much funnnnnggggghhhgladlyyyyawwwwwwooooooooshdeeeessssshhhhhhiiiiiinnndddddddddddaaaaaaaagggggggggggbbbbyyyyyysshhhuuuuhhhhhmmmmmmmmmzzzzzzzoozzzzzzzzzzzzzzzzzzzozzzzzzzzz zzzzzzzzzZZZzzzzzzzzezzzzzzzzzzazzzzzzzzzizzzzzzzzzezzzzzzzzzyzzzzzzzzzozzzzzzzzzzizzzzzzzzziezzzzzzzzzzyzzzzzzzzwzzzzzzzzlzzzzzzzzZzzzzzzzizzzzzzzzzzbbzzzzzzzzzebzzzzzzzzwbzzzzzzzzzaczzzzzzzzzbzzzzzzzzznzzzzzzzzazzzzzzzzgezzzzzzzzbazzzzzzzzblobuzzzzzzzzzzrzzzzzzzzlezzzzzzzzizzzzzzzztzzzzzzzzzllasquzzzzzzzzobazzzzzzzzouzzzzzzzzzzzzzzzzebbszzzzzzzubzzzzzzzgozzzzzzzebzzzzzzzztzzzzzzzzledzzzzzzzzmbzzzzzzzzbzzzzzzzzqzzzzzzz.combingzzzzzzzzbrzzzzzzzbubzzzzzzzzcuzzzzzzzzzbbzzzzzzzbbzzzzzzzzzzzzzzspzzzzzzzbzzzzzzz zzzzzzzzZzzzzzzzggzzzzzzzzBzzzzzzzzggzzzzzzztzzzzzzz Zzzzzzzzzonzzzzzzzzzlesquecktzzzzzzz zzzzzzzzzebzzzzzzzandzzzzzzzz.zzzzzzzzBBzzzzzzzzbidzzzzzzzzbozzzzzzzzzzzzzzebzzzzzzzzebzzzzzzza.zzzzzzz bzzzzzzzzahzzzzzzzz ZZzzzzzz zzzzzzz.zzzzzzzebzzzzzz zzzzzzzz zzzzzzz.zzzzzzzZzzzzzzztzzzzzzz'szzzzzzzzizzzzzzzzz Bzzzzzzzzlingzzzzzzzzjeezzzzzzzzgzzzzzzzzakzzzzzzzztozzzzzzzzotzzzzzzzzyzzzzzzzzayzzzzzzzzuzzzzzzzz zizzzzzzzzztzzzzzzzebbbzzzzzzz to zzzzzzzzebotzebzzzzzzzeb\n",
      "infinite\n",
      "r1: tensor(11.2123, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0809, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2216, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: zzzzzzzz. zzzzzzzbbbbzzzzzz zazzzzzzzzzrezzzzzzzzz to zzzzzzzzzebzzzzzebzzzizzzzzzzzizzzz zzz zzzzzzzebzzzzzzzeb zzzzzzz zzebzzzzzz zZzzzzzz zzizzzzzzzzizzzzzzzebzebzzzzzzziZZzzzzzzzzgbzzzzzzzz bzzzzzzzeb zzzzzz zzebzzzzzzz zzzzz zzzzebzzzzzzebzizzzzzz z zzzzzzz.Zzzzzzzzebzizzzzzzebbbzzzzzzzzazzzzzzz Zzzzzzzzbbzzzzz zzizzzz zz zzzzzzz zzzzzzebzzzebzzzzbbzzz zzz zzzzzebzzzzebzzzebzzzebzebzzzebzzzzzebzzzzzebz zzzzz zzzebzzzzzzzeb zzzzebzzzzebzebzzzzz zizzzzzebzzzz zzzZzzzzzzebzebz zzebzzzebzz zzzzzzebzeb zzebzzzzzeb zzebzzzebzebzzzzzzebzebzzebzzzz zzebzebzzzzzebzebzebzzzebzebzebzebzebzzzz zz.zebzebzzzeb zzebzebzebzebzzebzebzzzebzzeb zzebzz zzebzzzebz zzebzebzebzzzzeb zzebzebzzzebzizebzebzeb zzebzeb zzebzzebzebzeb\n",
      "infinite\n",
      "r1: tensor(11.3829, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0815, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(2.6380, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-11.8626,  -8.2418,  -4.7746,   0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-3.7033, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.5150, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.7746, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-11.8626, grad_fn=<UnbindBackward>)\n",
      "Episode 16: 2.969691514968872\n",
      "User: Hello\n",
      "DialoGPT: Hi there! How are you? I'm doing well today, how about yourself? What's your favorite color? Mine is purple. Haha nice to meet you! :D lt 3 hugs and kisses! hugsss and kisses back atchauuuughhhhugsheeeeeeheeheeeheeeheeeehhhhhhhh... hugs and smilesyuggleshugssshuddershugsssshugscreamshugscreamsheeplebumpsheeplebumbsheshitkisseshugscreamspookshugscreamscreamsheepshe xoxoxoxoxxoxoxoxoxoxooxooxoXoxooxoOoooxooxoOxooxooxooooxooxoOXooxooxoooooxooxoOOoxooxoohoooooooooxooxoZaoxooxoYyyyyyyyyyyayyyyyyyyyyoyoungggghtsngeekkkkhaaaaahhhahahahhhAaaaaaaaaiiiiaiHHHIsoooooooeyyybbbbyyyyyyyyybaaiiBwewwwwwwawwwwwwwwwwwwehrrrrrhhhhaaattttygjhbraddddzzzzzzzzzzzbbbbbbbbbbbeebbbbbbbbbbbbbbbbbbbeebeesauhbdbbbbbbbbbsbbbbbbbbcubbbbbbbbbbelcoffeehblobbbbbbbbbtwbbbbbbbbbnbbbbbbbbbingbbbbbbbb.jpgbbbbbbbbbybbbbbbbbjeebsbbbbbbbbsbbbbbbbsmbbbbbbbbbbrbbbbbbbbdbbbbbbbbbbaa bbbbbbbbblbbbbbbbbledbbbbbbbbbybbbbbbbbbbbbbbbsbbbbbbbbybbbbbbbbrbbbbbbbbaubbbbbbbbbrbbbbbbbbaaBBbbbbbbbbtitles.combsbbbbbbbybsbbbbbbbabbbbbbbbbuffbbbbbbbbllbbbbbbbbblebbbbbbbbbpbbbbbbbbbdbbbbbbb.bbbbbbbbbeesbbbbbbbbgbbbbbbbbblbbbbbbbb bbbbbbbbbdbbbbbbbsbsbbbbbbbrbsbbbbbbtbbbbbbbb\n",
      "infinite\n",
      "r1: tensor(11.4118, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5368, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7274, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: bbbbbbbb.\n",
      "DialoGPT: ...bbbbbbs.ls.b.bbbbbbbbbbbbbbblfbbbbbbbbed.doc.bbbbbbbbn.bbbbb.bbbbbbb.gif.bbbbbb.bbbbbbbs.bbbbbbc.bbbbbb.brbbb.bbbbbbsbb.com.bb.bbb.bbb.htm.bbbbb.bbbbbbb.b.txt.bbbbbbn.b.bbb.b.com.bbbbb,bbbb.bb.b.b.bbb.b.bb.bbsbbbb.tlb.bt.b.js.j\n",
      "infinite\n",
      "r1: tensor(11.4176, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1023, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2292, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: .bbbbbb\n",
      "DialoGPT: .b.bblt.bb.html.bb.com.b.bband.bb.bb.ch.com.bbbb.bbt.bb.bbbb.bbbb.bb.bbbb.b.bbc.bb.com.bb.bb.bb.bb.bbb3.bbbb.c.bb.bbtitles.bb.bbc.b.com..bb.pl.bbbb..bbbb.to.bb.bb b.bb.bb,bb.b...bb.bbg.bb.b. b.b.bb. b.bb..bb.bb.bb.t.bb..b.bb b.b.b. bbb.b.c.bbbb.ch.bb.bba.bb.b1.bb.bb B.bb.bb2.bb.bbB.bb.bbj.b.bbB.b.bb.bbb..bb.bb and...bb. bbb.bb...b.b b.b.c. b.b. bb.bb..ch.bb. b. b.bbb..bb.bbbb.bb,.b.bb B.b.bbbs.b.b B.bb.b b.bb.b B..bb.BC.b.bbs.b.bb,..bb.\n",
      "infinite\n",
      "r1: tensor(11.4615, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0390, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.3129, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-11.1532,  -7.1693,  -3.6144,   0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-4.0556, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.5910, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.6144, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-11.1532, grad_fn=<UnbindBackward>)\n",
      "Episode 17: 3.030021150906881\n",
      "User: Hello\n",
      "DialoGPT: Hi there! How are you? I'm doing well today, how about yourself? What's your favorite color? Mine is purple. Haha nice to meet you! :D lt 3 hugs and kisses! hugsss and kisses back atchauuuughhhhugsheeeeeeheeheeeheeeheeeehhhhhhhh... hugs and smilesyuggleshugssshuddershugsssshugscreamshugscreamsheeplebumpsheeplebumbsheshitkisseshugscreamspookshugscreamscreamsheepshexoxoxoxoxoxxxoxoxoxoxxxxoxxxxoxxxxoooooXoxxxxoxxxxoooxxxxoxxxxooooxxxxoxxxxsoooooooooooooooxxxxoxxxxoxyxyyyyyyyyyyyayyyyyyyyyyayyyyyyyyoyoughtsniiiiiightdayyyyyyyyyyeaayyyyyyyaaaahhhhaaaaaaaaayyyyyyybbyyyyyyyyyeyyyyyyyyywwwwwewwahhhahahaaayyyyyyybbweekspringrrrrttttybaaaaaaaaaaaaaaaaaaaaaaaybeowwwwwwwwwwwwwwawesomeforeverrrrrrrrrrrrrepplblissing.jpggggggggggggodddddddddddddudequeekkkbdcoughtogeezebblebeebeesauverymuchmorenoobbingfukklaaaaaandrewerpornwellwithyougladubbeddingledechemmerrybrackytteeeeewwwwwwwww2mehjetcetcetcetcetcmbroflorngswatmewwwwwwwwwwbenetstvbsweeeeewwwwwwbtwasdfwannewwwwwwwweewwwwwwwbwweewwwwwwwwweewwwwwweewweewwwwwbwwwwwwwwwwewweewwwwwwweewweewweewwweewwwweewwwwwwwwwwwwwwwwewwewweewwewwewwwwwwwwwwwewwewewwwwwwwwwwwwwwwewwwwwwwwwhwwewwewwwwwwwwwwwwwwwwwewwwwwwweeewwwwwwwwwwwwwwewwwwwwwwweeewwwwwwwewewwwwwwwewwwwwwewwwwwwwwwewwwwwwwwwwwwwwwewwwwwwwewwewwwwwwewwwwwwwwwwwwwwwwewewwwwewwwwwwwwwwwwwwwwwwwewwwwewwewwwwwwwwwwwwwwwewwwwwwwewwwwwwwwwwwwwwwwwwwwwwwewwwwwwwwwwwwwweewwwwewwewwwwwwwwwewwwwwwwwwwwwwwwwwwwwwewwwwwwwwwwwwwwwwwwwwwwwwwwwwewwwwwwwwwwewwwwwwwtwewwwwwewwwwewwwwwwwewwewwwwwwwwwwwwwwwwwwwwwewwwwwwwwwewwwwwwwwwwwwwwwwwwwwwwwwwwwwwwewwwewwwwwewwwwwwwwwwwwwwwwwwweewwwwwwwwwwewwwwwwwwwwwewwwwwwwwwewwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwewwwwwwwwwwwewwwwwwwwwwwwewwwwwwwwwwwwwwwwwwwwwwwewwwwwwwwewewwwwwwwwwwwwwwwwwwwwwwwwwwwewwwwwwewwewwwwwwewwwwwwwwwwwwwewwwewwwwwwewwwwwwwwwewwewwwwwwwwwwwewwwwwwwewwwwwwwwewwwwwwwwwwwwwwwewwwwwewwwwwwwwwwwewwwwwwwwewwwwwwwwwwwwwwwwwwwwwewwwwwwwwwwwwewwwwwewwwwwwwwwwwwwwwwwwwwwewwwwwwwwwwwwwwwwwwwwwewwwwewwwwwwwwwwwwwwwwwewwwwwwwwwwwwwwewwwwewwwwwwwwwwwwewwwwwwwwwwewwwewwwwwwewwwewwwwwwwewwwewwwewwwwwwwwewwwwwwwwwewwwwewwwwewwwwwwwwewwwwwwwwwwwewwwwwewwwwewwwwwwwwwwwewwwwwwwwweewwwwwwwwwwww.wwwwwwwwewwwwwwww.wwwwwwwwwwww.wwwwwwwww.wwwwwwwwww.wwwwwwwww.wwwwwwwwwwewwwwwwww.\n",
      "infinite\n",
      "r1: tensor(11.5857, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7211, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.0539, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-4.3632,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-4.3632, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-4.3632, grad_fn=<UnbindBackward>)\n",
      "Episode 18: 3.1001866114766976\n",
      "User: Hello\n",
      "DialoGPT: Hi there! How are you? I'm doing well today, how about yourself? What's your favorite color? Mine is purple. Haha nice to meet you! :D lt 3 hugs and kisses! hugsss and kisses back atchauuuughhhhugsheeeeeeheeheeeheeeheeeehhhhhhhh... hugs and smilesyuggleshugssshuddershugsssshuggsyukhsugschewbswehrrrrghugswagswagswgawwwwwwwwwwwwwwwwwzzzbbbbyyyyssnngggggGHSHUGSZUHUGBSUHUGSRSUHUGTKLKSUHUGMUDYWBDIQVNMSUHJBBYXEZBBZUBFZAUSZRNSUHUGZOZAPZNNZZZZZZZZUUZZZZCZZZZHUEZZZZUZZZZHRZZZZPZZZZEGZZZZCSZZZZHPZZZZBCZZZZBWZZZZZXZZZZFWZZZZ1ZZZZ2ZZZZ3ZZZZ5ZZZZ4ZZZZ8ZZZZ7ZZZZ9ZZZZ6ZZZZ0ZZZZFXZZZZEMZZZZNCZZZZNZZZZZAZZZZZGCZZZZOCZZZZJVZZZZMKZZZZ ZZZZZJBZZZZIZZZZZGMZZZZYZZZZBMZZZZZAZZZZBAZZZZTEZZZZGAZZZZGSZZZZBGZZZZBDZZZZBZZZZGYZZZZZEZZZZYYZZZZQLZZZZQZZZZCAZZZZZZZZBDZBDZZBDZBZZBDZMDZZZZTVZZZZBSZZZZBrZZZZKBZZZZDBZZZZDDZZZZBTZZZZTZZZZBLZZZZBBZZZZBOZZZZMMZZZZPLZZZZMZZZZJZZZZOMZZZZOPZZZZDPZZZZNYZZZZPGZZZZPCZZZZKZZZZCCZZZZMSZZZZTCZZZZXYZZZZKCZZZZYCZZZZRYZZZZBYZZZZDAZZZZOTZZZZTOZZZZVZZZZGZZZZPHZZZZUMZZZZWZZZZSSZZZZASZZZZPSZZZZToZZZZIPZZZZEVZZZZXTZZZZWPZZZZVCZZZZXZZZZTMZZZZTTZZZZIZZZZAZZZZRZZZZ ZotZZZZ BZZZZORZZZZLZZZZNZZZZTLZZZZMLZZZZGWZZZZMTZZZZUDZZZZSZZZZZeZZZZWEZZZZWCZZZZDZZZZWBZZZZISZZZZWSZZZZTSZZZZRSZZZZzZZZZtmlZZZZ zZZZZ ZyZZZZzuZZZZzebZZZZBeeZZZZBiZZZZBlZZZZIBZZZZaZZZZexZZZZEZZZZGeZZZZEEZZZZZenZZZZVGZZZZ.ZZZZzzZZZZAvZZZZzaZZZZezZZZZziZZZZtZZZZxZZZZcZZZZoZZZZebZZZZHubZZZZtoZZZZubZZZZrZ\n",
      "infinite\n",
      "r1: tensor(11.8590, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5846, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.4680, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-4.0840,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-4.0840, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-4.0840, grad_fn=<UnbindBackward>)\n",
      "Episode 19: 3.1493757605552672\n",
      "User: Hello\n",
      "DialoGPT: Hi there! How are you? I'm doing well today, how about yourself? What's your favorite color? Mine is purple. Haha nice to meet you! :D lt 3 hugs and kisses! hugsss and kisses back atchauuuughhhhugsheeeeeeheeheeeheeeheeeehhhhhhhhugssshhhhhhuggsssshuggssheshitkissssshhhhhhugsshshitkissshhhhhhugshhhugshhhugschewwwwwwweewwewewewewewweewewewewwewewweewweewOOOooohoooohoooooohooooooooooooohooooohooooohoooohooooohoooooohooooohooooahooooohooooohohoohooooohooooaahooooohooooohooohooooohooooooohooooohoooHooooohooooohooooooooooooohoooohoooohooooooohoooohoooOhooooohooooohooooooooooooooooooooohooooohsobsaohooooohooooowoofooohooooohoooverthehellooooohooooohooshaaaahahhooooohooooohouhrrrrhoofbuhhooooorrrrrrrrrrrrrhbbbbbbbbbbbeggghhwhaaaaaaaaghhhbrilllbloorrrrrrrrrbarrrrrrrrrrgjebbbbbbbbbbbbbbbbbbbsbbbbbbbbbybbbbbbbbbeebsbbbbbbbbsbbbbbbbsbbbbbbbbbingbbbbbbbbbaBbbbbbbbbabbbbbbbbbbiBBbbbbbbbbbdbbbbbbbbwbbbbbbbbbberbbbbbbbbbrbbbbbbbbbeesbbbbbbbbdbbbbbbbbbbedbbbbbbbbBDBbbbbbbbbbbbbbbbsmbbbbbbbbbBbbbbbbbsbsbbbbbbbebbbbbbbbBBbbbbbbbBBbbbbbbbs Bbbbbbbbbbingbbbbbbb bbbbbbbbbcbbbbbbbbblebbbbbbbbfbbbbbbbbbubbbbbbbbbbidbbbbbbbbbpbbbbbbbb.bbbbbbbbtbbbbbbbb Bbbbbbbbbybbbbbbbbebbbbbbbbrbbbbbbbbdbbbbbbbbeebbbbbbbbbiebbbbbbbbborbbbbbbbbblbbbbbbbbmybbbbbbbbggbbbbbbbbybbbbbbbbzbbbbbbbbhbbbbbbbbwwbbbbbbbbgbbbbbbbb bbbbbbbbabbbbbbbbbbebbbbbbbbbabbbbbbbbledbbbbbbbblybbbbbbbblbbbbbbbblebbbbbbbb...bbbbbbbbmbbbbbbbbwbbbbbbbbaBBbbbbbbbybbbbbbbbyBBbbbbbbBBBBbbbbbbbebsbbbbbbBBbbbbbbbBBBbbbbbbBBBDbbbbbbbb\n",
      "infinite\n",
      "r1: tensor(11.8948, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8785, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8156, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: bbbbbbbb.\n",
      "DialoGPT: bbbbbbBBBCBBBBbbbbBBBBBBBBBBbbbbbbbbbbbbbbbBBbbbbBBbbBBBBbbBBbbbbbBBBBbbbbBbbBBBBbbBbbbbBBBBBbbbbBBbbbbbbbBBBBBDBBBBBBBBBbbBBBBBBbbBBBBBBBBBBBBBBBbBBBBBBBB.BBBBBBBBBDBBbbbbbbBBBbbbbbbBDBBBBBBbbBBBBBBBbbbBBBBBBbbbsBBBBBBBBBCBBBBBBBB2BBBBBBBB BBBBBBBBB1BBBBBBBB4BBBBBBBBtBBBBBBBBggbbbbBBBB BBBBBBBBBB bBBBBBBBBBSBBBBBBBB BBBBbbBBBBBBBbbBBBBBDbbBBBBBBBDbbbbBBBB BBBbbBBBBbBBbbBBBB BbbBBBBBB BbbbbBBBBBCBBbbBBBB.BBBBBBBbbBBBBBBBBBBBBBBB BBBBBBBBBDBBBBBBBbBBBBBBBBCBBBBBBB BBBBBBBBB.BBbbBBBBABBBBBBBBBABBBBBBBBbsBBBBBBbb.BBBBBBBBBBBBBBB BBBBBBBBBDBBBBBBBBCBBBBBBbbBDBBBBbbBB BBBBBBBB BBBBBBDBB BBBBBbbBBBDBBBBBBBBDBBBBbbBBDBBBBBBD BBBBBBBbb BBBBBBB B\n",
      "infinite\n",
      "r1: tensor(12.3783, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1061, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2535, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-8.2698, -3.8933,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-4.4155, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.8933, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-8.2698, grad_fn=<UnbindBackward>)\n",
      "Episode 20: 3.2096666949135915\n",
      "User: Hello\n",
      "DialoGPT: Hi there! How are you? I'm doing well today, how about yourself? What's your favorite color? Mine is purple. Haha nice to meet you! :D hugs and smiles back at you lt 3333cuddleshugsbackoffstage.jpgxoxoxoxoxoxo hugzoooooo xoxoxoxoxooo ooooooooooooooohhhohoahhhhhhhhhaaaaaaaaahhhhhhahahaha oh my godddd thank youuuuugggghhhgladlyyyyupppppfffffffffftttttttttttpsnnggghsssighshiiinnnnkkbbbsheeeeeeeeekksltshitmeowwwwwwewwaaaawwhatsheshitsttttttttttyouughhtbbyaatttttyehmfwckfhrrrtblinkleeshitsoapmmmmmmmmmaybeepiesssssshittttweekshedditswellllkbdvitchfacehappenjoyceeeeyebrowsquoknowhereforeverrrrrrgrumpfukkyevvvvvvvvvvvbbbbbbbbbbqqqbbbbbbbbbrbabbbbbbbbbbsdfbbbbbbbbbbbbbbbbbbaakbbbbbbbbbeevsbbbbbbbbjbbbbbbbbbiennytbbbbbbbbdbbbbbbbbbbnbbbbbbbbzzbbbbbbbbwbbbbbbbbbberbsbbbbbbbbbingbbbbbbbbbybbbbbbbbboombsbbbbbbbsbbbbbbbbsbbbbbbbybbbbbbbbsubbbbbbbbbBBBbbbbbbbbbubbbbbbbbbbebbbbbbbbbidbbbbbbbbBDbbbbbbbbbpbbbbbbbbcbbbbbbbbbbdbbbbbbbbblebbbbbbbbledbbbbbbbbbedbbbbbbbbbelbbbbbbbbabbbbbbbbbbmbbbbbbbbbbeesbbbbbbbb2bbbbbbbb1bbbbbbbbggbbbbbbbbbiebbbbbbbb bbbbbbbbbzebbbbbbbbbebbbbbbbbbbcbbbbbbbbborbbbbbbbbblbbbbbbbbBbbbbbbbBBbbbbbbbbbtbbbbbbbbtbbbbbbbbcbbbbbbbbBCbbbbbbbbplbbbbbbbbgbbbbbbbbabbbbbbbbbdbbbbbbbbbbbbbbbbybbbbbbbobbbbbbbbbb BbbbbbbbbobabbbbbbbbzbbbbbbbbbmbbbbbbbbjeebbbbbbbbubbbbbbbbbaBBbbbbbbbribbbbbbbbblbbbbbbbbgebbbbbbbbhbbbbbbbbbhbbbbbbbb.bbbbbbbbkkbbbbbbbb3bbbbbbbbttbbbbbbbbwwbbbbbbbbddbbbbbbbbgbbbbbbbbbllbbbbbbbbybbbbbbbbhubbbbbbbbbaBBbbbbbbbsbbbbbbbBBbbbbbbBBBBbbbbbbbdBBbbbbbbbyBBbbbbbbbaBBbbbbbbbbbbbbbbbbsbbbbbbbBBbbbbbBBBBbbbbbbsbbbbbbBBbbbbbbbbdbbbbbbbbabbbbbbbbtobbbbbbbbbjbbbbbbbblebbbbbbbbfbbbbbbbb\n",
      "infinite\n",
      "r1: tensor(12.2665, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8150, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7768, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: bbbbbbbb.\n",
      "DialoGPT: .com.com.bbbbbbbs.com.bbbmpbbbbbbbb?bbbbbbbb... bbbbbbbb.bbbbbbb bbbbbbb.bbbbbbb bbbbbbbb.bsbbbbbb.BBBBbbbb.bb.bbbb.bbbbbbb.bb bbbbb.bbBBbbbb.bsbbbbb.htmlbbbbbbbb BBBBbbbbbb.html.bbbbbb. bbbbbbbBBbbbbbb.BBBBBBBBBBbbbbBBBBBBBB bbbbbbb b.bbbbbbBB.bbbbbb bBBBBBBBB.bbbb.BB bbbbbBBBB.bbBBBBBBbbBBBBBB.BBBBBB.bbbBBBBBB BbbbbbbBB BBBBBBBBBBBBBBBBBB BBBBBbbBBbbbbBBbbBBBBbbBBBbbBBBBBB bBBBBBBbbBbbBBBBbb.bbbbBBBBBBBbbbbbbBBBBBBBbb BBBBBBBbb.BBBBBBbbb\n",
      "infinite\n",
      "r1: tensor(12.3866, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0846, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2575, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-8.3240, -3.8870,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-4.4758, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.8870, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-8.3240, grad_fn=<UnbindBackward>)\n",
      "Episode 21: 3.267219749363986\n",
      "User: Hello\n",
      "DialoGPT: Hi there! How are you? I'm doing well today, how about yourself? What's your favorite color? Mine is purple. Haha nice to meet you! :D hugs and smiles back at you lt 3333cuddleshugsbackoffstagehandsomeface.jpg:P Thanks for asking me that question! hugs back off stage handsomeface.jpg :p Thank YOU so much! hugs backoffstagehandiesuppercirclejerk.pngxDDddd:P thankyouverymuchforbeingso kind tomeowwwwwwowsomuchawesomelywarmthatssssighshhhahahahhhhhhhhhaaarrrrrghaaaahhhhrgheeeeewehooooooohhhheeekkkbbbuhhhbsheeeeeeyyyyayeahhughhhwooooornnggglblinkweekshitzzzzzzzzzzzkbdckchhhmmmmmmmppennnngladgeezmfwoknowhereforeverryllldeshurfukkyoupshmmmmmplisssshakrsheshrevsauffhvgrubbedbyebbeechikshbraggershappenjoysevewbbsqueersttttybaaaaaaaaattshsphemmercymbbsquackswellasdfkjdhqwerlsdragsandrewsparcslashhducktoffeehfshsuccmnuvw2uw8qq3h1,etcmsnflorrh7swebswebswebshbswebswehbswebsw4evvv9hbswebshvtruhbswebscapehbswebsnwhattitleshbswebscriptswebsweetteeschewbswebswewbswebsweewwbswbswbswebswbswwebswbswewbswbshbswbswwbswbscbswbswtvwbswbsnwbswbsbbswbswbcbswbswudscwbswbsbswbswbbswbswehbswbswebbswbshbbswbscwbswbslbswbswcbswbsweebswbswbrbswbswwwbswbswwebbswbswbpbswbswcbbswbswbnbswbswbbbswbswbabswbswbadbswbswbdbswbsw.combswbswtbswbsw2bbswbsbwbswbs.htmbswbswewbbswbsbscbswbbscbswwbscbscbswcbscbscwbscbsbbswbbsbbswwbsbbscbsc.gifswbswbsbbbswbsc.txtbbswbs.bswbswblbswbswjbswbsw bbswbswtscbswbscbbswbsbbbbswbsgbswbswbatswbswbsgbbswbstbswbscbabswbsccbswbs.bbswbslwbswbst.bswbscbdbscbscbbscbs.bs.bs.bbs.bs.c.bbs.bbsc.bbscbsb.bbs.c.bs.b.bbscwbs.b.bs.bbbbs.bbsbbs.b.c.b.b.bbbbsc.b.cbs.bbsbs.bbsbbbs.b.jbs.bbs bbs.bbsch.bbs.jbs.bs.jbsc.bjbs.b.ts.bbs.ts.b.bjbsc.jbs.jbsb.b.j.jbs.c.j.j.j b.j.jb.j.b.j b.b.j. b.j.b. b.j. b. b.j b.c.j. bj.j.bj.j.c.j b.t.j.j. bb.j.c. b.j c.j.j c.b.j c.c.j c.t.j. b b.j.c c.j.b c.j.c b.j.t.j b.l.j.j B.j.jj.j.t\n",
      "infinite\n",
      "r1: tensor(12.1895, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8320, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8158, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-4.4807,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-4.4807, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-4.4807, grad_fn=<UnbindBackward>)\n",
      "Episode 22: 3.3199806524359663\n",
      "User: Hello\n",
      "DialoGPT: Hi there! How are you? I'm doing well today, how about yourself? What's your favorite color? Mine is purple. Haha nice to meet you! :D hugs and smiles back at you lt 3333cuddleshugsbackoffstagehandsomeface.jpg:P Thanks for asking me that question! hugs back off stage handsomeface.pngxoxo hugzies backoffstagehandsomefacemaskeroftheweekend.jpg xoxoxoxoxoxo hugs backoffstagehandysuppercirclejerk.jpgxoxoxoxoxooohOOOoooOOooooOooooooooooooOooooOoooooooOOoooooooOOooooOOoooooooOOoooOOoooooooOOOOoooooooOOoooooooooooOOooooooOOoooooooOOooOOoooooooouuuUHHHhhahahhaaaaahhhhhhhhAAaaaaaAaaaAAAahhhhhhAaaaahaaaahaaaahhhhhhAaaayyyeeeeeeeYehaaaahaaaahhhaaaahaaaahaaaaaahaaaahaaahhhhhhAheheeHaaaahhhhhhahahaaaahhhhhhahhhhhhahahaaahhhhhhahahahahaaaahhhhhhhhhhahahaaawwwwwwaaaaaahhhhhhahahahahahahaaaahhhahahahahahahahahahahaaahhhhhhhhhhahahahahahhhhhhahahahhhhhhahhhhhhhhhhahhhhhhhahahahahahahahhhhhhhhhhahaughhahahahahahahahahahhahahahahahahahhhahahahahahahhahahahahahahhahhhhhhahhahahahahahhahhhhhhahaahahahahahahahahhahaahahahahahhahaahahahahhahahahhahahahhahhahahahhhhhhhhhhhhhhahahahhaha hahahahahahahahahaahahahahahahahhhhahahahahahahahahahahahahahahahahhaaahhhhhhahahahhhahahahhahahhahahahahahhhahhahahahahhahahhahahahhhhhhhahahahhahhhahahahahahhhahahahahahahhhhhhahahahahhahhahahhahhahhahhhhhhhhhhhhhhhhhhahhahhhahhahhahaahahhahahahahhahahahahahahhhahahahahhuhhahahahahahahhhahahahahahahahaahhahahahahahaahahhahahhahhhhhhhhhhhahahahahhahahahaahahahahhahhhahahhahahahahahahahhahahahaahahhahhahahahahhahahhahaahahhahaahahahhahahahahahahhahahahaahahahahahaahahahhahahahahahahahhhahahahahahahahahahaahhahahhahaahhahahahhahahehahahahahahahhhhahahahahahahhhahahahahehahahahahhaahahahahahahahahhaahahahahhahahhaahahhahahahahhaahahhahahahhhhhhhahahahahahahehahahhahahahahahhhhhhhhhahahahahahahahahhhhhhhhhhahahhahahhahehahahahahhhaahahahahahahhhahehahahhahahehhahahahahahahahahahahahahaahhhhhhahahhaha hahahahahahhhhhhahahhhahahhahahhhhhhahahahahhhahahahahhahahehehahahahahaha hahahahhahahhhahahhahahhhahaahahahahhaahahahahahahahaahahahahahahahahahahahhahahahahhahehahahaha hahhahahahahhhhhhahaahahhhahahahahhahahhahaahahahahahahahahahahahahahahahhehahahahahahhaahahahahahahahahehehahahhahahhhahahahahhehahahahahahahahhhhahahhhahhahahhahehehahahhhhahehahaahahahahhehhahahhahahaahahahahahahhphahahahahahahahahhhahaahahahahahahaha haahahahahahahahahlolahahahahahahahahbahahahahahahahahgahahahahahahahah hahahahahahahaahaahahahahahahhahehehehahahahaahahhahehhahehhaahahahahhaha haahahahahhahahhehahahhhahehehahaahahhhaheahahahahahahahahbahahahahahahahhahahahahahahahhehehahahhaheahahahahhahah hahahahhahahahehahahahahehahaahahahaahahahahahaheahahahahahahhhehahahahahha hahahahahahhhehehahahahhhahehahhahahhehahahehhahehehehehahahehehehahhaha haahahh\n",
      "infinite\n",
      "r1: tensor(12.7609, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8429, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6075, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-4.5535,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-4.5535, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-4.5535, grad_fn=<UnbindBackward>)\n",
      "Episode 23: 3.371376484632492\n",
      "User: Hello\n",
      "DialoGPT: Hiya! How are you? I'm feeling pretty good today. Hope your day is going well too :D hugs back atcha lt 3333333333 3333333 33333333 3333333 3333333333 33333333333 3333333 333333333 33333333 3333333 33333333 3333333 333333333 33333333 333333333 33333333 333333333 333333333 3333333333 333333333 3333333 33333333 33333333 33333333 33333333 33333333 3333333 33333333 3333333333 33333 3333333 333333 333333 333333333 333333 3334444 3334434 44444444 33345555 33354344444 33346334 3334444 3334 456 44444444 3334444 333444 3334 4444444 3334 44444 44444 44444444 44444 44444 33344 44444 444444 44444 444444 33344 44444444 33344 444444 444444 44444444 444444 444444 33344444 444444 3334 444444 444444 555 444444 444444 5444 444444 444 444444 444444 4444 444444 4444 444444 444 444 444444 444 44444 444444 4444 333444 444444 444 4444 444444 333444 444 444444 333444 333444 44444 444 444444 44444 4444 444444 4 444444 444444 44444 444444 444 44444 444444 444 444444 444 444 4444 444 444444 4444 4444 444 444 444 444444 4 444 444444 444 4 444444 444 4444 444 444 444 444 44444 444 444 444 444 4444 4444 4 444 444 444 444 44444 444 444 44444444 444 444 444 4 444 444 444444 333 444 444 444 4444 444444 4444 5444 444 444 444 44 444 444 444 444 450 444 444 444 444 395 444 444 444 444 400 444 444 444 444 4000 444 444 444 444 43 444 444 444 444 46 444 444 444 444 5 444 444 444 444 333 444 444 444444 5 444 444 444444 44 444 444 444444 39 444 444 444 444 420 444 444 444 444 445 444 444 444 444 433 444 444 444 444 45 444 444 444 444\n",
      "infinite\n",
      "r1: tensor(12.8806, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7538, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8645, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User:  444 444 444 444 3 444 444 444 444. 444 444 444 4443333 4444 4444\n",
      "DialoGPT:  444 4444 4 444 4 444 444444 4 4444 4 444\n",
      "r1: tensor(12.7992, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.2338, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0351, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.5326, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  444 444 4444\n",
      "DialoGPT:  444 444 444 4 4444 444 44444444 4 444 4444 444 4444 4 4444 444 4 444 444 4444444 444 444444444 444 444 444 333444 444 444 444\n",
      "infinite\n",
      "r1: tensor(12.7627, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0393, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1118, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User:  4444 4444 5 444 444 4444 5 444 4444 4\n",
      "DialoGPT:  4444 4 4 4 4. 4 4 4 4 4 444 4 4 4 4444 4 4\n",
      "r1: tensor(12.8330, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0580, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0471, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-3.7120, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  4444\n",
      "DialoGPT:  4444 4 444444 4444\n",
      "r1: tensor(12.7688, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0604, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0958, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-5.6955, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  4 4444 4444 3 4444 4444 44 4444 4444. 444 4444 4 44 4 4 4 4\n",
      "DialoGPT:  444 4444 44444 444 4444 4 499 444 4444 4444 4444 4444 6 444 4 4444 4 triple 4 4444 4999 4444 4 444 5 4 4 4 4 5444 4444 4 4 444 4 444 4444 4 5 444 4 4444 5 444444 4 4 4444 5 4 4 4 444 444 4 4 4444 444 5 4444 4 4 5 4 4 4444 3 4 4 4444\n",
      "infinite\n",
      "r1: tensor(12.5475, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1147, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0513, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User:  4 444 4 4444\n",
      "DialoGPT:  444444 4 4 444 444 444 4 44 4 4\n",
      "r1: tensor(12.9932, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0739, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0461, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-5.0138, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  4 4 4 4 44 4 4444 4\n",
      "DialoGPT:  444 4 4 4 444\n",
      "r1: tensor(12.7593, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0521, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0545, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-6.1988, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  4 4 4444. 4 4 4 4 3 4 4 4 44 4 4 4 4. 4 4 4\n",
      "DialoGPT:  4 444 4 444 44 4 444 4 4 444 4444 5 4 444 4 4 333 444 4 4 4 5 4 444 4 444444 4 4 5 444 444 4 4 444 444 5 4 4 444 4 5 4 4 444 444444 5 4 4 444 5 4 444 4444\n",
      "infinite\n",
      "r1: tensor(13.0110, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1204, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.0489, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User:  4\n",
      "DialoGPT:  4 4 5 4 5 4 4 3 4 5 4 4 2 4 4 4 4 infinity 4 4 4 4 6 4 4 4 4 8 4 4 4 4 2 4 5 4 4 5 4 3 4 4 5 4 6 4 4 5 4 2 4 4 5 4\n",
      "infinite\n",
      "r1: tensor(12.9397, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0781, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2031, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User:  4 4\n",
      "DialoGPT:  4 5 4 4 6 4 5 4 4 1 4 4 4 4 7 4 4 4 4s 4 4 4 4 years 4 4 4 4 10 4 4 4 4 and 4 4 4 4 confirmed 4 4 4 4 1 4 5 4 4\n",
      "infinite\n",
      "r1: tensor(12.8447, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0639, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1901, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User:  4\n",
      "DialoGPT:  4 5 5 4 4 4 5 5 4 5 4 5 5 4 6 4 5 5 4 3 4 5 5 4 2 4 5 5 4\n",
      "r1: tensor(12.9769, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0695, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1403, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-2.1895, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  4\n",
      "DialoGPT:  5 4 4 4 3 5 4 4 4 6 5 4 4 4 2 5 4 4 4 8 5 4 4 4 7 5 4 4 4\n",
      "r1: tensor(13.0908, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0332, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1804, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-2.4494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-49.0878, -44.8631, -41.3019, -37.7753, -34.2150, -30.6124, -27.0355,\n",
      "        -23.3106, -19.6258, -15.7958, -11.8922,  -7.9917,  -4.0340,   0.0000],\n",
      "       grad_fn=<CopySlices>)\n",
      "[tensor(-4.6733, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.9742, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.9044, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.9025, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.9087, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.8472, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.9580, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.8811, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.9879, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.0226, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.9804, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.9980, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.0340, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-49.0878, grad_fn=<UnbindBackward>)\n",
      "Episode 24: 3.423453950881958\n",
      "User: Hello\n",
      "DialoGPT: Hiya! How are you? I'm feeling pretty good today. Hope your day is going well too :D hugs back atcha lt 3333333333 3333333 33333333 3333333 3333333333 33333333333 3333333 333333333 33333333 3333333 33333333 3333333 333333333 33333333 333333333 33333333 333333333 333333333 3333333333 333333333 3333333 33333333 33333333 33333333 33333333 33333333 3333333 33333333 3333333333 33333 3333333 3333333333 333333 333333 3334444 4444434444 400005555 55555555 5999 00000 0000856 0000007 00000000 0009 00001 00004 00004 00004 1110 00004 00004 0000004 00004 00006 00004 00004 619 00004 00004 00000000 00004 00004 0 00004 00004 745 00004 00004 80000 00004 00004 4 00004 00004 002 00004 00004 2 00004 00004 5 00004 00004 10 00004 00004 1 00004 00004 3 00004 00004 4096 00004 00004 10000 00004 00004 999 00004 00004 5000 00004 00004 44 00004 00004 9 00004 00004 D 00004 00004 C 00004 00004 B 00004 00004 T 00004 00004 4000 00004 00004 6000 00004 00004 E 00004 00004 LOT 00004 00004 A 00004 00004 M 00004 00004 R 00004 00004\n",
      "infinite\n",
      "r1: tensor(13.5982, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.5492, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.7849, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT:  00004 00004 YY 00004 00004 F 00004 00004 etc 00004 00004 16 00004 00004 0004 4 0000 4 00004 0000 4 00004 4 0000 4, 4 00004 0000 00004 00004 boom 4 00004 0000 5 00004 0000 40000 00004 0000 4. 0000 4 00004 YA 0000 4 4 00004 00005 0000 4 4 0000 4 0000 4 00005 0000 4 00004 5 0000 4 00004 3 4 0000 4 4 0000 0000 4 4 00005 00004 00004 Hanec 4 0000 4 0000 5 00004 4 0000 5 0000 4 00005 4 00004 0000ata 4 4 0000 4 4 4 0000 4 bad 4 4 00004 4 4 00004 3 00005 0000 4 bl\n",
      "infinite\n",
      "r1: tensor(13.1885, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0905, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(2.3252, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT:  4 0000 4 4 5 4 0000 4 4 3 4 4 0000 4 10000 4 0000 4 0000 6 0000 4 4 0000 5 4 0000 4 0000 3 4 4 00004 5 4 0000 40000 4 0000 4 0000 0000 400000000 4 0000 4 0000 1 4 4 0000 4 5 4 4 0000 40000 5 4 4 4 4 4 5 4 4 4 0000 5 0000 5 0000 4 4 4 4 00005 4 4\n",
      "infinite\n",
      "r1: tensor(12.7946, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0563, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(3.2210, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT:  4 4 4 0000 3 4 0000 4 0000\n",
      "r1: tensor(13.2645, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0642, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.9128, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.7550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT:  4 4 3 4 0000 4 3 4 4 4 0000\n",
      "r1: tensor(13.2579, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0482, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.2163, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.6487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT:  5 4 4 4 3 4 5 4 4 5 4 3 4 4 5 4 5 4 0000 3 4 3 4 4 3 4 3 4 3 5 4 4 4 5 3 4 4 4 4 3 3 4 4 4 3 5 4 3 4 5 5 4 4 4 infinite 4 4 4 4 2 4 4 4 4 6 4 4 4 4 infinity 4 4 4 4 space 4 4 4 4\n",
      "infinite\n",
      "r1: tensor(13.3385, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0963, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(20.7233, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT:  3 4 4 4 5 5 4 3 4 3 3 4 3 4 5 3 4 3 4 2 4 4 3 4 6 4 4 3 4\n",
      "r1: tensor(13.1888, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0900, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.4720, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-2.6798, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  4\n",
      "DialoGPT:  4 4 4 4 1 4 4 4 4 0 4 4 4 4. 4 4 4 4. 4 4 4 4 8 4 4 4 4 infinite\n",
      "r1: tensor(13.1167, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0481, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2718, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-2.6431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  4\n",
      "DialoGPT:  4 3 4 4 2 4 3 4 4\n",
      "r1: tensor(13.0186, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1083, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1177, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.3805, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  4\n",
      "DialoGPT:  3 4 4 3 3 3 4 4 3\n",
      "r1: tensor(13.1439, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0526, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1275, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-4.0788, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  4\n",
      "DialoGPT:  4 5 4 4 3 4 2 3 4 4 4 2 3 4 3 4 6 3 4 4 4 6 3 4 3 4\n",
      "r1: tensor(12.8783, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0607, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1641, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-3.2565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT:  4 3 4 3\n",
      "r1: tensor(13.1503, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0457, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.9673, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-6.3876, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User:  4 4\n",
      "DialoGPT:  4 4 3 3 5 4 4 3 3 2 4 4 4 3 2 4 4 3 3 6 4 4 4 3 6 4 4 3 3 8 4 4 4 3\n",
      "r1: tensor(12.9863, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0775, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.1692, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-2.4968, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "User: \n",
      "DialoGPT:  4 4 5 4 2 4 4 2 4 5 4 4 2 4 2 4 4 5 4 6 4 4 5 4 1 4 4 3 4 1 4 4 5 4. 4 4 3 4 a 4 4 4 4 7 4 4 4 4 10 4 4 4 4 and 4 4 4 4 4096 4 4 4 44 4 4 4 4 9 4 4 4 4 books 4 4 4 4 fire 4 4 4 4 paper 4 4 4 4 a 4 4 3 4 8 4 4 3 4. 4 4 3 4 7 4 4 3 4. 4 4 5 4. 4 4 5 4 8 4 4 5 4 7 4 4 5 4 10 4 4 3 4 0 4 4 3 4s 4 4 4 4 sig 4 4 4 4 b 4 4 4 4 C\n",
      "infinite\n",
      "r1: tensor(13.1679, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1572, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(3.2490, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-69.4702, -65.3774, -60.8212, -55.9033, -51.9549, -47.8236, -33.7497,\n",
      "        -29.3051, -25.4648, -21.6628, -17.8077, -13.9715,  -9.6161,  -5.6534,\n",
      "          0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-4.7465, device='cuda:0', grad_fn=<NegBackward>), tensor(-5.1644, device='cuda:0', grad_fn=<NegBackward>), tensor(-5.4770, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.4679, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.6096, device='cuda:0', grad_fn=<NegBackward>), tensor(-14.4113, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.7377, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.0950, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.0186, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.0332, device='cuda:0', grad_fn=<NegBackward>), tensor(-3.9759, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.4516, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.0192, device='cuda:0', grad_fn=<NegBackward>), tensor(-5.6534, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-69.4702, grad_fn=<UnbindBackward>)\n",
      "Episode 25: 3.474340374653156\n",
      "User: Hello\n",
      "DialoGPT: Hiya! How are you? I'm feeling pretty good today. Hope your day is going well too :D hugs back atcha lt 3333333333 3333333 33333333 3333333 3333333333 33333333333 3333333 333333333 33333333 3333333 33333333 3333333 333333333 33333333 333333333 33333333 333333333 333333333 3333333333 333333333 3333333 33333333 33333333 33333333 33333333 33333333 3333333 33333333 3333333333 33333 3333333 3333333333 333333 333333 3334444 4444434444 400005555 55555000 0000 0001999 0000007 00000000 00085600 00009 00001 00002 00006 00004 00004 00004 1110 00004 00004 50000 00004 00004 619 00004 00004 0 00004 00004 000000 00004 00004 00000000 00004 00004 004 00004 00005 00004 00004 7 00004 00004 845 00004 00004 10 00004 00004 2 00004 00004 1 00004 00004 4 00004 00004 4096 00004 00004 10000 00004 00004 999 00004 00004 3 00004 00004 5000 00004 00004 44 00004 00004 9 00004 00004 D 00004 00004 C 00004 00004 B 00004 00004 T 00004 00004 E 00004 00004 LOT 00004 00004 A 00004 00004 M 00004 00004 R 00004 00004\n",
      "infinite\n",
      "r1: tensor(13.2921, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.8081, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.0806, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT:  0000 5 0000 0000 0000 4 0000 0000 0000 5 0000 0000 5 0000 50000 0000 5 0000 0000 4 0000 5 0000 0000 6 0000 40000 5 0000 0000 0000 50000 0000 4 0000 4 0000 0000 4 0000 40000 0000 0000 5 0000 4 0000 0000 5 0000 3 0000 5 0000 0000 3 0000 578 0000 5 0000 5 0000 5 5 0000 5 0000 4 5 0000 5 0000 0000 8 0000 5 0000 0000\n",
      "infinite\n",
      "r1: tensor(13.4390, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1784, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.6135, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT:  0000 4 0000 00004 0000 4 0000 0000 3 0000 4 0000 00005 0000 0000 0000 0000 0000 40000 0000 4 0000 0000 00 0000 4 0000 0000 2 0000 0000 0000 0000 3 0000 0000 0000 0000 5 000000 0000 0000 0000 0000 4000 0000 0000 4 0000 3 0000 0000 4 0000 6 0000 0000 0000 00004 0000 0000 0000 0000\n",
      "infinite\n",
      "r1: tensor(13.3567, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0372, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(20.7233, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT:  4 0000 4 0000 4 4 0000 4 0000 5 4 0000 0000 4 4 0000 0000 0000 4 4 4 0000 4 4 4 4 0000 0000 4 00 0000 4 0000 4 5 0000 0000 4 4 00 0000 4 5 0000 4 0000 4 3 0000 4 0000 4 10000 4 0000 4 0000 3 4 0000 4 0000 2 0000 4 0000 4 00000000 4 0000 4 0000 00 0000 4 4 0000 3 0000 4 4 0000 5 0000 4 4 0000\n",
      "infinite\n",
      "r1: tensor(13.2087, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0349, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(2.6755, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT:  4 4 0000 4 5 4 0000 4 0000 6 4 0000 4 0000 1 4 0000 4 00004 4 0000 4 0000\n",
      "infinite\n",
      "r1: tensor(13.3056, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.0342, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(1.0916, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "True 1\n",
      "r1: tensor(13.1710, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(1.1190, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.8536, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-10.2175, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-38.1420, -33.5454, -28.9068, -14.6662,  -9.4428,  -4.9376],\n",
      "       grad_fn=<CopySlices>)\n",
      "[tensor(-4.9320, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.9277, device='cuda:0', grad_fn=<NegBackward>), tensor(-14.3872, device='cuda:0', grad_fn=<NegBackward>), tensor(-5.3178, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.5546, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.9376, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "----- Loss: tensor(-38.1420, grad_fn=<UnbindBackward>)\n",
      "Episode 26: 3.5283278977429426\n",
      "User: Hello\n",
      "DialoGPT: Hiya! How are you? I'm feeling pretty good today. Hope your day is going well too lt 3 hugs and kisses back atcha'heartsies xxoxo hugs back to ya'selfs xoxoxoxoxoxxoxoxoxoxxxoxoxoxoxxxxoooooooohhhohoahhahahahahaaaaahhhhhhhhheeeeeeheeheeheeheeheeheeeehooooooooooohhhohoahhhohoahhohoahhohoaaaaaaaayyyyuhuuuughhhhehehehehehheheheheheeheeheehee hehehe XOXOHIPSHUGAHHUBSKYAPZIXNKSBYYAASMRSKYEBDLMSGHTSNSRTSWATTTYBUDDGYYYJOYCQYSNCISNTFSFTTHUPLSKTOOKAYWELLZRYBCF1STVUE4MEFW2CVNYEH8TW3DS5CS9LY7EF0RT6PLZEGSUCHEWANMKJVHF4TEKOROCBDEEBAHWRFXHDTVNZXTURGH4AVXUVZMAUVR4EVYQLZNNIXZAX4PCZVMUX4GWZZXDXMW4U4U4UZCUZ1MX4U4UBC5DH4U4U5VB4U4U7SD4U4U2DL4U4U8TC4U4U1,VTEC4U4U3PO4U4UQ4U4UBS4U4U9U4U4K4U4UU4U4BY4U4UBY4U4K5U4U4UB4U4U6U4U4T4U4UYA4U4UBD4U4UVC4U4URY4U4U0U4U4RY4U4BY5U4U5U4U2U4U4BC4U4U494U4U1437004U4U1894U4U4Q4U4BYBY4U4BYU4U4V4U4U 4U4U4Z4U4U40U4U4.jpg4U4U574U4UV4U4BY2U4U5BY4U4RY5U4U3U4U44U4U10U4U4Y4U4UY4U4BY8U4U4A4U4UK4U4BY3U4U54U4U46U4U4UE4U4U424U4U484U4U434U4U44U4U45U4U7U4U4UM4U4U504U4U864U4U UV4U4UUV4U4UUE4U4BY7U4U5RY4U44BY4U5U5U4BY4BY4U2U5U44U5U2U4BY44U4BY1U4U43U4U2BY4U444U44RY4U5U3U4BY4RY4U2U2U44U2U3U44U3U5U4RY4RY4U3U2U4RY44U4RY84U4U 2U4U42U4U24U4U794U4U UE4U4U784U4U654U4U584U4UUB4U4RY3U4U34U4U.4U4U204U4U044U4U634U4U 34U4Uub4U4U 54U4U254U4Ua4U4434U4424U4454U44.4U441U4444U544U4344U4544U53444U3444443444.44U4.4444.54U4.54444u44.4.4.54.4.34.4.2.4.44.43.4.4U54.443.44.5.4.434.44,4.4.74.4.8.4.4,4.5.5.44\n",
      "infinite\n",
      "r1: tensor(13.6776, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.4579, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.6224, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-4.6435,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-4.6435, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-4.6435, grad_fn=<UnbindBackward>)\n",
      "Episode 27: 3.5681538496698653\n",
      "User: Hello\n",
      "DialoGPT: Hiya! How are you? I'm feeling pretty good today, how about yourself? What's your favorite color? Mine is purple. Haha nice to meet ya! :D hugs and smiles back at you lt 3333cuddleshugsbackoffstagehandsomeface.jpg:P hugsss back off stage handsome face backoffsstagehandiesuppercutslapstabbyfacesheepshoeshellscratchybackshitkissesbackoffstagehandsomesuppercutslapshotgunshotshotgunshotshotshotshotshotshotshotshotshotshotshotswewellowshotsweOOOoooohhhHooooooHoooooooooHooooHooooHooooHHHooooHooooAhhhhhhhhahahahhohoHhaaahhhAaaaahhhahahaaaaaHeeehrrrrrhhoHhheeHhBehbuhhghhbshbbbehbiHhboHhggehbaHhbrhhrhhrhhrbhhrhhrhrhhrhhhrhhrdhhrhhrhhtbhrhhrhbshhrhhrlhhrhhrmhrhhrhfhrhhrhbhrhhrhrhrhhrhrbhrhhrbhrhhhrhrhhhrbhrhrhhrbbshhrhhhhrhhbshhrhrhbshhhrhbshrhhrhbhhrhhrhphhrhhrhsphrhhrhbphrhhrhblhhrhhrphrhhrhbrhhrhrhbhrhrhhhhhrhrhrhrhhbshrhhhrnhhrhhrdhrhhrhjhrhhrhplhhrhhrhmhrhhrhprhhrhhrthrhhrhgrhhrhhrbshhrhbshihrhhrhtlhrhhrhlehrhhrhholicshrhhrhhshrhhrhtshhrhhrkhrhhrhsthrhhrhstrhhrhhrghrhhrhhhhrhhrhtrhhrhhrrhrhhrharrhhrhhrhornhhrhhrvhrhhrhthrhhrhrthrhhhrthrhrhhrtchhhrhhrbrhhrhhbrhhrthrthrhbshbshhrthrbhrhbshbrhhrbhrthrhbrhhhrhbrthrhhrtbshhrhbrhrhhrhmerhhrhhrwhrhhrhaurhhrhhrfhrhhrtbhrhhrthembshhrhbhhrhhbhrhhhbrhhhhrtbshrhhrthhrhhrathrhhrhrehrhhrhwhrhhrt hhrhhrh bhrhhrhsohrhhrhghrhhrtthrhhrbthrhhrrhhrhhrrrhhrhhrchrhhrhrhrhhrtbrhhrhbrbhrhhrrbrhhrhtbhrhhbrthrthrtbhrthrtbshhhrtbhhrthrrhrthrtthrthrbhhrtbbrhhrtbthrthrhrthrtbrhhhrttbhrtbhrbhrtbhhhrtbrthrtbhthrthrrhhrthrwhrthrthhrthrghrthrtfyhrthrtttthrthrwthrthrgthrthrmhrthrtwhrthrhthrttthrttbhhrhtthrtbwhrthrbwhrtthrbhrtthrrhrrhrtthrwhrrthrthrtlhrthrttlhrthrrthrrtthrttlbhrthrrtlhrthrbthrtttlhrtthrtlhrrhrttlhrrtlhrrtlbhrttlhrtlhrttlhrbhrttltlhrttltltlhrthrtltlhrtttltlhrtlhrtlhrrtltlhrtltltltltlhrtlbtltltltlbtltlhrtlbltltltltlbltltlhrtl ttltltltl drtltltltl trtltltl\n",
      "infinite\n",
      "r1: tensor(13.6218, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.7043, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.9525, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-4.9149,  0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-4.9149, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-4.9149, grad_fn=<UnbindBackward>)\n",
      "Episode 28: 3.614594977477501\n",
      "User: Hello\n",
      "DialoGPT: Hiya! How are you? I'm feeling pretty good today, how about yourself? What's your favorite color? Mine is purple. Haha nice to meet ya! :D hugs and smiles back at you lt 33333333 33333333 33333333 3333333 3333333333 33333333333 3333333 33333333 33333333333 3333 3333333333 3333 33333333 3333 3333 3333333 3333 33333333333 3333 33333 3333333 3333333 33333 3333333333 33333 33333 33333333333 33333 3333333 33333 3333333 33333333333333333334660044000400005555674310085006000456783580040010000000004640985909220054140190704880551805619087171337204942069450958561636785760503926952789497298375773004169658628534543879375417099999682587306317564611091297421194298462964584635041594637044449999413546194145474404398463994123456463014268563462804184394 4444494 3954 394 429946 3884 00000000 004 4645 444446 36bb4 4004 515 3445 3846 3964 6044 1094 3994 594 1114 1194 5846 4344 484 0.4 9600 4504 1045 5554 604 8888 1014 419944 459 459 459 394 459 459 3954 459 4598 459 4594 459 456 2944 459 456 394 459 394 394 4594 456 394 394 394 3954 459 3944 459 4x4 459 439 459 459 454 459 441 459 459 6645 459 459 564 459 4ppc4 459 4th 459 459 4000 459 459 2 459 459 404 459 469 459 459 354 459 4 459 459 1 459 459 444 459 446 459 459 464 459 4TH4 459 44 459 4 394 459 3954 394 39 459 459 559 459 4394 459 4 3954 4594 394 439 459 394 3954 394 395 459 459 420 459 459 3 459 459 494 459 4 594 459 4 559 459 39 459 439 394 459 559 394 439 394 394 559 459 395 459 439 3954 459 395 439 459 3954 3954 439 439 4594 439 4 459 439 559 4594 39 459 394 559 394 39 439 459 559 395 459 394 3 459 439 3 459 4 439 459 3 439 459 44 459 459 34 459 459 C4 459 449 459 459 37 459 459 759 459 4 4 459 4 4 394 4394 439 4 394 4 459 4 39 459 4 44 459 39 4 459 44 394 4 4 459 39459 459 4 39 394 459 3 4 459 4 395 459 4 4 395 459 39 439 4 4 459 3 4 39 459 39 4 39 459 3 44 459 3 39 459 4 39 395 459 4 39 3 459 4 39 5 459 459 6 459 459\n",
      "infinite\n",
      "r1: tensor(13.6105, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.9268, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.5890, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User:  4\n",
      "DialoGPT:  4 4 4 4 459 54 4 4 4 469 4 4 4 4 3 4 4 4 44 4 459 b 4 4 459 0 4 4 4 4 5 4 4 4 4 39 4 4 4 4 59 4 4 4 4 0 4 4 4 5 5 4 4 4 3 3 4 4 4 54 4 4 5 4 5 4 4 3 4 5 4 4 5 4 3 4 4 5 4 0 4 4 3 4 3 4 4 3 4 6 4 4 4 4 2 4 4 4 4 1 4 4 4 4 total 4 4 4 4 420 4 4 4 4 t 4 4 4 4 6 4 4 5 4 2 4 4 3 4 2 4 4 5 4 1 4 4 3 4\n",
      "infinite\n",
      "r1: tensor(13.9634, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1100, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.9854, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User:  4 4\n",
      "DialoGPT:  4 4 5 4 6 4 4 3 4 39 4 4 5 4 39 4 4 3 4 1 4 4 5 4. 4 4 4 4 10 4 4 4 4 7\n",
      "infinite\n",
      "r1: tensor(13.5548, device='cuda:0', grad_fn=<NegBackward>) r2: tensor(0.1111, device='cuda:0', grad_fn=<NegBackward>) r2_2: tensor(0.2445, device='cuda:0', grad_fn=<NegBackward>) r3: tensor(-20.7233, device='cuda:0', grad_fn=<LogBackward>)\n",
      "User: \n",
      "DialoGPT: \n",
      "dummy\n",
      "False 0\n",
      "tensor([-13.6901,  -8.9384,  -4.2442,   0.0000], grad_fn=<CopySlices>)\n",
      "[tensor(-4.8410, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.7367, device='cuda:0', grad_fn=<NegBackward>), tensor(-4.2442, device='cuda:0', grad_fn=<NegBackward>), tensor(0.)]\n",
      "----- Loss: tensor(-13.6901, grad_fn=<UnbindBackward>)\n",
      "Episode 29: 3.655475910504659\n",
      "User: Hello\n"
     ]
    }
   ],
   "source": [
    "chat_history_ids = tokenizer.encode(tokenizer.bos_token + \"Hello\" + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "reward = 0\n",
    "USE_CUDA = True\n",
    "if USE_CUDA:\n",
    "    device ='cuda'\n",
    "    model  = model.cuda()\n",
    "    user   = user.cuda()\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    model  = model.cpu()\n",
    "#     user = user.cpu()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "# replay_buffer = ReplayBuffer(10000)\n",
    "batch_size = 64\n",
    "gamma      = 0.99  # 0.99\n",
    "ep_rewards = []\n",
    "\n",
    "for episode in range(100):\n",
    "    chat_history_ids = tokenizer.encode(\"Hello\" + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "    questions = []\n",
    "    answers   = []\n",
    "    turns     = []\n",
    "    rewards   = []\n",
    "    model.eval()\n",
    "    max_length = 1000\n",
    "    for frame in range(20):\n",
    "        epsilon = epsilon_by_frame(frame)\n",
    "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    #     new_user_input_ids = tokenizer.encode(tokenizer.eos_token, return_tensors='pt')\n",
    "        input_ids = chat_history_ids[-(max_length-100):].to(device)\n",
    "        chat_history_ids = user.generate(input_ids, max_length=max_length, \n",
    "                                         pad_token_id=\n",
    "                                         tokenizer.eos_token_id, \n",
    "#                                          repetition_penalty=1.75,\n",
    "#                                          do_sample=True,\n",
    "#                                          temperature=0.98,\n",
    "#                                          top_k=0,\n",
    "#                                          num_beams=3,\n",
    "    # #                              num_return_sequences=1,\n",
    "    #                              early_stopping=True,\n",
    "                                         no_repeat_ngram_size=5\n",
    "                                ) if frame > 0 else input_ids\n",
    "        question = chat_history_ids[:, input_ids.shape[-1]:][0] if frame > 0 else input_ids[0]\n",
    "        questions.append(question.to(device))\n",
    "        turns.append(question)\n",
    "        print(\"User: {}\".format(decode(question)))\n",
    "\n",
    "        # append the new user input tokens to the chat history\n",
    "        input_ids = chat_history_ids[-(max_length-100):].to(device) # if step > 0 else new_user_input_ids\n",
    "        # generated a response while limiting the total chat history to 1000 tokens, \n",
    "        chat_history_ids = model.generate(input_ids, \n",
    "                                          pad_token_id=tokenizer.eos_token_id,\n",
    "                                          max_length=max_length, \n",
    "                                          repetition_penalty=1.25,\n",
    "#                                           min_length=2,\n",
    "#                                           do_sample=True,\n",
    "#                                           temperature=0.99,\n",
    "#                                           top_k=40,\n",
    "#                                           num_beams=3,\n",
    "                                          early_stopping=True,\n",
    "    #                                       num_return_sequences=3,\n",
    "                                          no_repeat_ngram_size=5\n",
    "                                         )\n",
    "\n",
    "        # pretty print last output tokens from bot\n",
    "        answer = chat_history_ids[:, input_ids.shape[-1]:][0]\n",
    "        answers.append(answer)\n",
    "        turns.append(answer)\n",
    "        print(\"DialoGPT: {}\".format(decode(answer)))\n",
    "        \n",
    "#         if len(question) == 0: questions[-1] = torch.tensor([tokenizer.eos_token_id])\n",
    "#         if len(answer) == 0: answers[-1] = torch.tensor([tokenizer.eos_token_id])\n",
    "        \n",
    "        if is_dummy_sentence(answer) or len(answer) == 0:\n",
    "            print('dummy')\n",
    "            print(len(answer) > 0, len(answer))\n",
    "            \n",
    "            reward = compute_reward(questions, answers) if len(answer) > 0 else torch.tensor(0.0)\n",
    "#             reward += torch.tensor(0.0 - 1*len(answers), requires_grad=True)\n",
    "            rewards.append(reward)\n",
    "            break\n",
    "        else:\n",
    "            reward = compute_reward(questions, answers)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            \n",
    "    # Train\n",
    "    model.train()\n",
    "    r = discount_rewards(rewards)\n",
    "    print(r)\n",
    "    print(rewards)\n",
    "#     model = model.cuda()\n",
    "    loss = rewards[0]#.to('cuda')\n",
    "    \n",
    "#     loss = r.mean()\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "    if loss.item() != np.NINF or True:\n",
    "        ep_rewards.append(rewards[0].item())\n",
    "        for l in r[:1]:\n",
    "            if l.item() != np.NINF and l.item() != np.nan:\n",
    "                print('----- Loss:', l)\n",
    "                optimizer.zero_grad()\n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "#         if loss.item() != np.NINF\n",
    "#             loss.backward()\n",
    "#                 for param in model.parameters():\n",
    "#                         param.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "#                 optimizer.step()\n",
    "\n",
    "#         optimizer.step()\n",
    "    print(f'Episode {episode}:', -np.mean(ep_rewards))\n",
    "    \n",
    "    # Limit chat_history_ids to 50 tokens\n",
    "    chat_history_ids = chat_history_ids[:,-30:]\n",
    "    \n",
    "#     model = model.cpu()\n",
    "#     reward -= jaccard_similarity(answer.numpy(), context.numpy())\n",
    "    \n",
    "#     state = torch.cat([question, answer_ids], dim=-1)  # add separation token?\n",
    "#     action = act(model, state, epsilon)\n",
    "    # next_state, reward, done, _ = next_step(action)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(reward)\n",
    "#     print(answer)\n",
    "#     print(context)\n",
    "#     print(chat_history_ids)\n",
    "print(tokenizer.decode(chat_history_ids[0], skip_special_tokens=True))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2108d6c5a48>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(-1*np.array(ep_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "trace() missing 1 required positional argument: 'example_inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-f2a187c876bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Creating the trace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtraced_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraced_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"traced_bert.pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: trace() missing 1 required positional argument: 'example_inputs'"
     ]
    }
   ],
   "source": [
    "# Creating the trace\n",
    "traced_model = torch.jit.trace(model)\n",
    "torch.jit.save(traced_model, \"traced_bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Did you see \"Titanic\"?\n",
      "DialoGPT: I did not see that one coming. Bravo sir. Bravo indeed. Bravo. Bravo is the only way to go. Bravo, Bravo is all you need to go through life. Bravo Bravo Bravo. bravo bravo. brava bravo bolto bolto bravo bola bolto bola bravo Bravo bolto Bravo bolo bolo brava bolto Bravo bravos bolto Bolto bravos bravo Bravos bolo Bravo Bravoos bolto Brola Bravos Bravos Brola Brola bravos Bravo Brola Bravo Bravos Bolo Bravashehinohinohehhohehadohehhadohadohohehehouhadohelmehadoehehhehbhohehohebhohhohhehehebhehjhohejhohbhehejhhebhhejhehbehhehohjhbhbhahehfhhefhjhehefhehghehghhheeheheehheghehegjhjbhebbehjthehchehahahhheehhehhaheheehhhehrhheeehhebehhheahehahhhehhhehhrhhhjehhehehrhehahahehhhheheeeeheehrhjfhhbbehehthehebehehehoeheehbheeewehefehheejhhfheheahemhehwehheewehhbeheebehbwhehwhehewheeehebweweewhhewewhhwewhehewehrtheewewewewhewwehwwehehrehhewehweheheewhwwewwewehwewtewewehbewebeeweeewwweeewewwwwewewwehewwheewwcwewbwweewebwwwhewewweehwwehrwewwwwewyeweowwewgwewaewe wewejwewcwwcewewwwwwwwweyewwwweweawwewfwewmewewhwewdwewlwewoewefwwfwwegwwtewwfeweherewewwwwwewherewwgwwewewwbwcwewcawwwawwweechwewbewewechwwjwwbewwwrwewwrwwadwewnwewswewrwewenewwtwewjwwebwtwewtbwwwtewtwwdwwywwapwwmwwwowwwechwwejwcjwtjwwwwwwcrewwwtwewwgjwgewwjtewywewshwwshwewtweweechwcgwcbewwewwwtawwcwywwwwwwwrewwzww www.wwewweshwwwhwwwerwwthwwtlwwnwwherewwefwc wwewerwwlwwhwcwerwewetwwetwewtlwewwowwewthwewwerwwetwwecwwewrwweaewwbtwwwontwwsoww.wwrewtwt wwweldww2wwowwwngwwbrtwcwrwttwjbwjjwjcwtcwcwswwgrwwwswewzwewkwwkwewtdwwgalwwtdwewblwwblwjrwwrwjgwjwewjhwjewwwrjwbjwwrrwcbwgwewgbwbwewwwjw.jwfjwtljwmjwbrwwtvwwbcww zww\n",
      "User: I saw it twelve times.\n",
      "DialoGPT: wjtljjjtlwj.jjtjjbjjrjw2jj.tjtlbjtltljtl jtljr.jtl.jrtlj.tlj\n",
      "User: I have the DVD.\n",
      "DialoGPT: \n",
      "User: Let's go to your home.\n",
      "DialoGPT: \n",
      "User: And then we can go to my home.\n",
      "DialoGPT: \n",
      "User: I always cry at the end.\n",
      "DialoGPT: \n"
     ]
    }
   ],
   "source": [
    "# Let's chat for 5 lines\n",
    "sentences = [\"Did you see \\\"Titanic\\\"?\",\n",
    "            \"I saw it twelve times.\",\n",
    "            \"I have the DVD.\",\n",
    "            \"Let's go to your home.\",\n",
    "            \"And then we can go to my home.\",\n",
    "            \"I always cry at the end.\"]\n",
    "for step in range(len(sentences)):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "#     new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    new_user_input_ids = tokenizer.encode(sentences[step] + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "    print(\"User:\", sentences[step])\n",
    "    \n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=3)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17.1766, 14.1621], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rews = discount_rewards(rewards)\n",
    "rews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "rewards[0].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-222-590a0fe59134>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrews\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "for r in rews:\n",
    "    r.backward()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "logits = model(v1)[0]\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "prev_input = torch.multinomial(probs, num_samples=1)\n",
    "decode(prev_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello!Hello! :DHow are you?I'm good! How are you?Pretty good!That's good!Can you recommend me a movie?I can!Tell meI will!\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(chat_history_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 15496, 50256,  ..., 50256, 50256, 50256],\n",
       "        [50256, 15496, 50256,  ..., 50256, 50256, 50256],\n",
       "        [50256, 15496, 50256,  ..., 50256, 50256, 50256],\n",
       "        ...,\n",
       "        [50256, 15496, 50256,  ..., 50256, 50256, 50256],\n",
       "        [50256, 15496, 50256,  ..., 50256, 50256, 50256],\n",
       "        [50256, 15496, 50256,  ...,   393,  1223, 50256]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ -1.0835, -18.4458, -17.6662,  ..., -14.8533, -13.5491,   0.9567],\n",
       "          [ -6.1334, -16.7606, -15.5976,  ..., -10.8078, -10.3935,   1.5186],\n",
       "          [  6.9852, -10.1889,  -8.7772,  ...,  -6.3700,  -5.0316,   5.8794],\n",
       "          ...,\n",
       "          [  5.4960, -10.9813,  -9.6111,  ...,  -6.2588,  -3.6435,   8.3031],\n",
       "          [ -2.2493, -12.7885, -12.5046,  ...,  -8.5337,  -5.7225,  11.4771],\n",
       "          [  5.3813, -11.0729,  -9.6908,  ...,  -6.2201,  -3.6808,   8.4822]]],\n",
       "        grad_fn=<UnsafeViewBackward>),\n",
       " (tensor([[[[[ 3.5647e-01,  2.9394e-02,  1.2568e-01,  ..., -5.8864e-01,\n",
       "              -1.5299e-02, -1.5551e-01],\n",
       "             [-2.4501e-01, -1.0144e-01,  9.3549e-02,  ..., -3.0358e-01,\n",
       "              -1.4868e-01,  2.2968e-02],\n",
       "             [-4.9309e-01, -4.7411e-02,  1.2059e-01,  ..., -2.4241e-01,\n",
       "              -3.4761e-01, -3.3903e-01],\n",
       "             ...,\n",
       "             [-8.7066e-01, -2.2038e-01, -2.6833e-01,  ..., -1.0996e-01,\n",
       "              -7.4337e-01, -1.9390e-01],\n",
       "             [-6.2056e-01, -3.1638e-01, -1.9239e-01,  ...,  4.4419e-03,\n",
       "              -4.6287e-01,  1.8755e-01],\n",
       "             [-8.7530e-01, -2.0415e-01, -2.5814e-01,  ..., -1.4883e-01,\n",
       "              -7.3084e-01, -2.2438e-01]],\n",
       "  \n",
       "            [[-6.1324e-01, -6.9942e-01, -1.6361e+00,  ...,  7.0650e-03,\n",
       "               3.5510e-01,  1.7768e-02],\n",
       "             [ 3.2067e-01, -1.6376e-01,  1.1313e+00,  ...,  1.0370e-01,\n",
       "              -1.0782e-01,  5.0387e-01],\n",
       "             [-5.4835e-01, -7.1475e-01, -1.2811e+00,  ..., -2.1157e-01,\n",
       "               3.4687e-01,  1.4071e+00],\n",
       "             ...,\n",
       "             [-6.0077e-01, -8.4454e-01, -1.4336e+00,  ..., -7.2469e-02,\n",
       "               5.2511e-01,  1.4458e+00],\n",
       "             [ 4.1453e-01, -2.4197e-01,  1.4339e+00,  ...,  2.2245e-01,\n",
       "              -1.6637e-01,  4.9240e-01],\n",
       "             [-6.4262e-01, -9.0989e-01, -1.4511e+00,  ..., -2.2200e-02,\n",
       "               5.2657e-01,  1.4381e+00]],\n",
       "  \n",
       "            [[-2.7557e-01, -2.0155e-01, -6.3953e-01,  ...,  1.0910e+00,\n",
       "              -4.3234e-03, -1.7263e-01],\n",
       "             [ 3.1270e-01,  1.7569e-01, -1.0122e-01,  ..., -3.8638e-01,\n",
       "               1.8320e-01,  9.4447e-02],\n",
       "             [ 7.5713e-01,  2.5464e-01, -1.0865e+00,  ...,  1.0854e+00,\n",
       "              -5.6328e-01,  3.9266e-01],\n",
       "             ...,\n",
       "             [ 4.8354e-01,  2.1161e-01, -1.0193e+00,  ...,  1.2306e+00,\n",
       "              -7.2439e-01,  3.9395e-01],\n",
       "             [ 3.9067e-02,  9.4769e-02,  1.1326e-01,  ..., -6.3839e-01,\n",
       "               1.7478e-01,  1.9321e-01],\n",
       "             [ 4.9378e-01,  2.1582e-01, -1.0807e+00,  ...,  1.2124e+00,\n",
       "              -7.0893e-01,  3.7200e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-2.7213e-01, -2.0335e-01, -1.8826e-02,  ..., -3.3091e-01,\n",
       "               3.3576e-01, -4.3095e-01],\n",
       "             [ 3.6834e-01,  1.4400e-01, -2.7347e-02,  ...,  4.5087e-01,\n",
       "              -2.3431e-01,  1.3409e-01],\n",
       "             [ 1.7375e-01,  2.0847e-01,  2.7629e-01,  ..., -7.6812e-01,\n",
       "               2.8449e-01,  3.5646e-01],\n",
       "             ...,\n",
       "             [ 3.7828e-01,  3.1220e-01,  1.9310e-01,  ..., -1.3968e+00,\n",
       "               5.8986e-01, -2.2694e-01],\n",
       "             [ 3.8820e-01,  8.5326e-02, -1.0958e-01,  ...,  5.6934e-02,\n",
       "              -7.2865e-02, -3.3456e-01],\n",
       "             [ 3.5121e-01,  2.5736e-01,  1.9702e-01,  ..., -1.3461e+00,\n",
       "               5.8669e-01, -2.5786e-01]],\n",
       "  \n",
       "            [[ 3.4766e-01, -3.1442e-01, -2.2133e-01,  ..., -1.0254e+00,\n",
       "              -8.3510e-01,  2.3008e-01],\n",
       "             [ 2.1702e-01, -4.7279e-01, -4.1508e-01,  ...,  7.1609e-01,\n",
       "              -4.1796e-02,  8.3373e-02],\n",
       "             [ 2.9578e-01, -1.3560e-01, -5.1633e-01,  ..., -4.2625e-01,\n",
       "              -1.1206e+00,  4.5758e-01],\n",
       "             ...,\n",
       "             [ 4.5645e-01, -5.9489e-02, -3.3366e-01,  ..., -6.8987e-01,\n",
       "              -1.5137e+00,  2.3145e-01],\n",
       "             [ 1.6369e-01, -3.9806e-01, -5.1567e-01,  ...,  8.8409e-01,\n",
       "              -2.5165e-02,  8.8157e-03],\n",
       "             [ 4.7713e-01, -6.7030e-02, -3.6515e-01,  ..., -6.5550e-01,\n",
       "              -1.5063e+00,  2.5451e-01]],\n",
       "  \n",
       "            [[ 2.9870e-01,  2.9359e-01, -1.7060e-02,  ...,  9.1998e-01,\n",
       "              -1.2900e+00, -4.4113e-01],\n",
       "             [ 1.3649e-02, -3.3173e-01,  1.1074e-01,  ..., -3.2771e-01,\n",
       "               5.3511e-01,  1.1888e-01],\n",
       "             [ 8.2113e-02,  3.5760e-01,  5.7908e-01,  ...,  1.3913e+00,\n",
       "              -5.6062e-01, -6.8358e-01],\n",
       "             ...,\n",
       "             [ 1.9669e-01,  5.1821e-01,  5.1821e-01,  ...,  1.4756e+00,\n",
       "              -1.0319e+00, -8.4158e-01],\n",
       "             [ 7.3062e-02, -4.5276e-01,  6.6683e-02,  ..., -6.3871e-01,\n",
       "               3.9133e-01,  2.6854e-01],\n",
       "             [ 2.1427e-01,  5.1307e-01,  5.4620e-01,  ...,  1.5034e+00,\n",
       "              -1.0510e+00, -8.5909e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-1.5427e-02,  1.4064e-02, -1.4428e-02,  ..., -2.6810e-02,\n",
       "              -1.0428e-02, -7.6356e-03],\n",
       "             [-3.7155e-02, -5.5862e-02, -3.3441e-02,  ...,  4.6452e-02,\n",
       "              -6.9445e-03,  7.6375e-03],\n",
       "             [-2.7414e-02,  2.5784e-02,  4.0973e-03,  ..., -4.4848e-02,\n",
       "              -4.5324e-02, -5.4945e-03],\n",
       "             ...,\n",
       "             [-3.0233e-02,  1.5706e-02, -1.8014e-02,  ..., -6.0905e-02,\n",
       "              -4.2522e-02,  1.8541e-03],\n",
       "             [-7.4506e-02, -7.2123e-02, -7.0580e-02,  ...,  7.2690e-02,\n",
       "               3.5773e-02,  1.4194e-02],\n",
       "             [-3.5731e-02,  2.1805e-02, -1.6192e-02,  ..., -6.3145e-02,\n",
       "              -3.5476e-02,  3.1879e-03]],\n",
       "  \n",
       "            [[ 8.3074e-03, -2.8373e-02, -2.6207e-02,  ..., -1.1710e-02,\n",
       "               1.6359e-02, -7.5925e-04],\n",
       "             [ 5.2416e-03,  3.3944e-02,  3.5075e-02,  ..., -4.5050e-03,\n",
       "              -2.3753e-03, -2.3858e-02],\n",
       "             [ 1.3297e-02, -2.9829e-02,  2.1122e-02,  ...,  1.2053e-03,\n",
       "               9.6025e-03,  4.8546e-03],\n",
       "             ...,\n",
       "             [ 2.4855e-02, -4.8428e-02,  7.3654e-03,  ..., -8.4170e-03,\n",
       "               1.0500e-02,  9.3064e-03],\n",
       "             [-6.5294e-04,  3.7523e-02,  4.9824e-03,  ..., -3.2325e-02,\n",
       "              -3.0355e-03, -3.4064e-02],\n",
       "             [ 2.3316e-02, -5.0718e-02,  1.1400e-03,  ..., -4.6231e-03,\n",
       "               1.1423e-02, -1.5631e-03]],\n",
       "  \n",
       "            [[ 7.5115e-03, -7.2156e-03, -2.1719e-03,  ...,  5.1252e-03,\n",
       "               1.3564e-02, -1.3940e-02],\n",
       "             [-1.2027e-02,  1.9729e-02,  2.6076e-03,  ..., -5.5551e-03,\n",
       "              -4.5144e-02,  3.6382e-02],\n",
       "             [ 7.1255e-03, -1.3193e-02,  4.3888e-03,  ...,  1.3248e-02,\n",
       "              -1.0062e-02, -9.6777e-03],\n",
       "             ...,\n",
       "             [ 1.3177e-02, -9.6703e-03,  6.9762e-03,  ...,  1.6377e-02,\n",
       "               3.3508e-03, -1.3634e-02],\n",
       "             [-2.9910e-02,  1.3213e-02,  1.1513e-02,  ..., -1.1051e-02,\n",
       "              -5.7365e-02,  4.4392e-02],\n",
       "             [ 9.6987e-03, -5.6534e-03, -5.4152e-03,  ...,  1.1793e-02,\n",
       "               7.7697e-03, -1.3555e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 2.8884e-03, -3.1777e-04,  1.2526e-02,  ...,  1.0357e-01,\n",
       "              -1.3195e-03,  2.7323e-02],\n",
       "             [ 1.4252e-02, -8.5170e-03,  1.0223e-02,  ..., -1.5446e-02,\n",
       "              -2.5261e-02, -2.9132e-02],\n",
       "             [ 6.4918e-04,  2.2602e-02,  2.3441e-02,  ...,  2.5597e-01,\n",
       "              -7.0115e-03,  2.0920e-02],\n",
       "             ...,\n",
       "             [ 5.2084e-03,  8.6208e-03,  2.6251e-02,  ...,  1.5231e-01,\n",
       "              -2.1594e-02,  1.9266e-02],\n",
       "             [ 1.1481e-02, -3.9369e-02,  3.1823e-02,  ..., -5.2330e-02,\n",
       "              -8.8497e-03, -3.6166e-02],\n",
       "             [ 3.0384e-03,  9.7844e-03,  1.9395e-02,  ...,  1.6410e-01,\n",
       "              -2.1198e-02,  2.2550e-02]],\n",
       "  \n",
       "            [[-3.8660e-02, -1.9084e-02, -4.5845e-03,  ..., -4.2062e-03,\n",
       "               1.2585e-02,  4.7612e-03],\n",
       "             [ 6.1396e-02,  4.5043e-02, -1.5445e-02,  ...,  3.3933e-02,\n",
       "              -2.8724e-02, -1.4793e-02],\n",
       "             [ 9.2317e-03, -1.5910e-02,  5.7461e-03,  ...,  4.7116e-02,\n",
       "              -6.7413e-03,  3.8151e-02],\n",
       "             ...,\n",
       "             [-6.6746e-03, -5.7703e-03,  4.2490e-03,  ...,  2.6396e-02,\n",
       "              -9.8569e-03,  3.3953e-02],\n",
       "             [ 5.5540e-02,  2.8859e-02, -2.8222e-02,  ...,  3.0147e-02,\n",
       "              -3.4354e-02, -2.3551e-02],\n",
       "             [-6.1915e-03, -1.4374e-02,  7.6163e-03,  ...,  2.7397e-02,\n",
       "              -1.7698e-02,  3.3610e-02]],\n",
       "  \n",
       "            [[ 1.0614e-01, -6.2515e-03,  8.8402e-03,  ...,  1.5647e-03,\n",
       "              -7.9393e-03, -3.1664e-04],\n",
       "             [-1.0708e-01, -3.1211e-03,  3.6196e-03,  ..., -7.2911e-03,\n",
       "               1.2847e-02, -4.6337e-02],\n",
       "             [ 1.8343e-01, -9.3105e-03,  1.3973e-02,  ..., -2.7889e-02,\n",
       "              -1.3634e-02,  1.4880e-03],\n",
       "             ...,\n",
       "             [ 1.6527e-01, -8.2843e-03,  5.8837e-03,  ..., -1.3642e-02,\n",
       "              -1.5869e-02,  1.7887e-03],\n",
       "             [-1.3140e-01, -1.1849e-02,  2.6250e-03,  ..., -5.2022e-03,\n",
       "               2.2451e-02, -6.0746e-02],\n",
       "             [ 1.5053e-01, -8.6654e-03,  6.2439e-03,  ..., -1.6795e-02,\n",
       "              -2.0212e-02,  5.8278e-03]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 1.3732e-03, -3.1152e-02, -2.1452e-01,  ..., -2.7631e-01,\n",
       "              -1.2909e+00,  7.3533e-01],\n",
       "             [ 5.5356e-01,  5.9802e-02,  7.7254e-01,  ...,  3.8367e-01,\n",
       "               6.7550e-01, -4.6682e-01],\n",
       "             [ 1.0180e-01,  6.6716e-02, -1.2580e-01,  ..., -5.1118e-01,\n",
       "              -6.9886e-01,  9.8184e-01],\n",
       "             ...,\n",
       "             [ 2.1958e-01,  1.2745e-01, -3.7242e-01,  ..., -5.6320e-01,\n",
       "              -7.5294e-01,  1.1791e+00],\n",
       "             [ 8.4037e-01, -3.2573e-02,  4.0821e-01,  ...,  2.3323e-01,\n",
       "               1.2273e-01,  4.4319e-02],\n",
       "             [ 2.1496e-01,  1.1876e-01, -4.4507e-01,  ..., -5.5489e-01,\n",
       "              -8.4894e-01,  1.1979e+00]],\n",
       "  \n",
       "            [[-1.1132e+00, -1.2448e-01,  1.0983e+00,  ..., -7.3489e-03,\n",
       "               6.9657e-02, -1.6341e-01],\n",
       "             [-2.6266e-01, -3.5057e-01,  1.0390e+00,  ...,  1.2706e+00,\n",
       "               1.6491e-02, -8.8436e-01],\n",
       "             [-1.7826e-01, -2.5279e-01,  2.9990e-01,  ...,  4.5925e-01,\n",
       "               6.1034e-01, -1.4961e-01],\n",
       "             ...,\n",
       "             [-4.8965e-02, -6.2697e-01, -1.8539e+00,  ..., -2.2442e-01,\n",
       "              -1.4169e-01,  1.9390e-01],\n",
       "             [-1.7084e-01, -6.4219e-01, -1.0467e+00,  ...,  4.3814e-01,\n",
       "              -6.7040e-01, -3.0298e-01],\n",
       "             [-4.8870e-02, -6.4088e-01, -2.0001e+00,  ..., -2.8450e-01,\n",
       "              -2.0127e-01,  2.0114e-01]],\n",
       "  \n",
       "            [[ 1.3171e+00,  6.7066e-01,  1.0201e+00,  ...,  1.4822e+00,\n",
       "              -6.5354e-04, -1.2782e+00],\n",
       "             [-4.9257e-01,  7.0507e-01,  2.8585e-01,  ..., -1.0026e+00,\n",
       "               3.3309e-01,  1.0703e+00],\n",
       "             [ 8.1321e-01, -1.3244e-01,  2.2641e-01,  ..., -2.8384e-01,\n",
       "               1.8885e-01, -1.3899e+00],\n",
       "             ...,\n",
       "             [-1.6844e-01, -4.6495e-01, -3.3195e-01,  ...,  3.2054e-01,\n",
       "               1.3388e+00, -2.3499e+00],\n",
       "             [-9.5213e-01,  4.4838e-01, -2.2329e-01,  ...,  1.0145e-01,\n",
       "               1.4386e+00, -5.5737e-01],\n",
       "             [-2.6512e-01, -4.4866e-01, -3.9866e-01,  ...,  3.9018e-01,\n",
       "               1.4138e+00, -2.3916e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-1.9582e-01,  9.0748e-02,  4.6948e-01,  ...,  2.0914e-01,\n",
       "              -1.9690e-01,  1.7281e-01],\n",
       "             [ 3.9708e-01,  4.9278e-01,  3.4358e-01,  ..., -5.7817e-01,\n",
       "              -1.1119e+00, -3.5681e-01],\n",
       "             [-5.3029e-01,  1.0232e+00,  2.6085e-01,  ...,  6.2739e-02,\n",
       "              -4.8667e-01, -5.2726e-01],\n",
       "             ...,\n",
       "             [ 4.9742e-02,  1.6388e-01,  5.5877e-01,  ...,  1.0970e+00,\n",
       "               3.0226e-01, -4.3555e-01],\n",
       "             [ 7.5735e-01, -5.1173e-01,  2.7250e-01,  ...,  1.0058e+00,\n",
       "               1.2645e-02, -4.5818e-01],\n",
       "             [ 1.0384e-01,  8.5821e-02,  6.2418e-01,  ...,  1.1076e+00,\n",
       "               3.4843e-01, -4.3180e-01]],\n",
       "  \n",
       "            [[-2.1836e-02, -1.6478e-02,  3.3250e-01,  ...,  4.6523e-01,\n",
       "              -3.0216e-01, -5.4917e-01],\n",
       "             [-7.5696e-01, -9.0302e-01,  3.9400e-02,  ...,  4.5685e-02,\n",
       "              -1.3434e-01, -1.3074e+00],\n",
       "             [ 4.3539e-02,  4.6468e-01,  1.2926e-02,  ..., -7.1159e-01,\n",
       "              -4.3663e-01, -2.5163e-01],\n",
       "             ...,\n",
       "             [ 1.8335e+00,  1.3168e+00,  1.2520e+00,  ..., -1.0992e+00,\n",
       "              -1.6982e+00, -2.4647e+00],\n",
       "             [ 1.6050e+00,  2.8709e-01,  7.1577e-01,  ..., -6.1914e-01,\n",
       "              -1.6414e+00, -2.9707e+00],\n",
       "             [ 1.9613e+00,  1.3047e+00,  1.2215e+00,  ..., -9.9180e-01,\n",
       "              -1.9706e+00, -2.5699e+00]],\n",
       "  \n",
       "            [[ 2.2117e-01, -2.2352e-01, -2.7293e-01,  ...,  1.2287e-01,\n",
       "               1.7066e+00,  4.3141e-02],\n",
       "             [-3.4082e-01, -2.3486e-01, -7.1640e-02,  ..., -5.6265e-01,\n",
       "              -8.6947e-01, -1.7060e-01],\n",
       "             [-6.1464e-02,  1.2267e-01, -1.9044e-01,  ...,  1.3128e-01,\n",
       "               1.2613e-01,  1.2517e-02],\n",
       "             ...,\n",
       "             [ 7.2369e-02,  5.6791e-02, -2.0905e-01,  ..., -1.4716e-01,\n",
       "               3.8101e-01,  1.8978e-02],\n",
       "             [-7.8229e-02, -1.7727e-01, -2.1226e-01,  ..., -3.4204e-01,\n",
       "               4.7143e-02, -2.3802e-01],\n",
       "             [ 9.2552e-02,  6.0483e-02, -2.2751e-01,  ..., -1.6088e-01,\n",
       "               3.3967e-01,  1.0776e-02]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 1.1288e-01, -7.3756e-02,  4.1562e-01,  ...,  1.0891e-01,\n",
       "              -2.2000e-01,  2.7641e-01],\n",
       "             [ 8.4596e-02, -3.1537e-01, -3.6905e-01,  ..., -8.2744e-03,\n",
       "               1.9836e-02, -8.6060e-04],\n",
       "             [ 2.1927e-01,  1.0642e-01,  7.3121e-04,  ...,  6.8869e-02,\n",
       "              -1.7593e-01,  2.3871e-01],\n",
       "             ...,\n",
       "             [ 5.9715e-01,  5.4864e-01,  4.9133e-01,  ..., -3.1126e-01,\n",
       "              -3.1610e-01,  3.2728e-01],\n",
       "             [-7.5342e-02, -1.2938e-01,  4.5588e-01,  ..., -4.2842e-01,\n",
       "              -1.6973e-01,  1.6444e-01],\n",
       "             [ 5.6355e-01,  5.7384e-01,  5.4991e-01,  ..., -3.3279e-01,\n",
       "              -3.2332e-01,  3.0293e-01]],\n",
       "  \n",
       "            [[ 1.7196e-01, -1.1144e-01,  1.4478e-03,  ...,  4.6665e-03,\n",
       "               1.4802e-01, -1.1413e-01],\n",
       "             [-8.7649e-01, -7.2703e-02,  4.7985e-01,  ..., -3.3779e-01,\n",
       "               3.2150e-01,  2.1270e-01],\n",
       "             [ 4.3443e-01,  1.9312e-01,  1.4849e-01,  ...,  6.1159e-02,\n",
       "              -9.9687e-02, -2.0709e-01],\n",
       "             ...,\n",
       "             [ 3.7974e-01,  1.0961e+00,  2.9356e-01,  ..., -1.1159e-01,\n",
       "              -9.1351e-01, -1.2655e-01],\n",
       "             [-9.5393e-01,  1.1897e+00,  3.8126e-01,  ..., -5.4444e-01,\n",
       "              -5.6157e-01,  2.5204e-01],\n",
       "             [ 3.9494e-01,  1.2013e+00,  2.6462e-01,  ..., -1.0423e-01,\n",
       "              -9.5901e-01, -1.1632e-01]],\n",
       "  \n",
       "            [[ 8.0207e-02, -6.9696e-03, -2.6643e-01,  ..., -2.5784e-02,\n",
       "               2.9343e-01, -1.4394e-02],\n",
       "             [ 7.5280e-01,  4.0011e-02, -6.5069e-02,  ...,  8.8561e-01,\n",
       "              -5.1950e-01,  1.6396e-01],\n",
       "             [ 1.4365e-01, -2.5928e-01, -1.1437e-01,  ..., -1.7281e-02,\n",
       "               7.2640e-02,  1.6000e-02],\n",
       "             ...,\n",
       "             [ 2.9043e-01, -2.3284e-01,  3.1093e-01,  ...,  8.2343e-02,\n",
       "               7.9434e-02, -2.0749e-01],\n",
       "             [ 6.4281e-01,  2.2744e-01,  3.1879e-01,  ...,  7.3648e-01,\n",
       "              -3.9788e-01, -1.0599e-01],\n",
       "             [ 3.0557e-01, -2.6229e-01,  3.0262e-01,  ...,  1.0792e-01,\n",
       "               5.6345e-02, -2.1503e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.9911e-01,  1.4074e-01, -7.1719e-02,  ..., -1.5074e-01,\n",
       "               3.8447e-02, -3.9748e-01],\n",
       "             [ 9.7150e-02, -4.8043e-01, -1.1247e-01,  ...,  1.7712e-01,\n",
       "               1.8198e-01,  5.7814e-01],\n",
       "             [ 1.6426e-01,  7.7734e-02,  1.6053e-01,  ..., -2.7912e-01,\n",
       "               6.3470e-02, -3.0118e-01],\n",
       "             ...,\n",
       "             [ 1.9972e-01, -3.9443e-02, -8.0196e-02,  ..., -5.4356e-01,\n",
       "               7.7398e-02,  1.7048e-01],\n",
       "             [-1.6864e-02, -4.0871e-01, -2.1529e-02,  ..., -8.4738e-02,\n",
       "               2.1150e-01,  7.0404e-01],\n",
       "             [ 1.8359e-01, -5.5178e-02, -6.8768e-02,  ..., -5.4374e-01,\n",
       "               7.6699e-02,  1.9276e-01]],\n",
       "  \n",
       "            [[ 4.5376e-01, -9.5503e-02, -2.1041e-01,  ...,  3.5804e-01,\n",
       "              -1.3702e-01, -4.7131e-01],\n",
       "             [ 6.1372e-02, -2.5000e-01,  4.7806e-01,  ...,  1.5363e-01,\n",
       "              -3.8773e-01, -5.7048e-02],\n",
       "             [ 3.0118e-01, -6.9409e-02, -2.2144e-01,  ...,  2.6856e-01,\n",
       "              -1.4035e-01, -3.5713e-01],\n",
       "             ...,\n",
       "             [ 6.7400e-02, -2.8573e-01, -8.7119e-02,  ..., -9.8326e-02,\n",
       "              -9.4396e-02, -1.4109e-01],\n",
       "             [-7.3351e-02, -1.8733e-01,  4.8566e-01,  ...,  9.9767e-02,\n",
       "              -2.3151e-01,  2.6291e-01],\n",
       "             [ 8.1810e-02, -2.6022e-01, -5.2111e-02,  ..., -1.2480e-01,\n",
       "              -7.0141e-02, -1.4968e-01]],\n",
       "  \n",
       "            [[ 5.9382e-02, -3.5986e-01, -5.5193e-02,  ...,  2.0033e-01,\n",
       "              -1.6054e-01, -4.1664e-03],\n",
       "             [-2.6004e-01,  5.2948e-01,  3.0210e-01,  ...,  1.3766e-01,\n",
       "              -6.0209e-01, -2.2138e-01],\n",
       "             [-1.4559e-02, -7.8340e-02, -5.4997e-03,  ...,  5.8090e-02,\n",
       "              -3.5198e-01, -1.0430e-01],\n",
       "             ...,\n",
       "             [-3.6104e-02, -4.7305e-01,  1.0073e-02,  ...,  5.2401e-01,\n",
       "              -2.3104e+00,  1.7670e-01],\n",
       "             [-2.4976e-01,  5.3703e-02,  9.7487e-02,  ...,  4.0168e-01,\n",
       "              -2.7093e+00,  1.2379e-01],\n",
       "             [-3.3083e-02, -5.4230e-01,  3.8988e-03,  ...,  5.4442e-01,\n",
       "              -2.3365e+00,  2.0276e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 1.9054e-01, -1.0591e-01, -4.6353e-01,  ..., -9.0642e-01,\n",
       "               3.1063e-01,  5.7120e-01],\n",
       "             [-1.1171e-01, -3.0458e-01, -2.0842e-01,  ..., -5.6840e-01,\n",
       "              -1.2728e-01, -1.4252e-02],\n",
       "             [ 2.4394e-02, -1.2025e-01, -6.0032e-01,  ..., -2.2887e-01,\n",
       "              -8.0994e-02,  3.8038e-01],\n",
       "             ...,\n",
       "             [ 3.7199e-01, -1.4595e-01, -1.6711e+00,  ...,  6.6729e-01,\n",
       "              -1.8593e-01,  1.6283e-01],\n",
       "             [ 3.0604e-01, -3.8023e-01, -1.5408e+00,  ...,  6.8192e-01,\n",
       "               2.2499e-02,  9.7974e-02],\n",
       "             [ 3.7747e-01, -1.3802e-01, -1.7531e+00,  ...,  5.8435e-01,\n",
       "              -1.8760e-01,  1.3263e-01]],\n",
       "  \n",
       "            [[ 1.4559e-01, -1.5670e-01, -4.9808e-01,  ...,  2.3234e-01,\n",
       "              -5.5468e-01, -2.4019e-01],\n",
       "             [ 2.2048e-01, -1.8740e-01,  1.5918e-01,  ...,  3.0774e-01,\n",
       "              -1.0063e+00,  2.5774e-01],\n",
       "             [ 4.2587e-02, -1.8807e-01, -1.2804e-01,  ..., -4.9654e-02,\n",
       "               8.0500e-01, -2.3177e-01],\n",
       "             ...,\n",
       "             [ 5.5299e-02, -3.4687e-03,  9.7126e-02,  ..., -1.0889e-01,\n",
       "               1.6101e+00, -1.3396e-01],\n",
       "             [ 2.1951e-01,  1.0742e-01,  3.2421e-01,  ...,  2.6101e-01,\n",
       "               6.4505e-01,  9.2614e-02],\n",
       "             [ 4.8023e-02,  1.6621e-02,  1.0357e-01,  ..., -8.9844e-02,\n",
       "               1.5801e+00, -1.3042e-01]],\n",
       "  \n",
       "            [[-3.5510e-01, -1.4094e-01, -5.2819e-01,  ...,  4.5299e-01,\n",
       "              -6.5512e-01, -7.8750e-01],\n",
       "             [ 7.9614e-01, -1.6137e-02, -1.3484e+00,  ...,  1.0741e+00,\n",
       "               1.1378e+00, -6.0276e-01],\n",
       "             [ 2.4603e-02,  1.4563e-01, -2.1560e-01,  ..., -2.8729e-01,\n",
       "               2.2414e-01, -1.7944e-01],\n",
       "             ...,\n",
       "             [-7.0532e-01, -1.3299e+00,  7.0482e-02,  ..., -1.6577e+00,\n",
       "               1.0023e+00,  2.7428e+00],\n",
       "             [-2.6476e-01, -1.5248e+00, -8.9148e-01,  ..., -7.3778e-01,\n",
       "               1.8105e+00,  2.0857e+00],\n",
       "             [-8.7015e-01, -1.4125e+00,  5.2351e-02,  ..., -1.5908e+00,\n",
       "               9.7487e-01,  2.8582e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-6.4292e-01,  3.7202e-02, -2.9920e-01,  ...,  3.4284e-01,\n",
       "               2.2034e-01,  5.4896e-01],\n",
       "             [-2.6560e-01,  4.2992e-01, -2.9731e-01,  ...,  3.0372e-03,\n",
       "              -7.4193e-02,  4.2945e-01],\n",
       "             [-3.9789e-01, -9.7989e-02, -2.5294e-01,  ...,  4.2359e-01,\n",
       "               3.9511e-02,  2.8348e-01],\n",
       "             ...,\n",
       "             [ 1.4876e+00,  2.6357e-01,  8.4809e-01,  ..., -1.0977e+00,\n",
       "              -9.3810e-02, -1.5059e+00],\n",
       "             [ 1.6310e+00,  6.9003e-01,  9.0988e-01,  ..., -1.4293e+00,\n",
       "               1.4535e-02, -1.0727e+00],\n",
       "             [ 1.5956e+00,  3.1386e-01,  9.2419e-01,  ..., -1.1977e+00,\n",
       "              -7.1769e-02, -1.5956e+00]],\n",
       "  \n",
       "            [[ 4.7137e-01, -5.3984e-01, -3.4725e-01,  ..., -4.3685e-01,\n",
       "               8.5513e-01, -2.2945e-01],\n",
       "             [ 8.7712e-01, -1.9372e-02, -8.7122e-02,  ..., -1.5379e-01,\n",
       "              -7.8581e-02, -1.0645e+00],\n",
       "             [-4.9247e-01,  3.9556e-01,  1.6823e-01,  ..., -6.1454e-01,\n",
       "               2.9115e-01, -3.8398e-01],\n",
       "             ...,\n",
       "             [ 2.7144e-01,  1.8478e+00,  3.5846e-01,  ..., -3.3485e+00,\n",
       "               3.0667e-01,  8.5043e-01],\n",
       "             [ 1.0158e+00,  1.6955e+00,  5.2189e-01,  ..., -3.1719e+00,\n",
       "              -6.8707e-02,  4.8756e-01],\n",
       "             [ 3.8996e-01,  1.8734e+00,  3.2702e-01,  ..., -3.4005e+00,\n",
       "               3.0370e-01,  8.8831e-01]],\n",
       "  \n",
       "            [[ 2.6854e-01,  5.7007e-02,  2.7600e-01,  ...,  2.7626e-01,\n",
       "              -2.6608e-01,  8.5073e-02],\n",
       "             [ 1.3142e-01,  4.8065e-01, -1.3366e+00,  ...,  1.2941e+00,\n",
       "               7.9773e-01, -7.2365e-01],\n",
       "             [-4.3432e-01,  2.4347e-02, -1.3807e-01,  ...,  6.7102e-01,\n",
       "               4.0416e-01,  1.6151e-01],\n",
       "             ...,\n",
       "             [-4.5139e+00,  2.8216e-01, -1.2896e+00,  ...,  1.0030e+00,\n",
       "              -2.3359e-01,  2.3094e+00],\n",
       "             [-4.1916e+00,  4.3797e-01, -2.4180e+00,  ...,  8.3489e-01,\n",
       "              -1.9207e-01,  1.9308e+00],\n",
       "             [-4.6746e+00,  3.8080e-01, -1.2599e+00,  ...,  1.0072e+00,\n",
       "              -3.0726e-01,  2.6478e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-2.8984e-01, -9.5053e-01, -4.2101e-01,  ...,  2.6560e-01,\n",
       "              -4.4052e-02,  2.6516e-01],\n",
       "             [-1.4658e-01,  3.2608e-01, -6.0178e-01,  ...,  4.3249e-01,\n",
       "              -2.7029e-01, -6.7224e-02],\n",
       "             [-1.7531e-01, -1.1327e-01,  7.2336e-01,  ...,  4.1022e-02,\n",
       "              -9.7736e-02,  1.3922e-02],\n",
       "             ...,\n",
       "             [-1.4306e-01,  3.6944e-02,  2.5615e+00,  ..., -3.1224e-01,\n",
       "              -1.7184e-01, -3.8713e-01],\n",
       "             [-2.9149e-01,  1.1917e-01,  2.5551e+00,  ...,  4.2880e-02,\n",
       "              -1.7940e-01, -4.1340e-01],\n",
       "             [-1.2743e-01,  3.1615e-02,  2.5359e+00,  ..., -3.2899e-01,\n",
       "              -1.4861e-01, -4.1738e-01]],\n",
       "  \n",
       "            [[ 1.6713e-01,  2.3165e-01,  3.0481e-01,  ...,  1.8356e-01,\n",
       "              -1.8613e-01,  3.7517e-01],\n",
       "             [-4.1503e-02,  2.6801e-01, -9.8477e-03,  ...,  3.7615e-01,\n",
       "              -7.9453e-02,  1.3847e-01],\n",
       "             [-9.6029e-02, -6.3760e-02,  7.4975e-02,  ...,  1.1340e-01,\n",
       "              -2.1548e-02,  7.4171e-03],\n",
       "             ...,\n",
       "             [-2.3708e-02, -7.6344e-02, -2.0201e-01,  ...,  1.3697e-01,\n",
       "               4.9620e-02,  4.1098e-02],\n",
       "             [ 1.4721e-01,  2.4555e-01, -1.9471e-01,  ...,  3.7734e-01,\n",
       "               1.9247e-02,  9.8152e-02],\n",
       "             [-1.1669e-02, -8.5673e-02, -2.1044e-01,  ...,  1.3666e-01,\n",
       "               6.7921e-02,  4.0517e-02]],\n",
       "  \n",
       "            [[ 3.9783e-01, -2.7834e-01, -2.9279e-01,  ..., -2.1629e-01,\n",
       "              -4.5279e-01,  1.4681e-01],\n",
       "             [-6.7982e-01,  4.9688e-01,  4.3375e-01,  ..., -4.7558e-01,\n",
       "              -1.1380e-01,  6.4052e-01],\n",
       "             [ 3.2737e-01, -1.1600e+00, -2.9227e-02,  ..., -1.1916e-01,\n",
       "              -2.7516e-01,  1.2488e-02],\n",
       "             ...,\n",
       "             [ 2.4549e-01, -1.2891e+00, -5.1070e-02,  ...,  1.8678e-02,\n",
       "              -1.9584e-01, -3.1466e-01],\n",
       "             [-7.5442e-01,  5.1603e-01,  3.1397e-01,  ..., -5.4113e-01,\n",
       "               4.7700e-01, -4.0424e-01],\n",
       "             [ 2.4675e-01, -1.2790e+00, -6.7882e-02,  ...,  4.0275e-02,\n",
       "              -2.0008e-01, -2.8999e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-4.5116e-02,  1.7099e-01, -6.1685e-02,  ..., -7.5759e-02,\n",
       "               1.6197e-01, -6.9073e-02],\n",
       "             [ 1.5362e-01, -3.1335e-01, -1.8791e-01,  ...,  8.0334e-01,\n",
       "               2.9655e-01,  4.4419e-01],\n",
       "             [-2.9417e-02,  1.2834e-01, -7.8101e-02,  ..., -7.4429e-01,\n",
       "               2.5614e-03,  2.5955e-02],\n",
       "             ...,\n",
       "             [ 1.4462e-01, -3.7819e-02, -1.7253e-01,  ..., -2.3780e-01,\n",
       "               9.1400e-02,  1.2483e-01],\n",
       "             [ 8.9699e-02, -1.9563e-01, -1.7455e-01,  ...,  1.9868e-01,\n",
       "               7.7451e-02,  3.6947e-01],\n",
       "             [ 1.6891e-01, -5.0240e-02, -1.2973e-01,  ..., -2.1291e-01,\n",
       "               8.8717e-02,  1.2839e-01]],\n",
       "  \n",
       "            [[ 1.2768e-01,  7.3946e-02,  7.6204e-01,  ..., -1.0533e+00,\n",
       "               8.6982e-01, -3.7628e-01],\n",
       "             [-3.4573e-01, -6.5017e-01, -8.0730e-01,  ...,  3.5180e-01,\n",
       "               8.0292e-01,  3.6796e-01],\n",
       "             [ 1.0631e-01,  4.1224e-01,  8.8968e-01,  ..., -6.1037e-01,\n",
       "               6.9314e-01,  4.0451e-02],\n",
       "             ...,\n",
       "             [ 4.7435e-02,  1.4579e-01, -1.1626e-02,  ..., -6.3331e-01,\n",
       "               4.7573e-01,  1.0259e-02],\n",
       "             [-6.3796e-01, -3.6450e-01, -7.6878e-01,  ...,  3.8392e-01,\n",
       "               9.1132e-02,  5.9173e-01],\n",
       "             [ 5.5140e-02,  1.0602e-01, -7.1952e-02,  ..., -6.3976e-01,\n",
       "               4.5596e-01, -2.5325e-03]],\n",
       "  \n",
       "            [[-8.5043e-01, -2.4333e-01, -4.2240e-01,  ...,  2.6253e-01,\n",
       "              -1.2320e-01, -8.8310e-02],\n",
       "             [ 3.1755e-02,  3.1480e-01,  8.9462e-02,  ...,  1.5860e-01,\n",
       "               2.6110e-01, -9.5993e-01],\n",
       "             [-9.3306e-02,  7.9739e-03,  4.9194e-02,  ...,  1.3967e-01,\n",
       "               1.8511e-02, -6.3664e-02],\n",
       "             ...,\n",
       "             [ 2.5211e-01, -8.7274e-02,  2.3610e-01,  ...,  2.0613e-01,\n",
       "              -4.5992e-02, -5.6887e-02],\n",
       "             [-3.2330e-01,  3.4576e-01, -2.3071e-02,  ...,  2.5203e-01,\n",
       "               1.8238e-01, -2.1287e-01],\n",
       "             [ 2.5353e-01, -9.7441e-02,  2.4608e-01,  ...,  2.1098e-01,\n",
       "              -5.0427e-02, -5.1211e-02]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 2.8918e-01,  1.2644e+00,  3.2592e-01,  ..., -8.0289e-02,\n",
       "              -1.6458e+00,  2.1959e+00],\n",
       "             [ 9.1457e-01, -5.9631e-01, -1.5752e+00,  ...,  5.4293e-02,\n",
       "              -3.7180e-01, -3.6848e-01],\n",
       "             [-8.5443e-02,  4.9928e-01,  1.9010e-03,  ..., -1.3677e-01,\n",
       "              -1.5002e-01,  3.7018e-01],\n",
       "             ...,\n",
       "             [ 1.2450e-01,  3.2955e+00, -1.6791e-01,  ..., -3.3784e+00,\n",
       "              -9.2105e-01, -2.1060e-01],\n",
       "             [ 1.6267e+00,  2.7565e+00, -1.0007e+00,  ..., -3.6766e+00,\n",
       "              -5.2225e-01, -1.2959e+00],\n",
       "             [ 1.0742e-01,  3.3835e+00, -1.5965e-01,  ..., -3.4897e+00,\n",
       "              -1.0027e+00, -2.1723e-01]],\n",
       "  \n",
       "            [[ 7.0905e-01, -1.6613e+00, -4.2812e-01,  ..., -1.6996e+00,\n",
       "               6.3429e-01,  7.1628e-02],\n",
       "             [ 1.4047e+00,  8.8538e-01, -9.8983e-02,  ..., -2.7486e-02,\n",
       "               1.8568e-01, -1.0226e+00],\n",
       "             [-1.1055e+00, -1.3358e+00,  4.9133e-01,  ...,  1.7235e-02,\n",
       "               3.8130e-01,  5.7154e-01],\n",
       "             ...,\n",
       "             [-6.1291e+00, -4.4223e+00,  1.8581e+00,  ...,  1.7797e+00,\n",
       "               3.2886e-01,  3.0593e+00],\n",
       "             [-4.1021e+00, -2.9515e+00,  1.8637e+00,  ...,  1.7410e+00,\n",
       "               3.9948e-01,  2.7388e+00],\n",
       "             [-6.1482e+00, -4.5917e+00,  1.8784e+00,  ...,  1.8406e+00,\n",
       "               3.2767e-01,  3.1094e+00]],\n",
       "  \n",
       "            [[-6.6846e-01,  1.7176e-01, -7.1971e-01,  ...,  3.0749e-01,\n",
       "              -5.1775e-01,  2.2710e+00],\n",
       "             [-7.2725e-01, -1.7903e+00, -2.5658e-01,  ...,  3.4214e-01,\n",
       "               5.3023e-01,  6.1467e-01],\n",
       "             [ 1.0476e+00, -5.9777e-02, -5.9343e-01,  ...,  4.2200e-01,\n",
       "               4.4247e-01, -2.2999e-01],\n",
       "             ...,\n",
       "             [ 2.9605e+00,  4.0557e+00,  7.3473e+00,  ...,  4.1034e+00,\n",
       "              -9.4784e-01, -1.9909e+00],\n",
       "             [ 1.8999e+00,  3.4891e+00,  8.1249e+00,  ...,  3.8013e+00,\n",
       "              -6.2681e-01, -1.5564e+00],\n",
       "             [ 2.9621e+00,  4.5328e+00,  7.4465e+00,  ...,  4.1889e+00,\n",
       "              -1.0214e+00, -2.1078e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.5119e+00, -3.5413e-01, -9.7055e-01,  ..., -2.7067e-01,\n",
       "               5.3213e-01,  8.0726e-02],\n",
       "             [-1.9718e-01,  4.4353e-01, -6.1901e-01,  ...,  2.2208e+00,\n",
       "              -9.6678e-01,  1.6465e-01],\n",
       "             [ 3.0909e-01, -9.3570e-01, -5.2592e-01,  ...,  9.2744e-02,\n",
       "               1.0825e+00,  8.7890e-01],\n",
       "             ...,\n",
       "             [ 6.6495e-02, -4.7285e-01, -4.4333e+00,  ..., -4.1169e+00,\n",
       "               4.5144e-01, -2.9160e+00],\n",
       "             [-8.5149e-01,  4.1586e-01, -4.3870e+00,  ..., -3.2010e+00,\n",
       "              -1.0747e+00, -3.2399e+00],\n",
       "             [ 9.1686e-02, -7.0371e-01, -4.4745e+00,  ..., -4.5097e+00,\n",
       "               3.0333e-01, -3.0811e+00]],\n",
       "  \n",
       "            [[ 1.2214e+00, -6.1433e-01,  7.3578e-01,  ...,  1.0523e+00,\n",
       "              -7.2978e-01,  8.5789e-02],\n",
       "             [ 1.6784e+00, -3.5812e-01,  5.7494e-01,  ...,  9.6930e-01,\n",
       "               4.6297e-01,  1.6093e+00],\n",
       "             [-3.6772e-01,  2.6426e-01,  5.1815e-01,  ...,  6.0338e-02,\n",
       "               5.1054e-01,  5.7864e-01],\n",
       "             ...,\n",
       "             [ 5.4237e-01,  2.1640e+00,  5.8478e-01,  ..., -8.5628e-01,\n",
       "               5.6154e+00, -6.1026e-01],\n",
       "             [ 2.1409e+00,  1.8291e+00,  1.6437e-01,  ..., -7.4168e-01,\n",
       "               5.6343e+00, -3.9523e-02],\n",
       "             [ 5.9729e-01,  2.2197e+00,  5.6205e-01,  ..., -8.7712e-01,\n",
       "               5.6763e+00, -7.4367e-01]],\n",
       "  \n",
       "            [[ 1.0824e+00, -8.8322e-01, -5.8215e-01,  ...,  9.8842e-02,\n",
       "              -7.9274e-01, -9.6895e-03],\n",
       "             [ 3.4742e-01, -9.5856e-01,  1.0344e-01,  ..., -5.1646e-01,\n",
       "              -3.1679e-01, -1.0867e+00],\n",
       "             [ 1.6602e+00, -5.5901e-01,  4.2316e-01,  ...,  3.5425e-02,\n",
       "               7.4290e-01, -7.3806e-01],\n",
       "             ...,\n",
       "             [ 8.7255e+00, -4.2076e+00,  9.5962e-01,  ..., -7.4126e-01,\n",
       "               4.1605e+00, -6.1550e-01],\n",
       "             [ 8.0094e+00, -4.2375e+00,  4.7914e-01,  ..., -1.0832e+00,\n",
       "               4.3223e+00, -1.0895e+00],\n",
       "             [ 8.9928e+00, -4.2666e+00,  7.4117e-01,  ..., -8.4989e-01,\n",
       "               4.5812e+00, -4.6102e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-6.6852e-02, -5.6592e-01,  4.2532e-01,  ..., -7.0091e-01,\n",
       "               3.8756e-01, -6.1264e-01],\n",
       "             [-1.9859e-01, -8.7832e-01, -1.9238e-01,  ...,  5.8986e-01,\n",
       "              -7.1951e-01,  1.2023e-01],\n",
       "             [-7.3895e-02,  1.5089e-01, -1.6994e-01,  ...,  2.5684e-01,\n",
       "               8.8383e-02, -4.5007e-01],\n",
       "             ...,\n",
       "             [-3.4077e-01, -9.1102e-02,  2.4719e-02,  ...,  3.1956e-01,\n",
       "              -1.8878e-01, -3.4889e-01],\n",
       "             [-7.8042e-01, -1.6842e-01,  2.9399e-01,  ...,  5.2441e-01,\n",
       "              -4.1002e-01,  2.9431e-01],\n",
       "             [-3.5606e-01, -8.4383e-02,  3.4986e-02,  ...,  3.3102e-01,\n",
       "              -1.8385e-01, -3.4531e-01]],\n",
       "  \n",
       "            [[ 8.2340e-01,  1.4297e-01,  2.9584e-01,  ..., -7.8567e-01,\n",
       "              -2.5546e-01, -2.4159e-01],\n",
       "             [ 1.5664e-01,  3.6113e-01, -4.1939e-01,  ..., -9.2128e-01,\n",
       "              -5.1178e-01, -5.5919e-02],\n",
       "             [ 3.5955e-01,  1.9441e-01, -1.8975e-01,  ..., -1.0018e-02,\n",
       "               3.9915e-02, -1.1263e-01],\n",
       "             ...,\n",
       "             [ 9.5230e-02,  6.3742e-02, -3.8110e-01,  ...,  1.6581e-01,\n",
       "              -2.9900e-02, -2.9628e-02],\n",
       "             [ 5.4784e-02,  1.9885e-01, -2.4651e-01,  ..., -6.0623e-01,\n",
       "              -2.0140e-01,  1.5841e-01],\n",
       "             [ 9.6416e-02,  4.8562e-02, -3.7818e-01,  ...,  1.5322e-01,\n",
       "              -2.9755e-02, -2.2827e-02]],\n",
       "  \n",
       "            [[-2.9850e-01, -7.9912e-02,  1.1419e-01,  ..., -5.4689e-01,\n",
       "               1.1660e-01,  3.3313e-01],\n",
       "             [ 5.9552e-01, -4.6070e-02, -7.1615e-01,  ...,  2.4028e-02,\n",
       "              -3.9326e-02, -2.5013e-01],\n",
       "             [-1.1187e-01,  6.3909e-03,  2.8814e-02,  ...,  5.0684e-02,\n",
       "               1.7826e-03,  1.5778e-02],\n",
       "             ...,\n",
       "             [-1.2377e-01, -1.0111e-01,  6.1354e-03,  ...,  1.1384e-01,\n",
       "               2.5782e-02, -6.1261e-02],\n",
       "             [ 6.5164e-03,  3.9157e-01, -5.8301e-01,  ..., -4.2655e-02,\n",
       "              -3.2794e-01, -2.4056e-01],\n",
       "             [-1.3216e-01, -1.1875e-01,  1.1836e-02,  ...,  1.0380e-01,\n",
       "               2.5231e-02, -6.9330e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.3784e-01,  2.3087e-01,  2.9162e-01,  ...,  4.1505e-01,\n",
       "              -4.9935e-01, -1.4532e-02],\n",
       "             [ 8.9929e-03, -4.3563e-01, -2.1139e-01,  ..., -1.4202e-01,\n",
       "               2.8848e-01,  6.4669e-02],\n",
       "             [-2.4237e-01,  5.2177e-01,  1.6602e-01,  ..., -5.7325e-03,\n",
       "              -6.3978e-01, -1.0185e-01],\n",
       "             ...,\n",
       "             [-2.0690e-02,  2.4675e-01,  1.8040e-01,  ..., -5.5178e-02,\n",
       "              -5.7811e-01,  2.8422e-02],\n",
       "             [-5.6921e-02, -2.6193e-01, -7.2066e-02,  ..., -2.2721e-01,\n",
       "               3.2617e-01, -1.0868e-01],\n",
       "             [-1.1236e-02,  2.3373e-01,  1.8179e-01,  ..., -7.4333e-02,\n",
       "              -5.5246e-01,  1.8711e-02]],\n",
       "  \n",
       "            [[-2.4285e-01,  5.7736e-01,  3.2711e-01,  ...,  2.5097e-01,\n",
       "              -3.5418e-01,  2.3419e-01],\n",
       "             [-6.3471e-02, -1.6851e-01, -8.1014e-01,  ..., -5.7526e-01,\n",
       "               1.9905e-01,  5.3377e-01],\n",
       "             [ 1.4993e-01,  2.3217e-01,  9.3562e-02,  ..., -1.9085e-01,\n",
       "               1.7783e-01, -4.2146e-03],\n",
       "             ...,\n",
       "             [-1.1577e-01,  3.2358e-01,  3.3604e-01,  ..., -1.0065e-01,\n",
       "               2.0470e-01, -8.4269e-02],\n",
       "             [-2.5959e-01, -1.2849e-01, -6.1569e-01,  ...,  3.7953e-01,\n",
       "               2.2843e-01,  6.9070e-01],\n",
       "             [-1.2440e-01,  3.3724e-01,  3.2618e-01,  ..., -8.2896e-02,\n",
       "               2.1031e-01, -8.6497e-02]],\n",
       "  \n",
       "            [[ 6.8262e-01,  5.9938e-02, -7.9621e-01,  ...,  8.9056e-02,\n",
       "               9.6512e-02,  3.0698e-01],\n",
       "             [ 4.0905e-01,  3.1792e-01, -2.6229e-01,  ..., -3.6171e-01,\n",
       "              -2.7358e-03,  1.5832e-01],\n",
       "             [ 4.5659e-01,  1.7325e-01, -6.3922e-01,  ...,  4.3049e-01,\n",
       "               6.2543e-01,  1.9892e-01],\n",
       "             ...,\n",
       "             [ 1.8475e-01,  1.4191e-01, -4.6684e-01,  ...,  2.4161e-01,\n",
       "               4.6827e-01,  3.2062e-01],\n",
       "             [ 4.0275e-01,  2.5731e-01,  2.6473e-01,  ...,  8.8358e-03,\n",
       "               1.0509e-01,  2.5707e-01],\n",
       "             [ 1.6854e-01,  1.4154e-01, -4.5306e-01,  ...,  2.3733e-01,\n",
       "               4.6155e-01,  3.3401e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-6.3325e-01,  3.9469e-01,  2.9316e-01,  ...,  6.4813e-02,\n",
       "              -9.1779e-01, -4.2052e-02],\n",
       "             [-4.5004e-02, -1.8257e+00,  1.7290e+00,  ...,  9.2967e-01,\n",
       "               1.7760e+00,  1.1929e+00],\n",
       "             [ 5.7945e-01, -2.4468e+00,  1.0202e+00,  ..., -4.1458e-01,\n",
       "               2.5667e-01,  6.5768e-01],\n",
       "             ...,\n",
       "             [ 5.7077e-01, -1.2703e+00,  1.2887e+00,  ...,  4.6652e-01,\n",
       "              -3.8547e+00,  8.7635e-01],\n",
       "             [-5.8669e-02, -9.1068e-01,  1.7302e+00,  ...,  1.4910e+00,\n",
       "              -3.3909e+00,  2.5928e-01],\n",
       "             [ 5.5127e-01, -1.1525e+00,  1.2543e+00,  ...,  5.1223e-01,\n",
       "              -4.0491e+00,  8.4678e-01]],\n",
       "  \n",
       "            [[-4.3842e-01, -5.9148e-01, -2.7844e-01,  ..., -2.2067e-01,\n",
       "              -4.7638e-01,  5.8577e-01],\n",
       "             [ 1.2357e+00, -7.4975e-01,  2.6301e-02,  ..., -5.0183e-01,\n",
       "               9.2459e-02, -6.7021e-01],\n",
       "             [-5.9259e-01,  1.4132e-02,  8.0689e-01,  ..., -7.2636e-02,\n",
       "              -2.7348e-01, -3.1937e-01],\n",
       "             ...,\n",
       "             [ 1.0297e+00,  6.8606e-01, -2.3024e-01,  ...,  5.9897e-01,\n",
       "               2.2278e-01,  2.2106e+00],\n",
       "             [ 2.6319e+00,  2.7007e-01, -5.6663e-01,  ...,  6.9342e-01,\n",
       "               2.1320e-01,  2.3100e+00],\n",
       "             [ 9.8669e-01,  6.4093e-01, -1.1399e-01,  ...,  6.5037e-01,\n",
       "               7.5651e-02,  2.3170e+00]],\n",
       "  \n",
       "            [[ 1.6223e-01,  5.1118e-01, -2.0861e-01,  ...,  9.5185e-01,\n",
       "              -3.0350e-01, -1.0133e-02],\n",
       "             [-5.3412e-01,  1.4999e+00, -9.6452e-01,  ...,  7.6231e-01,\n",
       "              -1.3834e-01, -2.8390e+00],\n",
       "             [ 1.0632e+00,  5.6889e-01, -9.1059e-01,  ..., -1.6277e+00,\n",
       "              -1.8745e+00,  7.3168e-02],\n",
       "             ...,\n",
       "             [-4.7926e+00,  2.8693e-01, -1.5259e+00,  ...,  7.9731e-02,\n",
       "              -4.7079e+00, -9.4559e-01],\n",
       "             [-5.5974e+00,  7.4334e-01, -1.5003e+00,  ...,  1.1226e+00,\n",
       "              -3.0924e+00, -3.3718e+00],\n",
       "             [-4.9517e+00,  2.0994e-01, -1.2985e+00,  ...,  1.7235e-01,\n",
       "              -4.7512e+00, -9.2952e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-1.9460e+00,  1.6198e+00, -2.2637e-01,  ...,  8.5561e-01,\n",
       "               1.5478e+00, -2.0620e+00],\n",
       "             [ 1.3899e-01,  1.6465e+00,  2.1241e+00,  ...,  2.0562e+00,\n",
       "               1.2509e+00, -8.2310e-01],\n",
       "             [-1.3150e+00,  4.2455e-01, -6.8421e-01,  ..., -4.7261e-01,\n",
       "               1.4553e-01, -9.8936e-01],\n",
       "             ...,\n",
       "             [-1.5762e+01,  1.1767e+01, -3.2362e+00,  ...,  2.4840e+00,\n",
       "               1.2166e+01, -8.4330e+00],\n",
       "             [-1.5866e+01,  1.2999e+01, -8.5682e-01,  ...,  5.1198e+00,\n",
       "               1.4119e+01, -9.2889e+00],\n",
       "             [-1.6218e+01,  1.1830e+01, -3.4768e+00,  ...,  2.2817e+00,\n",
       "               1.2677e+01, -8.6229e+00]],\n",
       "  \n",
       "            [[ 4.0023e-01, -4.7454e-01,  3.7902e-02,  ...,  3.6929e-01,\n",
       "               2.0590e-01, -2.5502e-01],\n",
       "             [ 1.7048e+00,  1.2144e-01,  8.7871e-01,  ..., -6.6316e-01,\n",
       "               5.0351e-01,  2.3513e+00],\n",
       "             [ 4.2953e-01,  2.3597e-01,  8.4505e-01,  ...,  5.1996e-01,\n",
       "              -3.1822e-01, -4.3976e-01],\n",
       "             ...,\n",
       "             [-1.2560e+00, -2.3782e+00, -1.1954e+00,  ..., -1.1961e+00,\n",
       "              -2.1249e+00, -2.2535e+00],\n",
       "             [-2.7344e-01, -2.2317e+00, -9.4684e-01,  ..., -1.3955e+00,\n",
       "              -1.7475e+00, -5.8950e-01],\n",
       "             [-1.4261e+00, -2.4795e+00, -1.1940e+00,  ..., -1.2528e+00,\n",
       "              -2.3218e+00, -2.2929e+00]],\n",
       "  \n",
       "            [[ 1.6767e-01, -6.3861e-02,  2.8639e-01,  ...,  9.1864e-02,\n",
       "              -3.9977e-01, -1.8771e-01],\n",
       "             [-1.9732e+00, -7.8003e-01,  6.9140e-01,  ..., -2.2954e+00,\n",
       "              -2.7288e-01, -9.5027e-01],\n",
       "             [ 3.1338e-02,  4.9833e-02,  3.7996e-01,  ..., -2.0243e-01,\n",
       "              -3.0730e-01, -3.8437e-01],\n",
       "             ...,\n",
       "             [-5.2304e-01,  9.8854e-01, -6.6637e-01,  ..., -7.2729e-01,\n",
       "               6.1644e-01,  8.7698e-01],\n",
       "             [-1.2670e+00,  8.9094e-01, -2.3781e-01,  ..., -1.9714e+00,\n",
       "               1.3418e+00,  4.7336e-01],\n",
       "             [-5.4655e-01,  1.0230e+00, -7.3006e-01,  ..., -7.4941e-01,\n",
       "               6.7928e-01,  9.2925e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-7.9187e-02, -3.1501e-02,  4.9325e-02,  ...,  1.0304e-01,\n",
       "              -9.0569e-02,  2.0801e-02],\n",
       "             [-7.3204e-01, -4.4449e-02,  2.3747e-01,  ..., -4.6206e-01,\n",
       "               5.8424e-01,  1.0133e+00],\n",
       "             [-7.6069e-02,  4.8222e-01,  3.1537e-03,  ...,  5.0171e-01,\n",
       "              -7.3100e-02, -2.9966e-01],\n",
       "             ...,\n",
       "             [ 2.2420e-01,  3.6184e-01,  5.1837e-02,  ...,  4.5890e-01,\n",
       "               2.0513e-01, -4.6064e-01],\n",
       "             [ 2.4331e-01,  6.1725e-01,  1.9288e-01,  ..., -3.0622e-03,\n",
       "               9.2698e-01,  2.3394e-01],\n",
       "             [ 2.3159e-01,  3.5013e-01,  4.3568e-02,  ...,  4.5835e-01,\n",
       "               2.0230e-01, -4.5020e-01]],\n",
       "  \n",
       "            [[ 1.8976e-02,  1.2167e-02, -7.1562e-02,  ...,  3.4018e-01,\n",
       "              -2.2299e-02, -1.6386e-01],\n",
       "             [-5.5225e-01, -4.4769e-01, -3.1093e-01,  ..., -1.4474e-01,\n",
       "              -4.7709e-01,  1.2398e+00],\n",
       "             [ 4.8762e-01,  5.1955e-01,  2.5512e-01,  ...,  6.6379e-01,\n",
       "              -7.6514e-01, -2.7496e-01],\n",
       "             ...,\n",
       "             [ 2.5473e-01,  3.4666e-01,  1.3168e-01,  ...,  1.4664e-01,\n",
       "              -3.8548e-01, -4.6793e-01],\n",
       "             [-6.2654e-01, -1.7804e-01,  2.0550e-01,  ..., -3.5974e-02,\n",
       "               5.2182e-01,  4.5127e-01],\n",
       "             [ 2.5811e-01,  3.3385e-01,  1.2555e-01,  ...,  1.4417e-01,\n",
       "              -3.1794e-01, -4.6807e-01]],\n",
       "  \n",
       "            [[ 4.6790e-02, -1.3087e-01,  4.9169e-02,  ..., -3.1279e-02,\n",
       "              -1.6421e-01,  1.5845e-01],\n",
       "             [ 1.7891e-01,  2.9642e-02, -1.0641e+00,  ..., -4.1416e-01,\n",
       "               3.8544e-01, -7.6640e-01],\n",
       "             [ 8.1145e-02,  9.4949e-02,  7.5022e-01,  ..., -1.5809e-01,\n",
       "               8.5609e-02,  5.2451e-01],\n",
       "             ...,\n",
       "             [ 8.9220e-03,  6.5013e-02,  5.3820e-01,  ..., -5.9273e-02,\n",
       "               1.7857e-01,  1.6612e-01],\n",
       "             [ 1.7468e-01,  1.7579e-01, -6.5151e-01,  ..., -2.4798e-01,\n",
       "               2.5008e-01, -6.1831e-01],\n",
       "             [ 5.7972e-03,  4.6068e-02,  5.5280e-01,  ..., -8.8765e-02,\n",
       "               1.8200e-01,  1.8204e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-1.0552e-01, -3.1239e-02, -2.2913e-02,  ...,  9.4098e-03,\n",
       "               3.1498e-02,  2.6320e-01],\n",
       "             [ 8.1381e-02, -4.2269e-01,  5.0452e-01,  ..., -4.2982e-02,\n",
       "               2.9911e-01,  5.1423e-01],\n",
       "             [ 3.7543e-02, -6.1714e-01, -2.3438e-01,  ...,  2.3281e-01,\n",
       "              -4.8434e-03,  9.7607e-02],\n",
       "             ...,\n",
       "             [-2.1548e-01, -3.6638e-01,  9.0460e-02,  ...,  3.9202e-01,\n",
       "               1.8590e-02,  1.8929e-01],\n",
       "             [-2.3925e-01, -3.3439e-01,  4.0127e-01,  ..., -2.1146e-01,\n",
       "               1.0109e-01,  3.9606e-01],\n",
       "             [-2.1557e-01, -3.4411e-01,  9.0680e-02,  ...,  3.9539e-01,\n",
       "               2.8538e-02,  1.6579e-01]],\n",
       "  \n",
       "            [[ 1.7947e-01, -3.5917e-03, -1.7183e-01,  ...,  4.9090e-02,\n",
       "               2.9142e-02,  3.9662e-02],\n",
       "             [ 5.3916e-01, -1.2580e+00, -5.2479e-02,  ...,  1.9864e-01,\n",
       "               6.5587e-02,  6.7887e-01],\n",
       "             [ 5.7233e-02,  3.9852e-01, -1.0671e-01,  ...,  5.6947e-02,\n",
       "              -9.3032e-03, -4.4979e-01],\n",
       "             ...,\n",
       "             [ 3.6108e-02,  4.5078e-01,  1.0899e-01,  ..., -1.2599e-01,\n",
       "               1.8311e-01, -4.7433e-01],\n",
       "             [ 7.3815e-01, -5.6228e-01,  1.5581e-01,  ...,  2.6655e-01,\n",
       "               4.7017e-01,  1.4620e-01],\n",
       "             [ 4.3145e-02,  4.5767e-01,  1.1392e-01,  ..., -1.2423e-01,\n",
       "               2.0611e-01, -4.9023e-01]],\n",
       "  \n",
       "            [[ 2.1530e-01,  1.5696e-01, -9.2665e-02,  ...,  2.6955e-01,\n",
       "              -8.6116e-02,  2.6278e-02],\n",
       "             [ 2.9228e-01,  4.1495e-01,  1.5488e+00,  ...,  4.1444e-01,\n",
       "              -2.3215e-01,  6.1795e-02],\n",
       "             [ 1.4042e+00, -2.7485e-01, -2.1201e-01,  ...,  1.1163e-02,\n",
       "               1.8750e-01, -1.1495e-02],\n",
       "             ...,\n",
       "             [ 2.9736e-01, -8.4980e-01,  3.9279e-01,  ..., -4.5134e-01,\n",
       "              -1.9227e-01, -4.2966e-01],\n",
       "             [-4.6369e-01, -1.2587e+00,  1.9488e+00,  ..., -2.1268e-01,\n",
       "              -3.7608e-01,  1.9360e-01],\n",
       "             [ 2.3984e-01, -8.9690e-01,  4.0535e-01,  ..., -4.4959e-01,\n",
       "              -2.0263e-01, -4.3026e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-1.2052e-01,  3.1460e-01, -1.2498e-01,  ..., -5.7691e-01,\n",
       "              -1.0106e-01,  5.1160e-01],\n",
       "             [ 8.8265e-01,  4.4766e-01,  3.4790e-02,  ..., -2.4390e-01,\n",
       "              -2.1853e+00, -2.0706e+00],\n",
       "             [-2.3373e-01,  4.9361e-02, -9.1293e-02,  ..., -1.1603e+00,\n",
       "              -1.3555e-01,  1.3860e-01],\n",
       "             ...,\n",
       "             [ 8.0619e-01,  2.9996e-01,  3.9072e-01,  ..., -2.3891e-01,\n",
       "              -1.0860e+00,  3.4973e+00],\n",
       "             [ 1.8534e+00,  2.5182e-01,  6.7582e-01,  ...,  1.1078e-01,\n",
       "              -2.3131e+00,  1.5638e+00],\n",
       "             [ 8.5917e-01,  2.9573e-01,  4.0553e-01,  ..., -2.3097e-01,\n",
       "              -1.0892e+00,  3.5896e+00]],\n",
       "  \n",
       "            [[-8.9517e-02,  4.9057e-01,  1.4619e-01,  ...,  3.7974e-01,\n",
       "              -2.5649e-02, -2.7030e-02],\n",
       "             [ 1.0619e+00, -7.4962e-01, -2.7398e-01,  ..., -1.1415e+00,\n",
       "              -4.3345e-02, -1.0186e+00],\n",
       "             [ 7.8154e-01,  1.1212e-01, -1.9386e-01,  ..., -4.0771e-03,\n",
       "               1.0709e+00, -5.0827e-01],\n",
       "             ...,\n",
       "             [-1.1508e+00,  6.3215e+00,  1.3684e+00,  ...,  4.3259e+00,\n",
       "               7.8122e-01, -2.0161e+00],\n",
       "             [-8.8067e-01,  6.4905e+00,  1.1133e+00,  ...,  4.0546e+00,\n",
       "               6.1298e-01, -2.8505e+00],\n",
       "             [-9.1681e-01,  6.6029e+00,  1.2692e+00,  ...,  4.5555e+00,\n",
       "               7.2940e-01, -2.0970e+00]],\n",
       "  \n",
       "            [[-3.3719e-02,  1.2957e-01, -5.7693e-01,  ...,  1.2116e-01,\n",
       "              -2.3101e-02,  8.2890e-01],\n",
       "             [-2.5057e-01,  1.2633e+00,  1.0270e+00,  ...,  5.1710e-01,\n",
       "              -1.2780e+00, -7.3460e-01],\n",
       "             [-8.3250e-01, -1.0980e-01,  1.1351e+00,  ...,  6.6763e-01,\n",
       "              -1.2242e+00, -3.3540e-01],\n",
       "             ...,\n",
       "             [ 4.2551e-01, -1.4327e+00,  1.1612e+00,  ..., -2.3588e+00,\n",
       "              -1.0415e+00,  7.6158e+00],\n",
       "             [ 1.4797e+00, -8.2905e-02,  1.1546e+00,  ..., -2.7280e+00,\n",
       "              -1.4252e+00,  8.0833e+00],\n",
       "             [ 6.5094e-01, -1.4681e+00,  1.1309e+00,  ..., -2.5426e+00,\n",
       "              -9.6533e-01,  7.6542e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-2.1414e-01,  4.7462e-02, -1.5474e-01,  ..., -8.3162e-02,\n",
       "              -1.1102e+00,  1.9495e-01],\n",
       "             [ 4.5022e-01, -2.3045e+00,  2.3217e+00,  ..., -3.9795e-01,\n",
       "               1.3822e+00,  1.4771e-01],\n",
       "             [ 5.5677e-01, -1.2439e+00,  1.7905e+00,  ..., -3.9029e-02,\n",
       "               1.0830e+00, -1.2510e+00],\n",
       "             ...,\n",
       "             [ 1.3848e+00, -3.9311e+00,  9.7458e-01,  ..., -4.3478e-01,\n",
       "              -4.0477e+00, -1.6007e-01],\n",
       "             [ 1.2519e+00, -4.1817e+00,  2.3294e+00,  ..., -6.6730e-01,\n",
       "              -3.9034e+00,  6.9753e-01],\n",
       "             [ 1.4027e+00, -4.0292e+00,  8.6477e-01,  ..., -4.2729e-01,\n",
       "              -4.1051e+00, -1.1145e-01]],\n",
       "  \n",
       "            [[ 2.4719e-01, -7.1770e-03,  1.7221e-02,  ..., -2.0889e-01,\n",
       "               1.0311e-02, -1.1938e-01],\n",
       "             [ 2.6888e+00,  4.0968e-01,  4.9493e-01,  ...,  5.7549e-01,\n",
       "               1.4951e+00,  2.6204e-01],\n",
       "             [ 7.0993e-01, -8.1806e-01,  8.7712e-01,  ...,  2.9560e-01,\n",
       "               7.3689e-01, -5.6554e-01],\n",
       "             ...,\n",
       "             [ 1.7369e+00, -1.6013e+00,  1.2524e+00,  ..., -7.3715e-02,\n",
       "               7.1026e-01, -1.0380e+00],\n",
       "             [ 2.7488e+00, -1.1873e+00,  1.3597e+00,  ..., -3.1033e-01,\n",
       "               2.1820e+00, -3.4525e-01],\n",
       "             [ 1.7638e+00, -1.6432e+00,  1.2377e+00,  ..., -1.4097e-01,\n",
       "               7.4573e-01, -1.0845e+00]],\n",
       "  \n",
       "            [[-8.3548e-01,  4.1693e-02, -2.6349e-01,  ...,  2.7683e-01,\n",
       "              -2.3358e-01, -4.7644e-01],\n",
       "             [ 1.1163e+00, -1.9889e+00,  4.0046e-02,  ..., -8.8493e-01,\n",
       "              -1.9877e+00,  1.6819e+00],\n",
       "             [ 4.9914e-01, -3.2179e-02, -2.8286e-01,  ..., -9.6201e-01,\n",
       "              -9.1476e-02, -4.0798e-01],\n",
       "             ...,\n",
       "             [-3.2654e+00, -1.3647e+00, -1.0656e+00,  ..., -1.4477e-01,\n",
       "               1.4681e+00,  5.2390e-01],\n",
       "             [-2.3891e+00, -2.5547e+00, -6.8467e-01,  ..., -6.8380e-01,\n",
       "              -3.6610e-01,  8.2402e-01],\n",
       "             [-3.4756e+00, -1.3652e+00, -1.1184e+00,  ..., -8.8397e-02,\n",
       "               1.6425e+00,  5.7475e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-1.0710e-01, -3.8192e-02, -5.6153e-02,  ...,  7.6919e-02,\n",
       "              -2.3796e-02,  1.4593e-02],\n",
       "             [ 2.3329e-01,  1.0652e-01,  8.2068e-01,  ...,  2.8625e-01,\n",
       "               7.3824e-01,  5.1752e-01],\n",
       "             [ 4.6713e-01,  9.5628e-02,  1.8415e-01,  ...,  4.3346e-01,\n",
       "               1.5455e-01, -1.9398e-01],\n",
       "             ...,\n",
       "             [ 1.4668e+00,  2.4883e-01,  7.0544e-01,  ...,  1.0487e-01,\n",
       "              -2.1463e-01, -6.4136e-01],\n",
       "             [ 1.7038e+00,  4.3160e-01,  6.9455e-01,  ...,  7.3743e-02,\n",
       "              -8.7207e-02, -2.9821e-01],\n",
       "             [ 1.4859e+00,  2.4740e-01,  7.3056e-01,  ...,  1.0831e-01,\n",
       "              -2.1551e-01, -6.3957e-01]],\n",
       "  \n",
       "            [[-8.0627e-02, -4.6858e-02,  1.3376e-03,  ..., -2.3709e-03,\n",
       "              -2.2702e-04, -5.5303e-02],\n",
       "             [-1.3234e+00, -5.9427e-01, -5.8242e-01,  ...,  5.1768e-01,\n",
       "              -3.0003e-01,  2.1547e-01],\n",
       "             [-3.8883e-01,  1.3008e-01,  4.3000e-01,  ...,  2.3015e-01,\n",
       "               4.6659e-01, -3.7167e-01],\n",
       "             ...,\n",
       "             [ 3.2133e-02,  4.7576e-01,  5.4518e-01,  ...,  1.9310e-01,\n",
       "               6.1286e-02,  3.8637e-01],\n",
       "             [-6.2409e-01, -3.2735e-01, -2.3400e-01,  ...,  7.0215e-01,\n",
       "               5.8598e-03,  5.2216e-01],\n",
       "             [ 5.6396e-02,  4.5940e-01,  5.5989e-01,  ...,  2.0900e-01,\n",
       "               5.2629e-02,  3.7191e-01]],\n",
       "  \n",
       "            [[ 4.4798e-02, -1.3215e-01,  8.4773e-02,  ..., -1.5579e-01,\n",
       "               5.5140e-02,  1.4401e-01],\n",
       "             [-9.7926e-02,  8.6342e-01, -3.8702e-01,  ...,  1.8498e-01,\n",
       "               1.6992e-01, -5.4536e-02],\n",
       "             [-2.1782e-01, -1.8004e-01,  2.6145e-01,  ..., -4.0524e-01,\n",
       "              -4.4305e-01,  8.6285e-01],\n",
       "             ...,\n",
       "             [-2.4447e-02, -2.0777e-01,  4.9620e-02,  ..., -3.0636e-01,\n",
       "              -2.5961e-01,  5.1677e-01],\n",
       "             [ 3.2635e-02,  4.6856e-01, -1.9789e-01,  ...,  4.4220e-01,\n",
       "               1.8311e-01,  1.6280e-02],\n",
       "             [-4.9732e-02, -1.9359e-01,  3.8774e-02,  ..., -2.9885e-01,\n",
       "              -2.4444e-01,  5.0236e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.0042e-02, -1.0485e-02,  1.6918e-01,  ..., -7.4782e-02,\n",
       "              -7.1054e-02, -5.4051e-02],\n",
       "             [-2.7242e-01,  6.4058e-02,  1.0032e+00,  ..., -2.6584e-02,\n",
       "              -2.5675e-01,  4.9826e-01],\n",
       "             [-1.8478e-02,  2.5151e-01, -6.4204e-01,  ..., -3.9094e-01,\n",
       "               7.0915e-01,  3.8277e-01],\n",
       "             ...,\n",
       "             [-4.5560e-01, -2.1959e-01,  5.4481e-02,  ..., -6.3666e-01,\n",
       "              -5.4701e-01, -7.2765e-01],\n",
       "             [-6.8903e-01,  5.7498e-02,  2.0126e-01,  ..., -5.4965e-01,\n",
       "              -6.2776e-01, -6.8481e-01],\n",
       "             [-4.6340e-01, -2.1821e-01,  4.2120e-02,  ..., -6.3281e-01,\n",
       "              -5.6399e-01, -7.1893e-01]],\n",
       "  \n",
       "            [[ 3.1571e-02, -2.4925e-02,  1.2156e-01,  ..., -9.6619e-02,\n",
       "              -1.8544e-01, -9.2648e-02],\n",
       "             [ 4.5478e-01, -1.1751e-01, -3.2476e-01,  ..., -1.6675e-01,\n",
       "              -4.4801e-01, -9.4666e-01],\n",
       "             [ 6.9083e-01, -2.0501e-01,  8.8175e-01,  ...,  5.0497e-01,\n",
       "              -1.7642e-01, -3.1971e-01],\n",
       "             ...,\n",
       "             [-3.1124e-01, -2.5737e-01,  1.0384e-01,  ...,  5.7330e-01,\n",
       "              -7.7170e-02,  1.1473e-01],\n",
       "             [-3.1404e-01, -4.7380e-01, -3.4186e-02,  ...,  2.3507e-01,\n",
       "              -1.0089e+00,  1.4769e-01],\n",
       "             [-3.0170e-01, -2.7659e-01,  8.7204e-02,  ...,  5.7619e-01,\n",
       "              -4.2394e-02,  1.6308e-01]],\n",
       "  \n",
       "            [[-2.0394e-01,  4.8224e-03, -1.5186e-01,  ...,  1.0724e-02,\n",
       "              -2.8669e-02, -2.4235e-02],\n",
       "             [ 3.7194e-01, -3.2409e-01,  1.2554e-01,  ...,  1.2304e-01,\n",
       "               5.2175e-01,  7.7896e-03],\n",
       "             [ 1.6817e-01, -3.2303e-01, -8.3657e-01,  ...,  3.4013e-01,\n",
       "              -5.8837e-01,  7.5414e-01],\n",
       "             ...,\n",
       "             [ 3.2087e-01, -2.2216e-01, -5.2580e-01,  ..., -2.4853e-01,\n",
       "              -4.3614e-02,  4.2875e-01],\n",
       "             [ 9.1567e-01, -6.4019e-01, -9.8221e-02,  ...,  2.6370e-02,\n",
       "               3.2114e-01, -8.2904e-02],\n",
       "             [ 3.1945e-01, -2.0866e-01, -5.1928e-01,  ..., -2.5009e-01,\n",
       "              -3.9465e-02,  4.0865e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 0.1522, -0.1238,  0.0806,  ...,  0.0079,  0.1238,  0.1840],\n",
       "             [-0.4660, -1.2609, -0.1456,  ...,  1.9608, -0.7999,  0.7594],\n",
       "             [-0.5371, -2.2926, -0.3693,  ..., -0.1720, -0.1752,  1.0457],\n",
       "             ...,\n",
       "             [-1.4090, -2.2825, -0.7236,  ...,  1.3987, -0.7897,  0.5247],\n",
       "             [-1.3876, -2.0831, -0.0911,  ...,  2.9715, -1.4077, -1.0845],\n",
       "             [-1.3838, -2.2687, -0.6756,  ...,  1.4140, -0.7528,  0.4453]],\n",
       "  \n",
       "            [[ 0.1666, -0.3944, -0.1043,  ...,  1.1979, -0.4242,  0.2155],\n",
       "             [ 0.4579, -1.2979,  0.1309,  ...,  1.7841,  0.7092, -0.4384],\n",
       "             [-0.7685,  0.1752, -1.3169,  ...,  3.5756, -0.7979,  2.9075],\n",
       "             ...,\n",
       "             [-0.4242, -0.9696,  0.0135,  ...,  1.7306,  0.3568,  0.5882],\n",
       "             [-0.2232, -1.3041, -0.9986,  ...,  0.6639,  1.0196, -0.1685],\n",
       "             [-0.4844, -0.9683,  0.0211,  ...,  1.6929,  0.3137,  0.6046]],\n",
       "  \n",
       "            [[ 0.2471, -0.2090,  0.0849,  ..., -0.0971, -0.2162, -0.0145],\n",
       "             [-2.1001, -0.3690,  2.2352,  ..., -0.5417, -1.1098,  2.3830],\n",
       "             [-0.0043, -0.6452, -0.4240,  ...,  0.7181, -0.9739,  1.4241],\n",
       "             ...,\n",
       "             [ 0.6807, -0.3870, -0.6354,  ...,  0.0532, -2.2537,  1.1704],\n",
       "             [-1.1359,  0.2690,  1.1584,  ...,  0.2841, -2.4995,  2.7685],\n",
       "             [ 0.6803, -0.3590, -0.6710,  ...,  0.0700, -2.2370,  1.1229]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-0.1001, -0.1053, -0.0964,  ...,  0.1972,  0.0656,  0.3014],\n",
       "             [-1.4928, -0.5012, -0.5930,  ...,  0.4150,  0.2703,  0.7185],\n",
       "             [-0.1107,  1.0035,  0.7458,  ...,  0.6817,  1.0631,  0.7142],\n",
       "             ...,\n",
       "             [-0.9490,  1.2392,  0.1111,  ...,  0.0168,  0.1435,  0.7578],\n",
       "             [-1.4154, -0.1837, -0.7714,  ...,  0.0888, -0.2381,  0.2331],\n",
       "             [-0.9152,  1.2111,  0.1225,  ..., -0.0385,  0.0925,  0.7444]],\n",
       "  \n",
       "            [[-0.1148,  0.5221,  0.2127,  ...,  0.2002, -0.1479,  0.3012],\n",
       "             [-2.0446, -3.5580, -2.9182,  ...,  0.2825,  1.2641, -1.4347],\n",
       "             [-1.5641, -2.9386, -3.0378,  ..., -0.1576, -0.0808, -0.8935],\n",
       "             ...,\n",
       "             [-0.7796, -0.8852,  3.4144,  ..., -0.8310, -0.4667, -2.1323],\n",
       "             [-0.7379, -1.6652,  3.6758,  ..., -0.9510, -0.0729, -3.1649],\n",
       "             [-0.7528, -0.5006,  3.4325,  ..., -0.8210, -0.5759, -2.1394]],\n",
       "  \n",
       "            [[ 0.1148, -0.4909, -0.0899,  ..., -0.3996,  0.1366, -0.4292],\n",
       "             [ 1.0752, -0.3143,  0.7523,  ...,  1.0655, -0.6817,  0.4163],\n",
       "             [-1.6916, -1.1988,  2.1872,  ...,  1.7348, -0.7664,  0.1665],\n",
       "             ...,\n",
       "             [ 0.0400, -3.0988,  2.4965,  ...,  0.6559, -1.1022, -3.4828],\n",
       "             [ 1.1552, -3.5060,  2.3746,  ...,  1.0693, -0.8055, -3.3478],\n",
       "             [-0.0836, -3.1210,  2.5807,  ...,  0.6501, -1.0899, -3.5258]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-0.0204,  0.0455, -0.1136,  ..., -0.0685, -0.0491, -0.0063],\n",
       "             [-1.5833, -0.1715,  0.8137,  ...,  1.5860, -0.4219,  0.2143],\n",
       "             [-0.0774,  1.2746,  0.3464,  ...,  0.2322, -0.3886, -0.5177],\n",
       "             ...,\n",
       "             [ 1.2403,  0.4966, -0.9015,  ...,  1.0825,  0.0485, -0.4068],\n",
       "             [-0.3811, -0.0931, -0.0660,  ...,  0.8470, -1.0236,  0.4968],\n",
       "             [ 1.2177,  0.4691, -0.8821,  ...,  1.0692,  0.0138, -0.3618]],\n",
       "  \n",
       "            [[ 0.1180,  0.1181,  0.1384,  ..., -0.0786,  0.0879, -0.0900],\n",
       "             [ 0.4230, -0.0881, -1.2667,  ...,  0.6964,  0.2801,  0.9957],\n",
       "             [-0.1096, -0.4304, -0.8970,  ...,  1.3079, -0.0702, -0.0203],\n",
       "             ...,\n",
       "             [ 1.0570,  0.3197,  0.2069,  ..., -3.3632,  0.2860, -0.2088],\n",
       "             [ 1.2921,  1.0605, -0.2923,  ..., -3.7712,  0.5944,  0.5460],\n",
       "             [ 1.0448,  0.3312,  0.2002,  ..., -3.4037,  0.2634, -0.1713]],\n",
       "  \n",
       "            [[ 0.1363,  0.0990,  0.2185,  ..., -0.0363,  0.0923,  0.1448],\n",
       "             [ 0.0458, -0.7376,  0.3980,  ...,  0.3582,  0.1635, -0.3253],\n",
       "             [-0.0984, -0.0613, -0.0065,  ...,  0.3470, -0.0763,  0.3218],\n",
       "             ...,\n",
       "             [-0.2028,  0.2188,  0.0738,  ..., -0.1002,  0.6261,  0.2141],\n",
       "             [-0.5296, -0.3441,  0.4984,  ...,  0.1910,  0.8891, -0.0182],\n",
       "             [-0.2107,  0.1922,  0.0421,  ..., -0.1268,  0.6032,  0.2125]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 0.0251,  0.0863,  0.1066,  ...,  0.0969,  0.0687, -0.0166],\n",
       "             [ 0.7474,  0.6776,  0.0560,  ...,  0.1618,  0.1376,  1.3513],\n",
       "             [ 0.0627,  1.1009,  0.7630,  ..., -0.1159,  0.5759,  0.0866],\n",
       "             ...,\n",
       "             [-0.2568, -0.0754, -0.1326,  ..., -0.9518,  0.5726, -0.1901],\n",
       "             [-0.3014, -0.4887, -1.5597,  ..., -0.4300,  0.0628,  1.3290],\n",
       "             [-0.2783, -0.0800, -0.1557,  ..., -0.9657,  0.5462, -0.1784]],\n",
       "  \n",
       "            [[-0.0421, -0.0243, -0.1207,  ...,  0.1521, -0.0094,  0.0965],\n",
       "             [-0.7639,  0.5166, -0.1799,  ..., -0.1223, -0.1309,  0.1492],\n",
       "             [ 1.2735, -0.3839, -0.8504,  ...,  1.0511, -0.5509,  0.5284],\n",
       "             ...,\n",
       "             [ 0.8194, -0.2286, -1.0354,  ...,  0.4890, -0.2652,  0.2940],\n",
       "             [-1.2244, -0.0647, -0.0435,  ...,  0.0621, -0.3496, -0.1790],\n",
       "             [ 0.7925, -0.2165, -1.0473,  ...,  0.4959, -0.3003,  0.2679]],\n",
       "  \n",
       "            [[-0.0684, -0.0282,  0.1385,  ..., -0.0458, -0.0772,  0.1848],\n",
       "             [-0.9698,  0.3077,  0.7843,  ...,  0.1362, -0.3643, -0.6377],\n",
       "             [ 0.8484, -1.9875, -0.3901,  ..., -0.5127,  0.7504,  0.3240],\n",
       "             ...,\n",
       "             [ 0.2670, -0.6692,  1.3751,  ..., -0.1892, -0.7515,  0.1590],\n",
       "             [-0.6329,  0.0805,  1.8714,  ...,  0.2167, -1.0237,  0.0263],\n",
       "             [ 0.2379, -0.6431,  1.3933,  ..., -0.1740, -0.7878,  0.1467]]]]],\n",
       "         grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 4.5990e-01, -1.6423e-01, -7.0473e-02,  ...,  1.2789e-02,\n",
       "               3.3595e-01, -3.0549e-01],\n",
       "             [ 1.2482e+00,  3.7890e-02, -1.9409e+00,  ..., -8.0578e-01,\n",
       "              -1.2600e-01,  6.0470e-01],\n",
       "             [ 8.8900e-01,  2.4907e-01, -4.4895e-01,  ..., -1.1019e+00,\n",
       "               8.3893e-01,  6.2958e-01],\n",
       "             ...,\n",
       "             [ 1.0981e+00,  1.6426e-01, -1.0801e+00,  ..., -1.6687e-01,\n",
       "               3.7372e-02, -2.3268e+00],\n",
       "             [ 8.1382e-01, -6.7716e-01, -1.4475e+00,  ...,  3.4822e-01,\n",
       "               4.2177e-01, -1.7670e+00],\n",
       "             [ 1.0660e+00,  9.8135e-02, -1.1025e+00,  ..., -2.1228e-01,\n",
       "              -5.9497e-03, -2.3879e+00]],\n",
       "  \n",
       "            [[ 1.1706e-01,  3.4325e-02,  3.3315e-01,  ...,  6.6974e-02,\n",
       "               3.6118e-01, -1.7523e-01],\n",
       "             [-5.3648e-02, -1.2139e+00, -6.2853e-01,  ...,  5.2832e-01,\n",
       "               1.6742e+00,  5.4137e-02],\n",
       "             [-4.6883e-01, -1.1231e-02, -1.2756e+00,  ...,  3.3899e-01,\n",
       "               1.0640e+00, -1.8882e-01],\n",
       "             ...,\n",
       "             [-5.1580e-01,  1.6949e+00, -5.7431e+00,  ...,  3.9008e-01,\n",
       "               5.9018e-01, -1.3713e+00],\n",
       "             [ 3.1070e-01,  6.7545e-01, -5.6964e+00,  ...,  5.7468e-01,\n",
       "               1.3603e+00, -9.8501e-01],\n",
       "             [-5.2418e-01,  1.6924e+00, -5.8958e+00,  ...,  4.2089e-01,\n",
       "               4.8843e-01, -1.3733e+00]],\n",
       "  \n",
       "            [[ 9.6266e-02,  5.3009e-01, -3.6293e-01,  ...,  3.0148e-02,\n",
       "               1.0204e-01,  2.1953e-01],\n",
       "             [ 1.0387e-02, -7.9064e-01, -8.9884e-01,  ..., -2.7221e-01,\n",
       "               1.5673e+00, -1.2077e+00],\n",
       "             [-1.1311e+00, -9.4840e-01, -5.1545e-02,  ...,  3.5841e-01,\n",
       "               1.2506e+00, -2.8423e+00],\n",
       "             ...,\n",
       "             [-7.4123e-02, -1.0736e+00, -1.7238e+00,  ..., -1.1126e+00,\n",
       "               3.7543e-01, -6.6150e-01],\n",
       "             [ 2.7488e-02, -1.5760e+00, -1.9093e+00,  ..., -1.0375e+00,\n",
       "              -8.9292e-02, -4.8788e-01],\n",
       "             [-7.4714e-02, -1.0654e+00, -1.6442e+00,  ..., -1.1032e+00,\n",
       "               3.8112e-01, -6.3468e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-3.4160e-02,  2.7792e-01,  1.9446e-01,  ...,  2.1648e-01,\n",
       "              -4.0505e-01,  2.1871e-01],\n",
       "             [-2.9885e+00, -1.8224e-01,  6.8868e-01,  ..., -3.1841e-01,\n",
       "              -1.9787e-01, -1.2035e-01],\n",
       "             [-1.0867e+00,  2.2213e+00,  4.0476e-02,  ...,  2.8628e-01,\n",
       "              -2.5353e+00,  1.4230e+00],\n",
       "             ...,\n",
       "             [ 6.9197e-01,  3.2871e+00,  3.2218e-01,  ...,  2.0622e+00,\n",
       "              -1.9082e+00,  7.6810e-01],\n",
       "             [-6.0836e-01,  1.9449e+00,  2.4467e+00,  ...,  1.4759e+00,\n",
       "              -8.3232e-01, -2.7693e-01],\n",
       "             [ 8.3263e-01,  3.2847e+00,  3.5998e-01,  ...,  2.1708e+00,\n",
       "              -1.8807e+00,  6.9558e-01]],\n",
       "  \n",
       "            [[-4.4253e-02, -5.1474e-03,  1.1470e-01,  ..., -6.7926e-02,\n",
       "               3.6750e-01,  2.0194e-01],\n",
       "             [ 7.7663e-01,  1.7269e+00, -1.8497e+00,  ..., -1.6134e+00,\n",
       "              -1.6338e+00, -4.3356e-01],\n",
       "             [ 1.3555e+00,  1.3743e+00,  1.1733e+00,  ..., -1.1017e+00,\n",
       "              -2.4731e-01,  1.7001e+00],\n",
       "             ...,\n",
       "             [-1.3996e+00,  2.8939e+00,  1.2424e+00,  ..., -3.8124e-01,\n",
       "              -3.6470e-02,  1.3434e+00],\n",
       "             [-3.0022e+00,  2.7067e+00, -7.0257e-02,  ..., -1.1823e+00,\n",
       "              -9.0575e-01, -1.5187e-01],\n",
       "             [-1.4168e+00,  2.8616e+00,  1.3390e+00,  ..., -3.8588e-01,\n",
       "              -3.8711e-02,  1.3494e+00]],\n",
       "  \n",
       "            [[ 4.9678e-02,  5.1285e-02,  8.7136e-02,  ..., -7.4805e-02,\n",
       "              -2.3733e-01, -1.7542e-01],\n",
       "             [ 3.6371e-02,  1.9350e-01, -1.0161e-02,  ..., -1.0966e+00,\n",
       "              -5.7417e-01, -1.7524e-01],\n",
       "             [-1.8043e+00, -4.1836e-01,  1.1041e+00,  ..., -2.9041e+00,\n",
       "              -2.7024e-01,  7.4560e-01],\n",
       "             ...,\n",
       "             [-2.0449e+00, -1.3430e+00, -1.3380e-01,  ..., -3.9875e+00,\n",
       "               6.8061e-01,  6.3249e-01],\n",
       "             [-1.1337e+00, -1.1366e+00,  5.2844e-01,  ..., -3.6932e+00,\n",
       "              -1.0545e+00, -1.4105e-01],\n",
       "             [-2.0809e+00, -1.3452e+00, -7.8858e-02,  ..., -3.9452e+00,\n",
       "               7.0290e-01,  6.1458e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-5.7782e-02, -4.7315e-02,  2.4928e-02,  ...,  5.5143e-02,\n",
       "               6.4048e-02,  4.7465e-02],\n",
       "             [-1.3786e-01,  6.7696e-01,  7.4004e-01,  ...,  9.2297e-01,\n",
       "               5.6387e-01, -8.7097e-02],\n",
       "             [ 3.1798e-01, -2.2610e-01, -6.8674e-01,  ...,  4.5118e-01,\n",
       "               6.5257e-01, -5.1282e-01],\n",
       "             ...,\n",
       "             [ 3.6494e-02, -4.3679e-01, -1.4050e-01,  ..., -2.1511e+00,\n",
       "              -5.5579e-01,  1.9740e-01],\n",
       "             [ 1.1142e-02,  3.8077e-01, -2.3947e-01,  ..., -7.8761e-01,\n",
       "               5.5888e-01, -1.3959e-01],\n",
       "             [ 5.5540e-02, -3.7554e-01, -2.0669e-01,  ..., -2.1263e+00,\n",
       "              -5.6430e-01,  1.8848e-01]],\n",
       "  \n",
       "            [[-6.3957e-02, -7.9304e-02,  1.5967e-02,  ...,  6.7480e-03,\n",
       "              -1.2040e-01, -2.2468e-01],\n",
       "             [ 2.3844e-02,  1.0196e+00,  1.2122e-01,  ...,  1.3954e+00,\n",
       "              -2.0906e+00, -6.7638e-01],\n",
       "             [ 6.9860e-01,  5.0115e-01,  6.4690e-01,  ...,  1.1850e+00,\n",
       "              -6.9568e-01,  1.3303e-01],\n",
       "             ...,\n",
       "             [ 1.5618e+00, -1.5785e-01,  6.5925e-01,  ...,  1.2683e+00,\n",
       "              -1.2242e+00, -6.0200e-01],\n",
       "             [ 2.9950e-01,  3.6721e-01,  3.3727e-01,  ...,  8.7919e-01,\n",
       "              -1.8889e+00, -1.1078e+00],\n",
       "             [ 1.4817e+00, -1.6266e-01,  6.1728e-01,  ...,  1.1992e+00,\n",
       "              -1.1882e+00, -6.3248e-01]],\n",
       "  \n",
       "            [[ 1.0753e-01, -4.3963e-02, -6.6306e-02,  ...,  4.0179e-01,\n",
       "              -9.0919e-02, -1.4785e-02],\n",
       "             [-2.7449e-01, -3.0073e-01,  3.2270e-01,  ..., -2.4372e+00,\n",
       "              -3.1224e-02, -7.9692e-01],\n",
       "             [-4.3965e-01, -1.3882e+00,  2.5976e-01,  ..., -2.1991e+00,\n",
       "              -1.6639e-01, -1.2122e+00],\n",
       "             ...,\n",
       "             [-9.1309e-01, -2.7926e+00, -2.6798e-01,  ..., -2.8105e+00,\n",
       "              -2.2858e+00,  4.1534e-01],\n",
       "             [-6.0546e-03, -2.2705e+00,  2.7309e-01,  ..., -2.5844e+00,\n",
       "              -1.4688e+00,  1.0231e+00],\n",
       "             [-9.3313e-01, -2.8158e+00, -2.6005e-01,  ..., -2.7976e+00,\n",
       "              -2.2347e+00,  4.3881e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 7.5646e-02,  3.5870e-03, -1.6026e-01,  ..., -3.9409e-02,\n",
       "              -1.7806e-01,  1.5072e-01],\n",
       "             [ 2.7148e-01, -7.3097e-01,  2.0453e-01,  ..., -7.3614e-01,\n",
       "              -1.3660e-01, -4.7905e-01],\n",
       "             [-5.6934e-02, -1.1385e-01,  2.4108e-01,  ...,  1.0072e-01,\n",
       "              -2.1553e-01,  1.0231e+00],\n",
       "             ...,\n",
       "             [-4.6304e-01, -5.3788e-02,  2.9625e-01,  ...,  1.2628e-02,\n",
       "              -4.7854e-01,  1.0911e+00],\n",
       "             [ 9.5285e-01, -2.9438e-01,  7.5982e-01,  ..., -1.7827e-01,\n",
       "              -8.8304e-02,  2.8551e-01],\n",
       "             [-4.4781e-01, -1.5304e-03,  2.7036e-01,  ...,  1.2610e-02,\n",
       "              -4.6309e-01,  1.0847e+00]],\n",
       "  \n",
       "            [[ 1.5349e-01, -8.3581e-03,  1.8578e-02,  ..., -1.0560e-01,\n",
       "               6.0635e-02,  2.1364e-02],\n",
       "             [-2.3644e+00, -3.4632e-02, -1.1961e+00,  ..., -1.2036e+00,\n",
       "               6.7561e-01, -5.9303e-01],\n",
       "             [-3.2115e-02, -4.0962e-01,  9.3737e-01,  ..., -1.0937e+00,\n",
       "              -6.3260e-01,  3.3339e-01],\n",
       "             ...,\n",
       "             [-1.1529e-01,  6.1468e-01,  2.5898e-01,  ..., -5.7044e-01,\n",
       "              -4.6099e-02,  9.0789e-01],\n",
       "             [-1.2440e+00,  8.6880e-01, -8.5127e-01,  ..., -6.7322e-01,\n",
       "               1.4799e+00, -9.1919e-01],\n",
       "             [-1.0191e-01,  6.2414e-01,  2.4962e-01,  ..., -5.3921e-01,\n",
       "              -3.5162e-02,  8.6342e-01]],\n",
       "  \n",
       "            [[ 2.7211e-02, -7.5024e-02, -5.6496e-02,  ..., -3.7664e-03,\n",
       "               1.7206e-02,  1.3883e-03],\n",
       "             [-2.2256e-01,  2.0972e+00,  6.6128e-01,  ..., -8.3051e-01,\n",
       "               5.4045e-01, -1.5901e-02],\n",
       "             [ 1.8208e-01, -3.1738e-01, -3.1187e-02,  ...,  1.2640e+00,\n",
       "               5.1007e-01, -8.3236e-01],\n",
       "             ...,\n",
       "             [-1.5674e-01, -1.0412e+00,  2.9375e-01,  ...,  1.7144e-01,\n",
       "               1.7886e-01, -8.4005e-01],\n",
       "             [-3.7356e-01,  6.1266e-01,  3.6698e-02,  ..., -4.7902e-01,\n",
       "              -1.5491e-01, -2.3358e-01],\n",
       "             [-1.5318e-01, -1.0205e+00,  2.9034e-01,  ...,  1.7920e-01,\n",
       "               1.9913e-01, -7.7656e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 1.3771e-01,  9.0157e-01, -3.5496e-01,  ..., -2.0243e-01,\n",
       "               9.8999e-02,  7.9731e-02],\n",
       "             [ 6.8603e-01, -8.9343e-01,  1.3114e+00,  ..., -1.1600e-01,\n",
       "               5.5447e-01, -8.8110e-01],\n",
       "             [ 7.1423e-01,  2.9217e-01,  1.4583e-02,  ..., -1.1546e+00,\n",
       "               2.3820e-01,  1.0250e+00],\n",
       "             ...,\n",
       "             [ 1.2194e+00,  7.9311e+00,  1.5823e+00,  ...,  5.1556e-01,\n",
       "               1.9357e+00,  2.5451e+00],\n",
       "             [ 1.1668e+00,  7.9978e+00,  2.5361e+00,  ...,  7.6883e-01,\n",
       "               1.9658e+00,  1.1794e+00],\n",
       "             [ 1.2941e+00,  8.0683e+00,  1.5205e+00,  ...,  5.8429e-01,\n",
       "               1.9638e+00,  2.4792e+00]],\n",
       "  \n",
       "            [[ 6.1901e-02,  8.8106e-01, -2.8147e-01,  ..., -2.6773e-01,\n",
       "              -1.8376e-01, -1.1549e-02],\n",
       "             [-8.8878e-01,  5.5974e-01, -3.4638e-01,  ...,  7.9472e-01,\n",
       "              -1.6082e-01,  8.5831e-01],\n",
       "             [ 1.8869e+00,  1.1415e-01, -1.9564e+00,  ..., -1.6116e+00,\n",
       "               7.9366e-01, -8.6390e-01],\n",
       "             ...,\n",
       "             [-1.4537e-01,  2.8966e+00, -4.6524e+00,  ..., -2.7780e+00,\n",
       "               8.6829e-01, -1.1170e+00],\n",
       "             [-2.0415e+00,  2.8823e+00, -2.8646e+00,  ..., -1.3703e+00,\n",
       "               9.7047e-01, -5.9055e-02],\n",
       "             [-2.5966e-01,  3.1442e+00, -4.6484e+00,  ..., -2.7134e+00,\n",
       "               8.0791e-01, -9.9943e-01]],\n",
       "  \n",
       "            [[ 2.1769e-01, -3.8806e-01,  1.8626e-01,  ...,  2.7785e-01,\n",
       "               3.8709e-01,  3.7514e-01],\n",
       "             [-3.6386e-01,  9.5170e-01, -8.2031e-01,  ..., -1.9245e+00,\n",
       "               7.9807e-01, -1.5011e+00],\n",
       "             [ 6.4954e-01, -4.5205e-01,  1.4459e+00,  ...,  5.1824e-01,\n",
       "               1.2267e-01, -3.4894e-01],\n",
       "             ...,\n",
       "             [-1.5725e+00,  1.9465e+00,  1.6578e+00,  ..., -9.3065e-01,\n",
       "               1.0014e+00, -1.8643e-01],\n",
       "             [-1.3059e+00,  2.3995e+00,  5.2118e-02,  ..., -1.3595e+00,\n",
       "               3.1534e+00, -5.7495e-01],\n",
       "             [-1.6305e+00,  2.0022e+00,  1.6790e+00,  ..., -1.0458e+00,\n",
       "               1.0953e+00, -1.9653e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 7.2850e-02, -9.8556e-02,  6.3139e-02,  ...,  9.9210e-02,\n",
       "               5.8900e-02, -1.7060e-01],\n",
       "             [ 5.8704e-01,  1.0792e+00,  7.3844e-01,  ...,  2.3723e+00,\n",
       "               5.5190e-01,  8.6259e-01],\n",
       "             [ 1.6026e+00,  8.4365e-01, -1.4142e+00,  ...,  1.1797e+00,\n",
       "              -5.2442e-01,  1.8882e-02],\n",
       "             ...,\n",
       "             [ 2.1647e+00,  3.9805e-01, -1.2591e+00,  ...,  3.8199e-01,\n",
       "              -2.1057e+00, -9.5578e-02],\n",
       "             [ 2.2274e+00,  1.2014e+00, -2.0953e-01,  ...,  8.7797e-01,\n",
       "              -2.2200e+00,  5.7655e-01],\n",
       "             [ 2.1243e+00,  4.3669e-01, -1.2126e+00,  ...,  3.5034e-01,\n",
       "              -2.1348e+00, -1.6700e-01]],\n",
       "  \n",
       "            [[ 1.3089e-01, -1.3481e-02,  9.4816e-02,  ...,  3.0806e-02,\n",
       "               5.2371e-03, -1.3107e+00],\n",
       "             [ 1.1604e-01,  5.0168e-01,  1.2727e+00,  ..., -1.1964e+00,\n",
       "              -2.5043e-01,  2.5933e+00],\n",
       "             [-1.1752e+00, -6.4684e-02, -4.6941e-01,  ..., -1.4915e+00,\n",
       "               6.2559e-01,  2.9623e+00],\n",
       "             ...,\n",
       "             [-1.1976e+00,  1.1133e+00,  8.7948e-01,  ..., -5.4465e-01,\n",
       "               9.8191e-01, -1.7758e+00],\n",
       "             [-1.0035e+00,  9.2925e-01,  1.8043e+00,  ..., -1.1288e+00,\n",
       "               4.4028e-01, -1.9522e+00],\n",
       "             [-1.2144e+00,  1.1093e+00,  9.0577e-01,  ..., -5.4871e-01,\n",
       "               1.0409e+00, -2.0743e+00]],\n",
       "  \n",
       "            [[-2.7150e-01, -3.4428e-01, -7.1111e-02,  ...,  5.7818e-01,\n",
       "               4.7107e-01, -2.5103e-01],\n",
       "             [-1.9029e-01,  2.2406e+00,  6.1711e-01,  ..., -7.3203e-01,\n",
       "               3.6917e-02, -1.0914e-01],\n",
       "             [-6.8767e-01, -1.1247e+00, -6.2679e-01,  ..., -8.4440e-01,\n",
       "              -7.2531e-01,  6.7224e-01],\n",
       "             ...,\n",
       "             [-2.2645e+00, -2.3166e-01,  5.8330e-01,  ..., -3.9381e-01,\n",
       "              -2.5768e+00,  1.0639e+00],\n",
       "             [-1.5708e+00,  1.0621e+00,  1.7766e-01,  ..., -6.1433e-02,\n",
       "              -1.9710e+00,  9.7734e-01],\n",
       "             [-2.2770e+00, -2.7306e-01,  5.7732e-01,  ..., -2.9944e-01,\n",
       "              -2.6747e+00,  1.0706e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 1.7769e-01, -1.9868e-02,  3.6486e-02,  ...,  9.6218e-03,\n",
       "               6.7924e-02,  3.1965e-03],\n",
       "             [ 4.3672e-02,  4.0235e-02,  4.7228e-01,  ...,  8.4840e-01,\n",
       "               1.6675e-01,  3.4818e-01],\n",
       "             [-5.6196e-01, -4.6746e-01,  5.8312e-01,  ...,  6.6042e-01,\n",
       "              -5.3383e-01,  3.6734e-01],\n",
       "             ...,\n",
       "             [ 2.9145e-01,  2.3742e-01,  6.9251e-01,  ..., -3.0051e-02,\n",
       "              -6.0954e-01,  4.8869e-02],\n",
       "             [ 2.4541e-01,  4.7130e-01,  4.6727e-01,  ...,  8.7736e-01,\n",
       "               4.5745e-01, -6.9198e-02],\n",
       "             [ 3.2747e-01,  2.8414e-01,  6.3209e-01,  ..., -2.3813e-03,\n",
       "              -5.5888e-01,  7.7983e-02]],\n",
       "  \n",
       "            [[-4.0079e-02,  4.7627e-02, -2.0990e-01,  ..., -1.3484e-01,\n",
       "               3.0816e-02,  6.4082e-02],\n",
       "             [ 4.6453e-01, -4.0604e-01,  8.2549e-01,  ...,  1.5971e-01,\n",
       "               9.2278e-02,  9.9117e-01],\n",
       "             [ 5.3281e-01,  7.3788e-01,  2.8048e-01,  ...,  3.3176e-01,\n",
       "              -1.3099e+00, -2.4246e-01],\n",
       "             ...,\n",
       "             [ 3.0422e-01, -2.4027e-01, -5.5547e-01,  ..., -8.0873e-01,\n",
       "              -2.7658e-01, -2.8836e-02],\n",
       "             [-6.6797e-01,  3.9216e-02,  9.4326e-01,  ..., -6.2509e-02,\n",
       "               3.0423e-01,  4.2278e-01],\n",
       "             [ 3.3224e-01, -2.2553e-01, -5.4199e-01,  ..., -8.0184e-01,\n",
       "              -2.6850e-01, -8.1512e-02]],\n",
       "  \n",
       "            [[ 7.2285e-02, -3.1630e-02, -1.6655e-01,  ..., -8.9160e-02,\n",
       "              -2.0111e-01, -6.5453e-02],\n",
       "             [ 7.7623e-01,  6.3914e-01, -2.2372e-01,  ..., -1.4663e-01,\n",
       "               2.6967e-01, -3.3026e-01],\n",
       "             [ 1.9590e-01,  4.8709e-01, -1.5344e-01,  ..., -5.6191e-01,\n",
       "               1.9368e-02,  5.6774e-02],\n",
       "             ...,\n",
       "             [ 2.8809e-01,  5.4727e-02, -1.7595e-01,  ..., -1.3927e-01,\n",
       "               1.0250e-01, -1.7690e-01],\n",
       "             [ 2.5719e-01,  3.9549e-01,  1.2690e-01,  ...,  7.1759e-01,\n",
       "               6.5449e-01, -7.9504e-01],\n",
       "             [ 2.5784e-01,  2.2483e-02, -1.6551e-01,  ..., -1.5103e-01,\n",
       "               1.4847e-01, -1.6177e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-4.5902e-02, -9.6472e-02, -1.7944e-02,  ...,  6.7100e-02,\n",
       "              -6.8915e-03, -4.4650e-02],\n",
       "             [ 5.2386e-01,  1.1234e-01,  7.9435e-01,  ...,  8.2008e-01,\n",
       "              -1.4411e-02, -6.8194e-01],\n",
       "             [-6.9178e-01, -2.1721e-01, -4.8476e-02,  ..., -1.6245e+00,\n",
       "               5.4425e-01, -1.1310e-01],\n",
       "             ...,\n",
       "             [-9.9717e-01, -6.3572e-01, -5.6292e-01,  ..., -6.4437e-03,\n",
       "               3.7575e-01, -4.7072e-01],\n",
       "             [-6.0714e-01,  7.0789e-02,  3.4317e-01,  ...,  4.4393e-01,\n",
       "               5.5097e-01, -2.8036e-01],\n",
       "             [-1.0006e+00, -5.9933e-01, -5.5422e-01,  ..., -1.7138e-02,\n",
       "               3.6980e-01, -4.7932e-01]],\n",
       "  \n",
       "            [[-9.2612e-02,  1.9121e-02,  4.8481e-02,  ..., -2.0642e-02,\n",
       "              -1.2847e-01,  3.2826e-02],\n",
       "             [-2.6852e-01,  4.9444e-01, -6.7064e-01,  ...,  9.5245e-02,\n",
       "               2.3633e-01, -1.0949e+00],\n",
       "             [ 1.6142e-01,  1.0994e+00, -9.6061e-01,  ...,  1.2240e+00,\n",
       "              -3.3590e-01, -7.3955e-01],\n",
       "             ...,\n",
       "             [ 7.2288e-02, -4.5554e-02, -8.1182e-02,  ..., -6.1590e-01,\n",
       "              -2.8048e-01, -1.2142e+00],\n",
       "             [-2.2214e-01, -1.5382e-01, -6.1776e-01,  ...,  7.5930e-02,\n",
       "              -2.6566e-01, -1.3068e+00],\n",
       "             [ 5.4183e-02, -8.2436e-02, -3.4182e-02,  ..., -5.7980e-01,\n",
       "              -2.7879e-01, -1.2075e+00]],\n",
       "  \n",
       "            [[ 1.3356e-02, -1.4301e-01, -3.2877e-02,  ...,  4.6615e-03,\n",
       "              -4.0957e-02,  1.0125e-02],\n",
       "             [ 4.9277e-01,  1.4792e+00, -1.0851e+00,  ...,  1.6033e+00,\n",
       "              -6.9564e-01, -2.0382e-01],\n",
       "             [-7.9968e-01,  3.1177e-01, -1.1993e+00,  ...,  2.4692e-01,\n",
       "               7.2428e-02,  1.2972e-01],\n",
       "             ...,\n",
       "             [-3.5555e-01, -5.3964e-01, -1.0287e+00,  ..., -2.0806e-01,\n",
       "              -3.9654e-02,  1.2974e+00],\n",
       "             [ 6.7501e-01,  4.1135e-02, -1.1275e+00,  ...,  1.0525e+00,\n",
       "              -2.5639e-01,  1.0310e+00],\n",
       "             [-3.9419e-01, -5.3765e-01, -9.6850e-01,  ..., -2.4740e-01,\n",
       "              -1.8159e-03,  1.2894e+00]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 6.8537e-01,  2.1427e-01, -4.5352e-02,  ..., -3.5849e-03,\n",
       "              -2.8943e-02, -6.6344e-02],\n",
       "             [-1.6954e+00, -2.2446e+00,  8.0909e-01,  ...,  1.0270e+00,\n",
       "              -1.7578e+00, -3.4627e-01],\n",
       "             [ 1.2369e+00,  1.0438e-01,  1.9135e+00,  ..., -8.9543e-01,\n",
       "               1.1045e+00, -2.0674e+00],\n",
       "             ...,\n",
       "             [ 2.2138e-01,  1.4968e+00,  1.6126e+00,  ..., -1.6487e+00,\n",
       "               1.9645e-01, -2.2584e+00],\n",
       "             [-1.7065e+00, -2.3399e-01,  1.5907e+00,  ..., -7.9670e-01,\n",
       "              -9.3064e-01, -1.2596e+00],\n",
       "             [ 2.9130e-01,  1.5144e+00,  1.5056e+00,  ..., -1.6140e+00,\n",
       "               1.7768e-01, -2.1949e+00]],\n",
       "  \n",
       "            [[ 2.1328e-01, -1.8435e-01, -1.6182e-01,  ...,  5.5558e-02,\n",
       "               2.4471e-02,  1.4388e-01],\n",
       "             [ 7.7110e-01,  7.4403e-02, -5.3017e-01,  ..., -3.8544e-01,\n",
       "               1.2777e+00,  4.4977e-01],\n",
       "             [ 7.8479e-01,  2.9403e-01, -1.8430e-01,  ...,  2.9821e-01,\n",
       "               4.1774e-01,  7.2198e-01],\n",
       "             ...,\n",
       "             [ 6.2793e-01,  1.3856e+00, -7.5469e-01,  ...,  8.9061e-01,\n",
       "              -8.9901e-01, -3.8924e-01],\n",
       "             [ 1.1514e+00,  4.7978e-01, -5.7582e-01,  ...,  7.4075e-01,\n",
       "              -1.2570e+00, -9.9033e-01],\n",
       "             [ 7.0615e-01,  1.3269e+00, -8.2562e-01,  ...,  9.2904e-01,\n",
       "              -9.7526e-01, -3.4804e-01]],\n",
       "  \n",
       "            [[-8.6636e-02,  1.2201e-01,  1.5518e-02,  ..., -4.4909e-02,\n",
       "              -1.5601e-01, -1.0241e-01],\n",
       "             [ 8.8407e-01, -1.3988e-01, -3.1171e-01,  ..., -5.3115e-01,\n",
       "               5.9184e-01,  3.0933e+00],\n",
       "             [ 1.3505e+00,  7.0934e-01,  1.1093e+00,  ...,  4.7968e-01,\n",
       "              -7.9301e-02, -1.3391e+00],\n",
       "             ...,\n",
       "             [ 7.1144e-01, -9.2167e-01,  1.3211e+00,  ...,  6.1275e-01,\n",
       "               5.7988e-01, -1.3222e+00],\n",
       "             [ 5.3658e-01, -1.5493e+00,  6.5235e-01,  ...,  1.0666e+00,\n",
       "               5.0489e-01,  1.1325e-01],\n",
       "             [ 7.3646e-01, -1.0215e+00,  1.1981e+00,  ...,  6.8063e-01,\n",
       "               6.5228e-01, -1.3886e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 2.6494e-02,  7.8296e-01,  1.8132e-01,  ...,  7.4352e-02,\n",
       "               4.1801e-02, -1.4314e-01],\n",
       "             [-1.1518e+00, -2.9566e+00,  1.5535e+00,  ...,  7.1298e-01,\n",
       "               1.2990e+00, -1.9996e+00],\n",
       "             [-1.3578e+00, -1.6476e+00, -1.8125e-01,  ...,  1.2472e+00,\n",
       "               6.5089e-01, -1.2741e+00],\n",
       "             ...,\n",
       "             [-1.8210e+00,  8.9778e-01, -1.4465e+00,  ...,  1.1802e-01,\n",
       "              -1.3118e+00, -1.8856e+00],\n",
       "             [-2.3797e+00,  6.0693e-01, -7.0705e-01,  ...,  4.1585e-01,\n",
       "              -2.2993e-01, -2.1793e+00],\n",
       "             [-1.7358e+00,  1.0732e+00, -1.4490e+00,  ...,  6.9380e-02,\n",
       "              -1.3049e+00, -1.8447e+00]],\n",
       "  \n",
       "            [[ 2.0060e-01,  7.5593e-02, -2.8681e-02,  ...,  4.2734e-01,\n",
       "              -2.1390e-02, -7.7349e-02],\n",
       "             [-2.6036e+00, -5.6494e-01,  3.4092e-01,  ..., -1.9044e+00,\n",
       "               6.0452e-01,  7.2233e-01],\n",
       "             [ 8.2164e-01, -7.3673e-01, -5.6234e-01,  ..., -2.9345e+00,\n",
       "               1.5968e-01, -1.6516e+00],\n",
       "             ...,\n",
       "             [-4.9517e-01, -2.2784e-01,  4.8798e-01,  ...,  1.6577e+00,\n",
       "              -9.3673e-01, -1.1740e+00],\n",
       "             [-2.2997e+00, -2.5739e-01,  1.4306e-01,  ...,  2.1128e+00,\n",
       "               4.8050e-01,  9.2496e-01],\n",
       "             [-5.1131e-01, -2.2419e-01,  4.5266e-01,  ...,  1.8985e+00,\n",
       "              -9.4349e-01, -1.1182e+00]],\n",
       "  \n",
       "            [[-2.2082e-01,  2.9831e-01,  3.0861e-02,  ..., -1.0502e-01,\n",
       "              -1.8531e-02,  2.1701e-01],\n",
       "             [ 2.3554e+00, -1.2918e+00, -1.3569e+00,  ...,  6.8881e-01,\n",
       "               1.6159e+00,  9.3151e-01],\n",
       "             [ 9.6836e-01,  9.8888e-01, -1.3195e+00,  ...,  3.7121e-01,\n",
       "              -1.1329e+00, -3.6213e-01],\n",
       "             ...,\n",
       "             [ 7.5807e-01,  1.1804e+00,  1.0688e-01,  ..., -1.4068e+00,\n",
       "              -2.3224e+00, -8.0463e-01],\n",
       "             [ 1.7630e+00, -3.3871e-01,  1.2034e+00,  ..., -6.9985e-01,\n",
       "              -7.2815e-01,  1.8362e-01],\n",
       "             [ 6.9994e-01,  1.2109e+00,  2.3022e-01,  ..., -1.4998e+00,\n",
       "              -2.3277e+00, -7.7729e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-1.2125e-01,  2.2008e-02, -6.5314e-02,  ...,  9.2584e-02,\n",
       "               3.5623e-02, -3.7969e-04],\n",
       "             [ 2.0111e+00,  5.1528e-01,  4.3825e-01,  ...,  1.8468e+00,\n",
       "               7.3727e-01, -3.9190e-01],\n",
       "             [ 6.8265e-02,  8.6446e-01,  6.6308e-01,  ...,  4.7508e-01,\n",
       "              -7.2122e-01,  7.6963e-01],\n",
       "             ...,\n",
       "             [-3.6802e-01,  3.4177e-01,  7.8910e-01,  ...,  6.6477e-01,\n",
       "              -5.5517e-01,  6.0912e-01],\n",
       "             [ 1.0162e+00,  7.3033e-01,  3.6478e-01,  ...,  1.0292e+00,\n",
       "              -1.2334e-01,  6.9081e-02],\n",
       "             [-2.8647e-01,  3.2836e-01,  7.4716e-01,  ...,  6.4711e-01,\n",
       "              -5.1705e-01,  5.5538e-01]],\n",
       "  \n",
       "            [[-7.2903e-02,  4.9875e-02, -5.0968e-02,  ...,  5.6493e-02,\n",
       "              -7.1445e-02, -9.9216e-02],\n",
       "             [-3.3335e-02,  2.4220e-01,  3.5223e-02,  ...,  1.1298e-01,\n",
       "              -5.1401e-01, -2.3367e-01],\n",
       "             [-3.0075e-01,  8.0762e-01, -2.8344e-01,  ...,  1.5244e-01,\n",
       "              -1.1921e+00,  3.8304e-01],\n",
       "             ...,\n",
       "             [-2.3796e-02,  1.2921e+00, -1.0582e+00,  ...,  1.2896e+00,\n",
       "              -4.9185e-01,  4.8608e-01],\n",
       "             [-1.2282e+00,  5.2326e-01, -7.8570e-01,  ...,  1.9048e+00,\n",
       "              -2.1608e-01,  4.4203e-01],\n",
       "             [-8.1367e-02,  1.2290e+00, -1.0009e+00,  ...,  1.2673e+00,\n",
       "              -3.9385e-01,  4.7337e-01]],\n",
       "  \n",
       "            [[ 8.0475e-03, -1.6137e-02, -1.0294e-01,  ...,  7.2216e-02,\n",
       "              -1.7923e-01,  1.1483e-01],\n",
       "             [ 4.3647e-01, -1.1528e+00,  1.7065e+00,  ..., -4.8328e-01,\n",
       "               4.6387e-02,  5.5228e-01],\n",
       "             [ 2.8434e-01, -5.1202e-01,  2.5652e-01,  ..., -1.3514e+00,\n",
       "              -1.3213e+00, -7.1779e-01],\n",
       "             ...,\n",
       "             [ 4.5747e-01, -1.6742e-02, -1.9834e-01,  ..., -4.8238e-02,\n",
       "              -9.2763e-01, -6.0046e-02],\n",
       "             [-1.9865e-01, -3.8533e-01,  1.0194e+00,  ..., -8.0254e-03,\n",
       "              -1.0806e-01,  2.1529e-01],\n",
       "             [ 4.5575e-01,  3.2244e-02, -2.0221e-01,  ..., -1.0476e-01,\n",
       "              -9.0852e-01, -6.5058e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 9.4708e-02,  4.8268e-02, -1.7377e-01,  ...,  4.2699e-02,\n",
       "               1.0605e-01,  5.5429e-03],\n",
       "             [-4.3943e-01, -3.5992e-01,  1.0120e+00,  ..., -2.1329e-01,\n",
       "              -4.1847e-02,  4.4506e-01],\n",
       "             [ 6.3501e-01, -6.0898e-02, -1.4341e+00,  ...,  5.3269e-01,\n",
       "              -2.5241e-02, -1.1466e+00],\n",
       "             ...,\n",
       "             [-2.6318e-01, -1.2083e+00, -2.5827e+00,  ...,  1.7172e+00,\n",
       "               2.2955e+00, -1.0936e+00],\n",
       "             [-1.6644e+00, -1.1592e+00, -1.3508e+00,  ...,  1.0737e+00,\n",
       "               2.6780e+00, -8.1391e-01],\n",
       "             [-3.6019e-01, -1.1988e+00, -2.5079e+00,  ...,  1.6575e+00,\n",
       "               2.3004e+00, -1.0948e+00]],\n",
       "  \n",
       "            [[-1.6755e-02,  8.8640e-02, -2.5329e-01,  ...,  3.8905e-02,\n",
       "              -2.4743e-01,  1.7618e-02],\n",
       "             [ 8.9692e-01, -1.2612e-01,  1.3151e+00,  ...,  7.0664e-02,\n",
       "              -9.2286e-01, -1.3151e-01],\n",
       "             [ 8.4719e-01,  2.4142e-01,  2.6738e-01,  ..., -3.8943e-01,\n",
       "              -2.7978e-01, -5.1305e-01],\n",
       "             ...,\n",
       "             [ 1.3561e+00,  4.0042e-02,  1.0466e+00,  ..., -5.1884e-01,\n",
       "              -1.0089e+00,  7.1590e-01],\n",
       "             [ 8.0763e-01,  4.0774e-01,  1.5238e+00,  ...,  5.9635e-01,\n",
       "              -1.2650e+00,  7.1216e-01],\n",
       "             [ 1.2497e+00,  4.4024e-02,  1.0708e+00,  ..., -5.1786e-01,\n",
       "              -1.0033e+00,  6.9646e-01]],\n",
       "  \n",
       "            [[-7.2593e-03,  1.8752e-02, -1.1463e-01,  ..., -1.3021e-02,\n",
       "               7.3455e-02,  1.1168e-01],\n",
       "             [ 1.2464e-01, -4.2998e-01,  2.6983e-01,  ..., -3.5254e-01,\n",
       "               1.6719e-01, -6.7581e-01],\n",
       "             [ 7.8095e-01, -9.6077e-01,  4.0577e-01,  ...,  3.1818e-03,\n",
       "              -2.4233e-01,  3.1054e-01],\n",
       "             ...,\n",
       "             [ 1.5630e-01, -3.2900e-01, -8.1370e-02,  ..., -2.8281e-01,\n",
       "              -1.2955e-01,  4.1290e-01],\n",
       "             [ 1.3419e-01, -4.0031e-01, -1.2824e-01,  ...,  1.6867e-01,\n",
       "              -1.1820e-01, -8.9405e-01],\n",
       "             [ 1.2531e-01, -3.6874e-01, -9.4275e-02,  ..., -3.7238e-01,\n",
       "              -1.3620e-01,  5.1232e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-0.1725, -0.1518, -0.0961,  ...,  0.1084, -0.2766, -0.0596],\n",
       "             [ 0.7464, -0.5116, -0.6193,  ...,  1.3030,  0.3052, -0.9409],\n",
       "             [-2.5683,  0.0542, -2.2359,  ..., -0.3944, -1.0062, -0.2594],\n",
       "             ...,\n",
       "             [-3.9871,  0.6364, -0.6240,  ..., -1.3251, -0.6507,  1.5664],\n",
       "             [-3.0463,  0.1919, -0.4736,  ...,  0.2192, -0.3783,  1.0022],\n",
       "             [-4.0613,  0.6211, -0.5828,  ..., -1.3436, -0.6599,  1.5943]],\n",
       "  \n",
       "            [[-0.2496,  0.0091, -0.6572,  ...,  0.6731, -1.0039, -0.4439],\n",
       "             [ 0.6287, -0.6484, -1.1698,  ...,  0.9320, -0.3400,  0.8464],\n",
       "             [ 1.0040, -0.8979, -1.5295,  ...,  0.0955,  1.0207,  0.2944],\n",
       "             ...,\n",
       "             [ 1.4279, -3.2427,  0.1518,  ...,  3.0343,  3.0296,  0.3985],\n",
       "             [ 0.8271, -3.7780,  0.5777,  ...,  2.9330,  2.2817,  1.5809],\n",
       "             [ 1.4319, -3.2761,  0.1365,  ...,  3.0843,  3.0228,  0.3730]],\n",
       "  \n",
       "            [[-0.2305, -1.0570, -0.0190,  ..., -0.9610,  0.0973, -0.1769],\n",
       "             [ 1.1877,  1.6812, -0.0397,  ...,  2.8911,  0.3245,  1.9362],\n",
       "             [ 2.5933, -0.0926,  0.6095,  ...,  2.9049,  0.1403,  0.3236],\n",
       "             ...,\n",
       "             [ 0.6820, -2.9862,  1.4882,  ..., -1.5353, -1.4353,  2.3769],\n",
       "             [ 1.2176, -2.6057,  0.9623,  ..., -1.6224, -1.4099,  3.5245],\n",
       "             [ 0.7311, -3.1263,  1.5489,  ..., -1.6933, -1.4675,  2.4115]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-0.1501, -0.1770,  0.0240,  ..., -0.0088, -0.1996, -0.1223],\n",
       "             [-0.1600, -0.7143,  0.5969,  ..., -0.1437,  0.9807,  0.5207],\n",
       "             [-1.0315, -0.1915, -0.9737,  ..., -0.1807,  2.7774, -0.5854],\n",
       "             ...,\n",
       "             [ 2.5506,  2.5869,  2.4115,  ...,  3.3484, -1.0548, -0.9053],\n",
       "             [ 2.2443,  2.0326,  2.3781,  ...,  2.7347, -1.9070, -0.1328],\n",
       "             [ 2.6559,  2.6714,  2.5148,  ...,  3.4722, -1.2035, -0.9089]],\n",
       "  \n",
       "            [[-0.0109, -0.3856,  0.1653,  ..., -0.0477,  0.0577,  0.1044],\n",
       "             [ 1.6053, -0.7605,  0.1032,  ...,  0.5636, -0.1899,  0.6509],\n",
       "             [ 2.5088, -2.8085, -0.4755,  ..., -0.2166, -0.8767,  0.8165],\n",
       "             ...,\n",
       "             [ 1.6020, -0.7100, -0.6736,  ...,  1.9320, -1.6346,  2.1945],\n",
       "             [ 2.0030,  0.4486, -0.3469,  ...,  1.6738, -1.8228,  2.1408],\n",
       "             [ 1.5387, -0.6330, -0.6183,  ...,  1.9330, -1.7079,  2.2002]],\n",
       "  \n",
       "            [[-0.0486, -0.1727,  0.0658,  ...,  0.0131, -0.0802,  0.3659],\n",
       "             [-0.2701, -0.3370,  1.0234,  ..., -0.0816, -0.0574, -1.3917],\n",
       "             [ 1.1309,  0.0310,  0.8943,  ...,  0.0368,  0.9252,  1.6798],\n",
       "             ...,\n",
       "             [ 0.9999, -1.8376,  0.3742,  ...,  1.1552, -0.9392, -0.5580],\n",
       "             [ 0.3958, -2.2266,  1.0668,  ...,  1.7600, -1.6932, -2.6588],\n",
       "             [ 0.9568, -1.8587,  0.3589,  ...,  1.2013, -0.9659, -0.7194]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 0.0312, -0.0368,  0.0323,  ..., -0.2076, -0.1007,  0.1123],\n",
       "             [ 0.3485,  0.0500, -1.0935,  ..., -1.1667,  0.0831, -1.5754],\n",
       "             [-0.3529,  0.7031, -1.1933,  ..., -1.1075, -0.5890,  0.1130],\n",
       "             ...,\n",
       "             [ 0.2540,  0.0776, -0.8328,  ..., -0.2318,  0.0531, -0.4256],\n",
       "             [-0.3853,  0.2326, -0.9210,  ..., -0.0453,  0.1055, -0.6854],\n",
       "             [ 0.2738,  0.0930, -0.8134,  ..., -0.2573,  0.0633, -0.4171]],\n",
       "  \n",
       "            [[ 0.0123, -0.0266, -0.0128,  ..., -0.0382, -0.0596,  0.0701],\n",
       "             [-0.4253,  1.3090, -1.0143,  ..., -0.1980,  0.4484,  0.7133],\n",
       "             [ 0.5735,  1.3106, -0.2318,  ..., -0.0358,  1.3607, -0.2642],\n",
       "             ...,\n",
       "             [ 0.3378,  0.7209, -0.2407,  ..., -0.0363,  0.4884,  0.2024],\n",
       "             [-0.2335,  0.4285, -0.8415,  ..., -0.0657, -0.2582,  1.2119],\n",
       "             [ 0.3568,  0.6982, -0.2684,  ...,  0.0369,  0.4604,  0.2416]],\n",
       "  \n",
       "            [[ 0.0264,  0.0569, -0.0903,  ...,  0.0139,  0.0220, -0.0556],\n",
       "             [-0.0226,  1.1313, -0.4856,  ...,  0.4479,  0.1301,  0.1747],\n",
       "             [-0.3229,  0.2677,  0.9708,  ..., -1.2685,  1.6110, -1.1742],\n",
       "             ...,\n",
       "             [-1.0428,  0.4100, -0.0278,  ...,  0.1193,  0.4352,  0.0973],\n",
       "             [-0.3195,  0.4140, -0.4357,  ...,  0.5867,  0.2708,  0.0982],\n",
       "             [-1.0422,  0.3663, -0.0104,  ...,  0.1104,  0.4047,  0.0441]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 0.1105,  0.1602,  0.0478,  ...,  0.0070, -0.1864,  0.0194],\n",
       "             [-1.1568,  0.4072,  1.7580,  ..., -0.8772,  0.4214, -1.2068],\n",
       "             [-0.6857,  0.0191,  0.4976,  ..., -0.5413,  0.6075,  0.7398],\n",
       "             ...,\n",
       "             [-0.1869, -0.6568,  0.7816,  ...,  0.0395,  0.2143,  0.5802],\n",
       "             [-1.2585, -0.7074,  1.2800,  ..., -0.2988,  0.5696, -0.3933],\n",
       "             [-0.2174, -0.6372,  0.7504,  ...,  0.0530,  0.2524,  0.6088]],\n",
       "  \n",
       "            [[ 0.0688, -0.0814,  0.1127,  ..., -0.0949,  0.0803, -0.0690],\n",
       "             [-0.8122,  0.0583,  0.6352,  ...,  0.4150, -0.2034,  0.4368],\n",
       "             [ 0.0730, -0.6045,  0.1518,  ...,  0.0512,  0.0130,  0.2126],\n",
       "             ...,\n",
       "             [-0.0592, -1.2808,  0.9065,  ..., -0.2051,  0.4805, -0.5722],\n",
       "             [-0.9348, -0.1863,  0.6156,  ...,  0.7969, -0.4455,  0.7270],\n",
       "             [-0.0547, -1.2175,  0.9086,  ..., -0.2053,  0.4505, -0.5660]],\n",
       "  \n",
       "            [[-0.1646, -0.1855, -0.0153,  ..., -0.1758, -0.0934, -0.1412],\n",
       "             [-0.2879, -0.2852,  0.3780,  ..., -0.8950,  1.0840,  0.0219],\n",
       "             [-0.5403, -0.8192, -0.3513,  ...,  0.0673,  0.3819,  0.3494],\n",
       "             ...,\n",
       "             [-0.5440, -0.7614,  0.0929,  ..., -0.3380,  0.9736,  0.5806],\n",
       "             [-0.5529, -0.5317,  0.1874,  ..., -0.5057,  0.8674, -0.0787],\n",
       "             [-0.5204, -0.7341,  0.0621,  ..., -0.3713,  0.9285,  0.5746]]]]],\n",
       "         grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 5.6166e-02, -4.2002e-01,  3.8002e-01,  ...,  3.8938e-01,\n",
       "              -4.0163e-01,  2.1940e-01],\n",
       "             [-9.0036e-01,  2.5164e-01,  1.0827e+00,  ...,  8.3210e-01,\n",
       "               1.2399e+00, -5.2130e-01],\n",
       "             [-1.2126e+00, -4.3218e-01, -8.0373e-01,  ...,  3.1813e-01,\n",
       "               2.3243e+00, -8.6250e-01],\n",
       "             ...,\n",
       "             [ 1.0634e+00,  3.6366e+00, -4.3412e+00,  ...,  2.6793e+00,\n",
       "              -1.2157e+00,  1.7478e+00],\n",
       "             [ 1.1861e+00,  3.9963e+00, -3.7035e+00,  ...,  2.0344e+00,\n",
       "              -5.5688e-01,  1.3168e+00],\n",
       "             [ 1.1166e+00,  3.7670e+00, -4.3342e+00,  ...,  2.7086e+00,\n",
       "              -1.2816e+00,  1.7340e+00]],\n",
       "  \n",
       "            [[ 1.5011e+00, -2.4253e-01, -1.8437e+00,  ..., -1.6119e+00,\n",
       "              -2.3475e+00,  1.6712e-01],\n",
       "             [ 1.2258e+00,  8.9521e-01,  9.8843e-01,  ...,  3.5756e-01,\n",
       "               1.7873e-01, -6.6019e-01],\n",
       "             [-1.1977e-01, -1.6766e+00,  1.5651e-01,  ..., -2.2401e+00,\n",
       "               2.1953e+00,  7.0043e-01],\n",
       "             ...,\n",
       "             [ 5.7594e-01, -3.2278e+00, -8.8773e-01,  ..., -2.5146e+00,\n",
       "              -1.4448e-01, -7.8484e-01],\n",
       "             [ 1.0864e+00, -1.9657e+00, -1.2011e-02,  ..., -9.0885e-01,\n",
       "              -7.2953e-01, -2.3621e+00],\n",
       "             [ 6.5725e-01, -3.2337e+00, -9.7704e-01,  ..., -2.4551e+00,\n",
       "              -3.2943e-01, -9.6910e-01]],\n",
       "  \n",
       "            [[ 1.2856e+00, -1.9217e-01, -9.8805e-01,  ..., -2.8358e-01,\n",
       "              -7.0646e-01, -5.0923e-02],\n",
       "             [ 2.4959e-01, -7.2976e-01,  1.9933e+00,  ...,  6.2721e-01,\n",
       "               4.9795e-01,  1.4856e+00],\n",
       "             [ 1.2986e+00, -2.7844e+00, -9.0845e-01,  ...,  6.6660e-01,\n",
       "               3.2374e-02,  9.0445e-01],\n",
       "             ...,\n",
       "             [ 6.4543e-01, -1.5075e+00, -2.7834e+00,  ...,  3.0820e+00,\n",
       "              -5.0923e+00, -2.9048e+00],\n",
       "             [ 5.0992e-01, -1.7386e+00, -1.8178e+00,  ...,  3.2099e+00,\n",
       "              -5.4081e+00, -3.0420e+00],\n",
       "             [ 6.7985e-01, -1.4265e+00, -2.8677e+00,  ...,  3.2350e+00,\n",
       "              -5.1615e+00, -3.0517e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 5.9247e-01, -8.0191e-02, -4.8320e-01,  ..., -4.2196e-02,\n",
       "               1.2568e-01, -1.1414e-01],\n",
       "             [ 1.4345e-01, -1.2341e-01, -2.9809e-01,  ..., -2.7742e-01,\n",
       "              -1.0967e+00,  1.4368e+00],\n",
       "             [-1.3090e+00, -7.5311e-01, -8.4933e-01,  ..., -3.0142e-01,\n",
       "              -5.3921e-01, -9.8285e-01],\n",
       "             ...,\n",
       "             [-2.0666e+00,  3.3036e-01, -3.5907e-01,  ...,  1.8929e-01,\n",
       "              -3.3404e+00, -1.5453e+00],\n",
       "             [-1.3719e+00,  3.4992e-01, -5.9277e-01,  ..., -4.4082e-01,\n",
       "              -3.1663e+00, -1.3159e+00],\n",
       "             [-1.9634e+00,  4.2107e-01, -3.5189e-01,  ...,  1.6480e-01,\n",
       "              -3.4214e+00, -1.5606e+00]],\n",
       "  \n",
       "            [[ 2.7973e-01,  6.7818e-01, -1.7939e-01,  ..., -2.2796e-01,\n",
       "              -7.6827e-02, -4.5045e-01],\n",
       "             [ 8.6182e-01, -2.2256e+00,  1.8621e+00,  ...,  1.5628e+00,\n",
       "              -1.3158e+00,  1.3275e+00],\n",
       "             [ 2.3336e+00, -2.0955e-01,  1.7138e-01,  ...,  1.7505e+00,\n",
       "               5.6696e-02,  1.0674e+00],\n",
       "             ...,\n",
       "             [-4.1930e+00, -7.0871e-01,  2.0013e+00,  ...,  1.5869e+00,\n",
       "               2.0522e+00, -4.4835e-01],\n",
       "             [-4.8349e+00, -1.9820e+00,  1.4516e+00,  ...,  1.1793e+00,\n",
       "               1.2066e+00, -5.4100e-01],\n",
       "             [-4.3638e+00, -7.4093e-01,  2.0279e+00,  ...,  1.6074e+00,\n",
       "               2.0619e+00, -5.4973e-01]],\n",
       "  \n",
       "            [[-2.1772e-01, -1.0921e-01, -3.6384e-02,  ...,  1.1472e-01,\n",
       "               1.4872e-01,  7.3530e-02],\n",
       "             [-1.8979e-01,  5.0915e-02, -1.9807e-01,  ..., -1.1196e+00,\n",
       "               1.6558e-01, -3.7173e-01],\n",
       "             [-4.4602e-01,  6.2886e-01,  1.0505e+00,  ...,  3.5230e-01,\n",
       "               1.0677e+00,  1.1299e+00],\n",
       "             ...,\n",
       "             [-1.7302e-02, -6.5697e-01, -1.4642e+00,  ...,  3.7024e-01,\n",
       "              -1.2113e+00,  1.9589e+00],\n",
       "             [ 1.8845e-01, -2.6722e-01, -1.1788e+00,  ...,  8.1768e-01,\n",
       "              -7.9566e-01,  1.1179e+00],\n",
       "             [-4.7589e-03, -7.2274e-01, -1.4505e+00,  ...,  4.2071e-01,\n",
       "              -1.1965e+00,  1.8555e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 2.8610e-02,  1.2853e-02, -1.2269e-01,  ..., -1.1257e-02,\n",
       "               2.9263e-02, -1.5723e-01],\n",
       "             [-2.0106e-01,  2.7768e-01,  2.5367e+00,  ...,  3.7636e-01,\n",
       "              -5.3303e-02, -7.3146e-02],\n",
       "             [-7.5599e-02, -1.8415e-02,  5.8910e-01,  ...,  3.6737e-01,\n",
       "              -3.0556e-01,  3.4870e-01],\n",
       "             ...,\n",
       "             [-2.5798e-01,  6.6433e-01,  4.4798e-02,  ...,  4.2006e-01,\n",
       "               2.4894e-01,  2.7327e-02],\n",
       "             [ 2.0591e-01, -5.9180e-02,  6.9254e-01,  ...,  8.0513e-01,\n",
       "               1.4813e-01, -2.2537e-01],\n",
       "             [-2.3338e-01,  7.0667e-01,  4.6292e-02,  ...,  4.4014e-01,\n",
       "               2.7583e-01,  5.9266e-03]],\n",
       "  \n",
       "            [[ 4.5970e-02, -7.6100e-02, -3.1406e-02,  ...,  4.2137e-03,\n",
       "               2.9127e-02,  8.3232e-02],\n",
       "             [-6.6416e-01,  1.1792e+00,  7.2355e-01,  ...,  6.1842e-01,\n",
       "               1.3291e+00,  1.3557e-01],\n",
       "             [-1.0069e-01,  3.1926e-01,  3.5223e-01,  ..., -3.6035e-01,\n",
       "               6.9251e-01, -4.2601e-01],\n",
       "             ...,\n",
       "             [-6.4514e-03,  4.0228e-01,  6.0551e-01,  ...,  4.1727e-01,\n",
       "              -3.4731e-01, -7.8465e-01],\n",
       "             [-6.6201e-01,  8.0433e-01,  5.1797e-01,  ...,  6.3021e-01,\n",
       "               4.6219e-01,  1.2468e-01],\n",
       "             [ 4.2583e-02,  4.0989e-01,  6.0239e-01,  ...,  3.7964e-01,\n",
       "              -3.5404e-01, -7.1969e-01]],\n",
       "  \n",
       "            [[ 4.0437e-02, -3.9120e-03, -1.8649e-01,  ...,  1.9926e-02,\n",
       "               1.0219e-01,  2.2396e-02],\n",
       "             [ 4.3508e-01,  1.3748e+00, -1.8874e-02,  ...,  7.6127e-01,\n",
       "              -2.5055e-02,  7.8726e-01],\n",
       "             [ 5.1185e-01,  1.2991e+00, -6.2457e-03,  ...,  5.4653e-01,\n",
       "               5.6407e-01, -9.0313e-01],\n",
       "             ...,\n",
       "             [-2.9639e-01,  4.4766e-01, -8.5618e-01,  ..., -4.7889e-01,\n",
       "               4.4166e-01,  9.8398e-02],\n",
       "             [ 6.7280e-02,  9.1599e-02, -3.8354e-01,  ...,  7.7066e-01,\n",
       "              -1.1416e-02,  7.9639e-01],\n",
       "             [-2.6635e-01,  4.1709e-01, -8.4446e-01,  ..., -4.8175e-01,\n",
       "               4.1025e-01,  7.1735e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.9750e-01,  6.8529e-03, -1.4536e-02,  ...,  3.4890e-03,\n",
       "               1.0463e-01,  7.3523e-02],\n",
       "             [ 1.1149e+00,  8.2626e-01,  1.5518e-02,  ...,  9.0221e-01,\n",
       "              -4.6150e-01, -7.2491e-01],\n",
       "             [-7.0829e-01,  3.8576e-02,  4.5917e-01,  ...,  1.3060e-01,\n",
       "              -4.5200e-01,  6.5450e-02],\n",
       "             ...,\n",
       "             [ 1.6055e-01, -4.4959e-02, -6.3658e-01,  ..., -3.9343e-02,\n",
       "               7.7898e-02,  4.1651e-02],\n",
       "             [ 7.6647e-01,  3.1331e-01, -7.0777e-01,  ...,  2.7662e-01,\n",
       "              -3.3727e-01, -7.9312e-01],\n",
       "             [ 2.2596e-01,  2.5328e-02, -6.5446e-01,  ..., -3.9717e-02,\n",
       "               6.9850e-02,  4.4686e-02]],\n",
       "  \n",
       "            [[-9.8877e-02, -4.6599e-03, -3.8561e-03,  ...,  1.1805e-01,\n",
       "               9.6648e-02, -5.6808e-02],\n",
       "             [ 5.3670e-01, -5.6381e-01,  4.5461e-01,  ...,  5.7934e-01,\n",
       "               8.2111e-02, -4.6666e-01],\n",
       "             [-9.4565e-01, -5.2629e-02, -2.8324e-02,  ...,  6.6512e-01,\n",
       "               2.9557e-01, -4.9475e-01],\n",
       "             ...,\n",
       "             [ 2.4102e-01, -9.1359e-01,  1.8441e-01,  ...,  4.5317e-01,\n",
       "              -1.4818e-01, -7.8914e-01],\n",
       "             [ 4.9549e-01, -1.6623e-01,  3.3844e-01,  ...,  4.1715e-01,\n",
       "               2.2420e-01, -3.4012e-01],\n",
       "             [ 2.2019e-01, -9.0551e-01,  1.9570e-01,  ...,  4.5624e-01,\n",
       "              -1.2058e-01, -7.7331e-01]],\n",
       "  \n",
       "            [[ 1.3056e-01,  2.2470e-02,  5.5132e-02,  ..., -9.6777e-03,\n",
       "               4.2867e-02,  8.0464e-02],\n",
       "             [-7.1081e-01, -5.3793e-01,  7.0893e-02,  ...,  1.6059e+00,\n",
       "               8.8559e-01,  1.6009e+00],\n",
       "             [-7.5900e-01,  3.2864e-01,  2.9079e-01,  ...,  6.1201e-01,\n",
       "               1.7177e-01, -4.9661e-01],\n",
       "             ...,\n",
       "             [ 2.3875e-01,  7.7463e-01, -8.4165e-01,  ..., -3.9675e-01,\n",
       "              -4.2066e-01, -5.9187e-02],\n",
       "             [ 6.4349e-02,  3.0666e-01, -4.1727e-01,  ...,  6.6901e-01,\n",
       "               3.8666e-01,  4.8330e-01],\n",
       "             [ 1.9610e-01,  7.6599e-01, -8.8106e-01,  ..., -3.4469e-01,\n",
       "              -4.2299e-01, -3.2146e-02]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-8.0509e-01, -1.5786e-01,  4.0455e-01,  ...,  5.1511e-02,\n",
       "               7.3780e-01, -2.3081e-01],\n",
       "             [ 2.0069e+00, -1.2103e+00, -3.2134e-01,  ...,  2.0463e+00,\n",
       "              -1.7248e+00, -8.9961e-02],\n",
       "             [ 1.1574e+00, -1.6836e+00,  2.9710e-03,  ...,  1.5643e+00,\n",
       "              -8.6848e-01, -2.4166e-01],\n",
       "             ...,\n",
       "             [-2.2699e+00, -1.2972e+00, -4.7365e+00,  ..., -1.1557e+00,\n",
       "              -1.4084e+00,  4.0186e+00],\n",
       "             [-2.7211e+00, -1.4570e+00, -4.5784e+00,  ..., -8.5334e-02,\n",
       "              -2.4154e+00,  3.2588e+00],\n",
       "             [-2.3536e+00, -1.3064e+00, -4.8190e+00,  ..., -1.1959e+00,\n",
       "              -1.4213e+00,  4.0587e+00]],\n",
       "  \n",
       "            [[-9.3736e-01, -5.2172e-02,  3.7902e-01,  ...,  2.6934e-01,\n",
       "              -7.0688e-01,  4.3571e-01],\n",
       "             [-4.7712e-01, -1.7614e+00,  1.4042e+00,  ..., -1.9707e+00,\n",
       "               1.1611e+00, -1.2010e+00],\n",
       "             [ 1.8771e+00,  1.2277e+00, -8.7057e-01,  ..., -1.0569e-01,\n",
       "               2.4207e-01, -1.4343e+00],\n",
       "             ...,\n",
       "             [ 2.3636e+00,  1.5779e+00,  1.3397e+00,  ...,  3.4857e+00,\n",
       "               7.2268e-01,  1.8681e+00],\n",
       "             [ 1.8394e+00,  4.1946e-01,  1.4512e+00,  ...,  1.6969e+00,\n",
       "               1.3930e+00,  1.2559e+00],\n",
       "             [ 2.3054e+00,  1.4767e+00,  1.3321e+00,  ...,  3.3702e+00,\n",
       "               7.6097e-01,  1.8429e+00]],\n",
       "  \n",
       "            [[ 1.2431e-01, -1.9596e-01, -3.7100e-01,  ..., -2.9573e-01,\n",
       "               1.2071e-01, -1.8707e-01],\n",
       "             [-6.1152e-01, -1.1212e+00,  7.7419e-01,  ..., -7.6331e-02,\n",
       "              -1.2396e+00,  2.1420e+00],\n",
       "             [ 1.0026e+00, -8.8863e-01,  1.1224e+00,  ...,  4.6487e-01,\n",
       "               1.1094e+00,  1.5170e+00],\n",
       "             ...,\n",
       "             [ 2.7676e+00,  9.1408e-01,  4.2194e-01,  ...,  2.1513e+00,\n",
       "              -1.1351e+00,  4.1803e+00],\n",
       "             [ 2.0050e+00,  6.5823e-01,  5.1451e-01,  ...,  2.4423e+00,\n",
       "              -2.2873e+00,  4.5584e+00],\n",
       "             [ 2.7651e+00,  9.9362e-01,  3.5207e-01,  ...,  2.1813e+00,\n",
       "              -1.2712e+00,  4.2558e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.1101e+00, -1.4262e+00,  1.3988e+00,  ...,  1.5434e+00,\n",
       "               4.4440e-01,  1.8507e+00],\n",
       "             [ 4.2952e-01,  6.6493e-01, -4.2849e-01,  ...,  1.0792e+00,\n",
       "               1.2027e+00,  1.5726e+00],\n",
       "             [-1.9389e-01, -8.9660e-01,  1.8397e+00,  ..., -1.0483e+00,\n",
       "              -7.0499e-01,  1.8053e+00],\n",
       "             ...,\n",
       "             [-5.1135e+00, -7.6571e-01,  1.1407e+00,  ..., -2.9861e+00,\n",
       "               2.8615e+00,  1.0116e+00],\n",
       "             [-6.4364e+00,  2.7259e-01,  6.3638e-01,  ..., -1.7919e+00,\n",
       "               4.2762e+00,  7.6588e-01],\n",
       "             [-5.3074e+00, -7.0848e-01,  1.2337e+00,  ..., -2.9743e+00,\n",
       "               2.9812e+00,  9.4407e-01]],\n",
       "  \n",
       "            [[-2.6173e-01,  1.5671e-01, -2.8757e-01,  ..., -3.8101e-01,\n",
       "              -2.0463e-01,  9.5139e-02],\n",
       "             [-1.8710e+00, -7.0214e-01,  1.0967e+00,  ...,  3.2663e-01,\n",
       "              -1.6948e-02,  9.2062e-01],\n",
       "             [-1.9617e+00, -4.0355e-01, -1.0796e-01,  ..., -5.0139e-01,\n",
       "              -1.1405e-01,  1.2661e-01],\n",
       "             ...,\n",
       "             [ 1.1409e+00, -2.0893e+00, -2.8737e+00,  ..., -4.1695e+00,\n",
       "              -5.0880e+00, -3.1322e+00],\n",
       "             [ 4.7062e-01, -1.8036e+00, -1.9398e+00,  ..., -3.6469e+00,\n",
       "              -4.8252e+00, -2.7708e+00],\n",
       "             [ 1.1975e+00, -2.1957e+00, -2.9622e+00,  ..., -4.2747e+00,\n",
       "              -5.1553e+00, -3.2145e+00]],\n",
       "  \n",
       "            [[-1.7156e-01, -5.8267e-02,  7.7460e-01,  ...,  3.0813e-01,\n",
       "               7.6626e-01, -1.3414e-01],\n",
       "             [-1.4913e+00,  4.0281e-01, -1.0599e+00,  ...,  1.0718e+00,\n",
       "              -1.2053e+00,  1.2962e-02],\n",
       "             [-1.0439e+00, -1.8174e+00,  1.0110e+00,  ...,  1.6480e+00,\n",
       "              -2.0945e+00, -1.0982e+00],\n",
       "             ...,\n",
       "             [-2.2487e+00,  2.8088e+00,  8.0010e+00,  ...,  6.3947e-02,\n",
       "              -4.6936e+00,  1.1263e+00],\n",
       "             [-2.5354e+00,  3.3772e+00,  7.4754e+00,  ...,  5.1553e-01,\n",
       "              -4.7355e+00,  1.6568e+00],\n",
       "             [-2.3555e+00,  2.9145e+00,  8.1140e+00,  ..., -7.9245e-02,\n",
       "              -4.6512e+00,  1.2580e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 5.8372e-02,  3.1311e-02, -1.4783e-01,  ...,  1.1438e-01,\n",
       "               1.3060e-02,  2.3378e-01],\n",
       "             [ 8.4837e-01,  2.4797e-01, -1.5988e+00,  ..., -1.0958e+00,\n",
       "               5.3335e-01, -8.2253e-02],\n",
       "             [-1.1586e-01, -1.2235e-01, -8.2173e-01,  ..., -8.7066e-01,\n",
       "               5.5435e-01,  3.5810e-01],\n",
       "             ...,\n",
       "             [ 1.4167e-01,  5.4759e-01, -2.4479e-01,  ..., -5.1462e-01,\n",
       "               3.6379e-01,  5.7067e-01],\n",
       "             [ 4.0622e-01,  5.5799e-01, -4.2934e-01,  ..., -8.4903e-01,\n",
       "               4.0601e-01,  3.4820e-01],\n",
       "             [ 1.3251e-01,  5.3892e-01, -1.9960e-01,  ..., -4.7376e-01,\n",
       "               3.4650e-01,  5.9987e-01]],\n",
       "  \n",
       "            [[ 5.3258e-02,  6.8870e-02,  1.0893e-01,  ...,  5.2471e-02,\n",
       "               2.4778e-02, -1.1068e-01],\n",
       "             [-4.8206e-01, -7.2946e-01,  3.7010e-01,  ..., -1.4647e-01,\n",
       "               1.0994e+00, -5.5253e-01],\n",
       "             [ 1.0664e-01,  6.6453e-01, -1.1115e-01,  ...,  1.1596e+00,\n",
       "              -1.2741e+00, -5.4519e-01],\n",
       "             ...,\n",
       "             [-2.2164e-01,  1.0101e+00,  1.4202e-01,  ...,  3.0003e-01,\n",
       "               5.4102e-01, -1.6095e-01],\n",
       "             [-4.7768e-01,  3.5226e-01,  6.7886e-01,  ..., -3.2422e-01,\n",
       "               6.7535e-01, -1.3981e+00],\n",
       "             [-2.1656e-01,  9.7024e-01,  1.8375e-01,  ...,  2.6772e-01,\n",
       "               5.7848e-01, -1.5512e-01]],\n",
       "  \n",
       "            [[-3.3564e-02, -1.6812e-02, -2.7630e-02,  ..., -2.0661e-01,\n",
       "              -2.0624e-02,  1.5071e-01],\n",
       "             [-1.8632e-02,  2.6415e+00, -9.8896e-01,  ..., -2.5421e-01,\n",
       "              -2.5654e-01, -1.0942e+00],\n",
       "             [ 4.3388e-01,  1.3108e+00, -8.4434e-01,  ..., -9.1371e-01,\n",
       "               6.9896e-01, -4.4035e-01],\n",
       "             ...,\n",
       "             [-2.0192e-01,  4.9667e-01, -5.9729e-02,  ..., -2.8124e-01,\n",
       "               4.8397e-01, -1.0883e+00],\n",
       "             [-4.9207e-01,  8.5241e-01,  2.2433e-01,  ..., -1.7091e-01,\n",
       "              -8.6071e-02, -1.4243e+00],\n",
       "             [-2.1454e-01,  4.5218e-01, -6.6088e-02,  ..., -2.8687e-01,\n",
       "               4.1840e-01, -1.0410e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 3.5601e-02, -8.6612e-02,  4.3746e-02,  ..., -1.2505e-01,\n",
       "               1.2904e-02, -6.3012e-02],\n",
       "             [ 2.0843e-01,  1.2784e+00, -1.0727e-01,  ...,  4.7901e-03,\n",
       "              -2.2344e-01, -8.6769e-01],\n",
       "             [ 2.0866e-01,  2.9216e-01,  1.1828e+00,  ...,  1.3029e-01,\n",
       "              -7.2860e-01, -5.2975e-01],\n",
       "             ...,\n",
       "             [ 8.6252e-01,  1.8769e-01, -3.4279e-02,  ...,  2.3206e-01,\n",
       "              -1.0320e-01, -2.3436e-01],\n",
       "             [-5.9651e-03,  3.8795e-01,  4.9396e-01,  ..., -6.1235e-01,\n",
       "               5.2938e-01, -1.1389e+00],\n",
       "             [ 7.7963e-01,  2.0020e-01, -3.7951e-02,  ...,  2.3512e-01,\n",
       "              -1.3809e-01, -2.2619e-01]],\n",
       "  \n",
       "            [[-8.3317e-02, -1.1641e-01,  1.3257e-01,  ..., -2.0688e-02,\n",
       "              -4.3591e-02,  1.5613e-01],\n",
       "             [-8.7765e-01, -1.8110e-01,  9.0772e-04,  ...,  3.4550e-01,\n",
       "               4.3784e-01, -8.5332e-01],\n",
       "             [-5.8751e-01,  4.5552e-01, -1.4623e-01,  ..., -3.3811e-01,\n",
       "               7.7644e-01, -1.4031e-02],\n",
       "             ...,\n",
       "             [-7.8987e-01,  2.7637e-01, -1.8619e-01,  ...,  7.2958e-01,\n",
       "               3.9757e-01, -2.3940e-01],\n",
       "             [-8.4212e-01,  2.0331e-02,  1.6109e-01,  ..., -9.0122e-02,\n",
       "               8.0954e-01, -2.9008e-01],\n",
       "             [-7.9814e-01,  3.0385e-01, -1.7667e-01,  ...,  7.3262e-01,\n",
       "               3.8308e-01, -1.7138e-01]],\n",
       "  \n",
       "            [[-1.0493e-01, -6.0311e-02, -1.7198e-02,  ...,  1.2304e-01,\n",
       "              -1.2854e-01,  2.3725e-03],\n",
       "             [ 1.2170e-02,  8.6749e-01,  1.5740e+00,  ...,  1.1157e+00,\n",
       "               3.6348e-01,  2.1335e-01],\n",
       "             [-4.7504e-01, -3.1139e-01, -2.4286e-02,  ...,  2.8995e-01,\n",
       "               9.6717e-01, -8.7367e-02],\n",
       "             ...,\n",
       "             [-1.6668e-01, -3.2424e-01,  5.3034e-02,  ..., -1.2974e-01,\n",
       "               1.0413e-01,  2.8462e-01],\n",
       "             [-1.8066e-01,  7.1741e-01,  7.3482e-01,  ...,  3.4216e-01,\n",
       "              -5.0389e-01, -6.7674e-01],\n",
       "             [-1.5682e-01, -3.5008e-01,  2.5355e-02,  ..., -9.4031e-02,\n",
       "               7.2551e-02,  1.9553e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-3.5626e-02,  9.6058e-02,  2.6860e-02,  ..., -8.9990e-02,\n",
       "              -1.9404e-01,  1.8616e-01],\n",
       "             [-1.9478e-01,  1.2086e+00,  2.5259e+00,  ..., -1.0809e+00,\n",
       "              -2.8474e+00,  1.1295e+00],\n",
       "             [-6.7777e-01,  2.0095e+00,  1.1494e+00,  ..., -6.2736e-01,\n",
       "              -7.5870e-01,  1.4799e+00],\n",
       "             ...,\n",
       "             [-1.7186e+00,  3.9500e+00, -1.8548e+00,  ..., -1.7138e+00,\n",
       "               8.2255e-01,  1.2757e+00],\n",
       "             [-9.1280e-01,  3.4560e+00, -1.8048e-01,  ..., -2.4396e+00,\n",
       "               7.2674e-01,  2.0220e+00],\n",
       "             [-1.6321e+00,  4.0120e+00, -1.9003e+00,  ..., -1.7738e+00,\n",
       "               7.9802e-01,  1.2192e+00]],\n",
       "  \n",
       "            [[ 1.4859e-01,  4.1946e-01, -2.2613e+00,  ..., -5.9651e-01,\n",
       "              -1.5599e-02, -9.8328e-02],\n",
       "             [ 8.3303e-02,  1.3943e+00,  3.4867e+00,  ...,  4.4353e-01,\n",
       "              -1.2919e+00,  8.0335e-01],\n",
       "             [ 1.5714e+00,  1.1583e+00,  2.1803e+00,  ...,  1.0743e+00,\n",
       "              -6.1733e-01,  7.0874e-01],\n",
       "             ...,\n",
       "             [ 5.2559e-01, -1.1066e+00, -4.1979e+00,  ...,  1.3285e+00,\n",
       "              -1.4407e+00, -1.0478e+00],\n",
       "             [ 3.3856e-01, -1.2271e+00, -3.6175e+00,  ...,  1.3964e+00,\n",
       "              -1.5940e+00,  1.1369e-01],\n",
       "             [ 5.3394e-01, -1.1370e+00, -4.4529e+00,  ...,  1.3289e+00,\n",
       "              -1.4031e+00, -1.0683e+00]],\n",
       "  \n",
       "            [[-1.0476e+00,  7.8033e-01,  3.7009e-01,  ..., -7.8949e-01,\n",
       "              -1.7861e-01, -3.5614e-01],\n",
       "             [-4.4819e-01, -2.3335e+00, -3.8445e-01,  ...,  3.4558e-01,\n",
       "               8.7486e-01,  1.2523e+00],\n",
       "             [ 3.9366e-01, -2.0292e+00,  2.0745e-01,  ...,  1.0650e+00,\n",
       "              -1.4602e-01,  1.7951e+00],\n",
       "             ...,\n",
       "             [-1.4020e+00, -4.2545e+00,  2.2487e+00,  ..., -1.4094e+00,\n",
       "              -4.9603e-01, -3.9276e+00],\n",
       "             [-1.7254e+00, -4.2003e+00,  1.7347e+00,  ..., -2.0224e+00,\n",
       "               1.0840e-02, -3.5232e+00],\n",
       "             [-1.4327e+00, -4.3681e+00,  2.2557e+00,  ..., -1.5592e+00,\n",
       "              -4.9671e-01, -4.0285e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-4.5795e-01, -1.7472e-01, -7.6552e-01,  ..., -2.3013e-02,\n",
       "               4.5836e-01,  3.8976e-01],\n",
       "             [-1.4109e-01, -1.3885e+00,  1.5742e+00,  ..., -1.4984e+00,\n",
       "              -1.7863e-01,  1.8931e+00],\n",
       "             [ 3.5257e-02,  7.6608e-01,  1.1566e+00,  ..., -2.7326e+00,\n",
       "               7.7622e-01,  1.4166e+00],\n",
       "             ...,\n",
       "             [-2.4759e+00, -2.0726e+00,  1.4003e+00,  ..., -4.5185e+00,\n",
       "               1.4408e-01, -6.7974e-01],\n",
       "             [-1.8249e+00, -3.9657e+00,  4.7548e-01,  ..., -4.5437e+00,\n",
       "              -1.1691e+00, -5.9599e-01],\n",
       "             [-2.5894e+00, -2.2503e+00,  1.3842e+00,  ..., -4.4890e+00,\n",
       "               1.0650e-01, -6.8354e-01]],\n",
       "  \n",
       "            [[ 2.8074e-01,  9.7224e-01,  2.6496e-01,  ...,  1.9854e-02,\n",
       "              -1.1354e-01,  1.5440e-01],\n",
       "             [ 2.9735e-01, -8.5662e-01, -1.7143e-01,  ..., -4.2318e-01,\n",
       "               2.3457e+00,  1.4826e+00],\n",
       "             [ 4.0542e-01, -1.6285e-01, -1.6692e+00,  ...,  1.7143e+00,\n",
       "              -9.8580e-01,  2.0312e+00],\n",
       "             ...,\n",
       "             [-9.2772e-01, -1.4200e+00,  2.5189e-01,  ..., -7.2397e-01,\n",
       "              -1.5358e+00,  4.7808e-01],\n",
       "             [-2.3484e-01, -7.2803e-01,  9.8886e-02,  ..., -1.3927e+00,\n",
       "               4.9278e-01,  3.0268e-01],\n",
       "             [-9.3088e-01, -1.3284e+00,  3.6368e-01,  ..., -7.9778e-01,\n",
       "              -1.5207e+00,  4.1118e-01]],\n",
       "  \n",
       "            [[-8.6930e-02,  2.1569e-02,  7.8588e-02,  ..., -8.8409e-02,\n",
       "               2.9754e-01,  2.5058e-01],\n",
       "             [-7.8535e-01,  1.4004e+00, -7.1586e-02,  ...,  5.8438e-01,\n",
       "              -1.3186e+00, -3.3546e-01],\n",
       "             [ 3.1227e-01,  2.2924e-01,  3.0289e-01,  ...,  3.2061e-01,\n",
       "              -5.5615e-01,  1.3075e+00],\n",
       "             ...,\n",
       "             [ 1.0487e+00, -5.8908e-01, -9.7855e-01,  ...,  2.8311e+00,\n",
       "               2.3205e+00,  3.5746e+00],\n",
       "             [ 2.9843e-01,  5.6715e-01, -7.6179e-01,  ...,  2.3176e+00,\n",
       "               2.0353e+00,  3.0026e+00],\n",
       "             [ 1.0540e+00, -5.5265e-01, -8.9185e-01,  ...,  2.8961e+00,\n",
       "               2.4416e+00,  3.6135e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 1.8299e-01, -7.4142e-02, -7.0574e-02,  ...,  5.6379e-02,\n",
       "              -7.3365e-02, -4.6098e-02],\n",
       "             [ 1.3737e+00,  1.5504e+00,  4.2807e-01,  ..., -6.1071e-01,\n",
       "               6.7113e-01,  6.9169e-01],\n",
       "             [ 1.9625e+00, -4.4112e-01, -9.7198e-01,  ..., -9.5132e-01,\n",
       "              -1.0951e+00, -5.1565e-01],\n",
       "             ...,\n",
       "             [ 5.5925e-01, -3.1237e-02, -9.9782e-01,  ...,  1.0609e-01,\n",
       "              -1.6642e+00, -5.4093e-01],\n",
       "             [ 2.9720e-01,  9.5041e-01,  6.8391e-03,  ..., -5.8683e-02,\n",
       "              -1.4329e-01,  1.0249e-01],\n",
       "             [ 5.0838e-01,  2.2538e-02, -9.5318e-01,  ...,  9.3157e-02,\n",
       "              -1.6114e+00, -4.9090e-01]],\n",
       "  \n",
       "            [[-9.6329e-02,  1.6889e-02,  3.2881e-02,  ..., -4.6500e-02,\n",
       "               1.3689e-01,  1.4687e-02],\n",
       "             [ 1.9450e+00, -4.7742e-02,  1.2742e+00,  ...,  4.0864e-02,\n",
       "              -1.5111e+00,  2.9228e-02],\n",
       "             [ 9.9490e-01, -2.9075e-01,  2.7348e-01,  ...,  1.2490e+00,\n",
       "              -1.4511e+00, -8.1816e-01],\n",
       "             ...,\n",
       "             [ 1.3516e+00, -2.2480e-02,  2.8711e-01,  ..., -2.5484e-01,\n",
       "              -9.3909e-01, -7.5213e-01],\n",
       "             [ 1.7167e+00, -6.5455e-01,  5.1284e-01,  ...,  6.2249e-02,\n",
       "              -7.6691e-01, -1.0805e+00],\n",
       "             [ 1.3261e+00, -2.6576e-02,  2.4561e-01,  ..., -2.9411e-01,\n",
       "              -9.1086e-01, -7.8877e-01]],\n",
       "  \n",
       "            [[ 8.6372e-02, -1.3211e-01,  9.9665e-02,  ...,  1.7509e-03,\n",
       "              -1.4730e-02, -4.9049e-02],\n",
       "             [-7.9739e-01,  9.1485e-02, -1.3586e-01,  ...,  6.1312e-01,\n",
       "              -7.3886e-01, -2.3035e+00],\n",
       "             [-4.8863e-01,  6.2138e-01,  3.5179e-01,  ..., -4.8970e-01,\n",
       "              -6.8810e-01, -1.2448e+00],\n",
       "             ...,\n",
       "             [ 7.0990e-02, -3.7396e-02,  4.5078e-01,  ..., -6.8112e-01,\n",
       "              -2.5030e-01, -6.7158e-01],\n",
       "             [-2.6501e-01, -1.5887e-01, -3.6186e-01,  ..., -4.9099e-01,\n",
       "              -5.7055e-01, -1.5924e+00],\n",
       "             [ 9.8731e-02, -2.6620e-02,  4.2697e-01,  ..., -6.7852e-01,\n",
       "              -3.0273e-01, -6.7067e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.1563e-01,  7.7200e-02,  2.8498e-02,  ..., -9.3503e-02,\n",
       "              -1.6608e-02,  8.3871e-02],\n",
       "             [ 2.7667e-01,  2.0680e-01, -4.1773e-01,  ...,  1.9115e-01,\n",
       "              -1.1667e+00, -1.3154e+00],\n",
       "             [ 7.5924e-01, -2.7966e-01, -9.2384e-02,  ..., -1.0054e+00,\n",
       "              -4.3723e-01, -4.4194e-01],\n",
       "             ...,\n",
       "             [ 9.6144e-01,  4.1668e-01, -6.2466e-01,  ..., -4.5936e-01,\n",
       "              -5.4238e-01, -5.5113e-01],\n",
       "             [ 3.8808e-01,  3.5090e-03, -1.8062e+00,  ...,  4.4019e-01,\n",
       "              -4.4628e-01, -7.7932e-01],\n",
       "             [ 8.9202e-01,  3.8933e-01, -6.4758e-01,  ..., -4.9424e-01,\n",
       "              -4.9389e-01, -5.1038e-01]],\n",
       "  \n",
       "            [[-7.9168e-02,  5.1150e-02,  7.2856e-02,  ...,  1.0386e-01,\n",
       "               8.3135e-02, -1.1139e-01],\n",
       "             [-1.1810e+00, -5.0970e-01, -4.6497e-01,  ..., -3.2378e+00,\n",
       "              -4.2474e-01, -1.1595e-01],\n",
       "             [ 4.7672e-02, -3.7796e-01,  5.4111e-01,  ...,  7.2553e-01,\n",
       "               7.5427e-01, -1.9289e-01],\n",
       "             ...,\n",
       "             [-3.7523e-02,  3.5921e-01,  6.7582e-01,  ...,  7.2582e-01,\n",
       "               4.6708e-01, -3.8831e-01],\n",
       "             [-2.4542e-01, -4.6032e-01,  3.3361e-01,  ..., -1.9231e+00,\n",
       "              -3.2129e-01, -7.4452e-02],\n",
       "             [ 1.3230e-02,  3.5396e-01,  6.9046e-01,  ...,  6.3652e-01,\n",
       "               4.6656e-01, -3.6635e-01]],\n",
       "  \n",
       "            [[ 7.2860e-02,  1.9324e-01,  1.8858e-01,  ...,  1.1595e-02,\n",
       "               1.3806e-02, -1.3101e-01],\n",
       "             [ 9.3098e-02,  3.1130e-01,  7.3523e-01,  ..., -1.0186e+00,\n",
       "              -6.8469e-02,  5.8171e-01],\n",
       "             [-9.7061e-02,  4.9559e-01,  8.4866e-01,  ...,  6.7414e-02,\n",
       "              -1.7660e+00,  9.1924e-02],\n",
       "             ...,\n",
       "             [-3.2793e-01,  6.8008e-01,  9.2097e-01,  ...,  1.2371e-01,\n",
       "              -2.8955e-01,  4.2327e-02],\n",
       "             [-5.7163e-01,  4.5604e-01,  4.4431e-01,  ...,  4.0308e-01,\n",
       "               7.0040e-02, -7.2576e-01],\n",
       "             [-2.9994e-01,  7.0878e-01,  9.1150e-01,  ...,  1.3040e-01,\n",
       "              -3.1020e-01,  7.5951e-02]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 8.9367e-02, -7.8190e-01,  4.0781e-03,  ..., -6.3994e-02,\n",
       "              -3.4035e-01,  1.1557e-04],\n",
       "             [ 4.8697e-01,  8.7834e-01, -5.9861e-01,  ..., -1.1025e+00,\n",
       "               8.5854e-01,  1.7138e+00],\n",
       "             [-1.2371e-01,  6.2749e-01, -1.7620e-01,  ..., -3.3778e-01,\n",
       "              -6.9913e-01,  2.1432e-01],\n",
       "             ...,\n",
       "             [-1.1898e+00, -1.2330e+00,  1.2196e-01,  ..., -1.4832e+00,\n",
       "               1.3773e+00,  1.3300e+00],\n",
       "             [-1.1495e+00, -9.4264e-01, -6.8237e-01,  ..., -2.1637e+00,\n",
       "               1.5807e+00,  2.3811e+00],\n",
       "             [-1.1984e+00, -1.3661e+00,  1.2195e-01,  ..., -1.4987e+00,\n",
       "               1.3976e+00,  1.3283e+00]],\n",
       "  \n",
       "            [[-1.1463e-01,  1.5414e-01, -3.3556e-01,  ..., -1.2091e-01,\n",
       "              -6.0446e-02,  1.5057e+00],\n",
       "             [-2.0503e+00, -1.6268e+00, -8.0584e-03,  ...,  5.7306e-01,\n",
       "              -2.2473e-02, -2.4654e+00],\n",
       "             [-1.1604e+00, -8.0109e-02,  1.2182e+00,  ..., -2.7711e-01,\n",
       "               2.9322e-01, -3.7471e+00],\n",
       "             ...,\n",
       "             [-2.2172e+00,  4.4815e-01,  3.4076e+00,  ...,  3.1115e-01,\n",
       "              -4.0088e-01,  5.4946e+00],\n",
       "             [-2.7808e+00, -1.3333e+00,  2.5807e+00,  ...,  5.9998e-01,\n",
       "              -1.3772e-01,  5.6201e+00],\n",
       "             [-2.1908e+00,  4.5444e-01,  3.4168e+00,  ...,  2.7999e-01,\n",
       "              -4.5920e-01,  5.8262e+00]],\n",
       "  \n",
       "            [[ 7.0791e-02, -5.5965e-01, -2.2243e-02,  ...,  4.8049e-01,\n",
       "               5.3726e-01, -1.5731e-01],\n",
       "             [-8.5409e-01,  1.7755e+00, -2.9613e-01,  ..., -1.0768e+00,\n",
       "               1.1004e-01,  6.1195e-01],\n",
       "             [-5.3527e-01, -7.5270e-01, -1.5838e-01,  ...,  6.5029e-04,\n",
       "               5.7236e-01, -4.0013e-01],\n",
       "             ...,\n",
       "             [-3.7295e+00,  5.0293e-01, -1.8564e+00,  ...,  4.3107e+00,\n",
       "               7.7906e+00, -1.5311e+00],\n",
       "             [-4.3253e+00,  1.8817e+00, -1.8383e+00,  ...,  1.9279e+00,\n",
       "               7.9243e+00, -1.8689e+00],\n",
       "             [-3.6699e+00,  5.5439e-01, -1.8270e+00,  ...,  4.3574e+00,\n",
       "               7.9080e+00, -1.6615e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.3915e-01,  2.1767e-01, -6.5029e-01,  ...,  9.7640e-01,\n",
       "               6.4497e-01, -1.0495e+00],\n",
       "             [ 9.5624e-01,  1.6763e+00,  1.1208e+00,  ..., -1.3697e+00,\n",
       "              -8.4024e-01,  3.0501e+00],\n",
       "             [-7.7095e-01, -4.2066e-01,  2.7154e-01,  ..., -2.9740e+00,\n",
       "              -6.2394e-01,  2.0342e+00],\n",
       "             ...,\n",
       "             [ 1.2427e+00, -6.2246e+00, -1.7313e+00,  ...,  2.9262e+00,\n",
       "              -5.5176e-01,  1.9014e+00],\n",
       "             [ 2.4950e+00, -5.2072e+00, -2.0803e+00,  ...,  3.2605e+00,\n",
       "              -1.2203e+00,  3.4013e+00],\n",
       "             [ 1.2940e+00, -6.3190e+00, -1.7960e+00,  ...,  3.1776e+00,\n",
       "              -5.6973e-01,  1.8997e+00]],\n",
       "  \n",
       "            [[ 5.7570e-01,  7.4366e-01,  6.8049e-01,  ...,  1.1685e+00,\n",
       "              -4.2434e-01,  7.9872e-01],\n",
       "             [ 1.6935e+00, -1.6617e+00, -1.2794e+00,  ..., -6.9484e-01,\n",
       "              -2.1966e-01, -5.1784e-01],\n",
       "             [ 2.1669e+00, -7.9123e-01, -3.3049e-01,  ...,  9.4472e-01,\n",
       "               4.2394e-01, -1.8480e-01],\n",
       "             ...,\n",
       "             [ 1.9234e+00, -2.4876e+00, -3.0990e+00,  ...,  6.9780e-01,\n",
       "              -1.1071e+00, -2.2631e+00],\n",
       "             [ 1.5725e+00, -3.5509e+00, -3.8835e+00,  ..., -7.6856e-01,\n",
       "              -1.8001e-01, -2.1097e+00],\n",
       "             [ 1.8032e+00, -2.5010e+00, -3.1748e+00,  ...,  6.4680e-01,\n",
       "              -1.1083e+00, -2.2558e+00]],\n",
       "  \n",
       "            [[ 1.0457e-01, -1.8363e-01,  1.3462e-01,  ..., -1.2287e-01,\n",
       "               1.0313e-01, -2.4325e-01],\n",
       "             [ 1.9624e-02,  6.7755e-01,  9.9019e-01,  ..., -4.9596e-01,\n",
       "              -4.3477e-01,  8.3797e-01],\n",
       "             [-4.9421e-01, -1.6213e-01,  6.3992e-01,  ...,  2.0407e-01,\n",
       "              -6.4433e-01, -1.7984e-01],\n",
       "             ...,\n",
       "             [-5.2600e-01, -1.1804e-01,  8.1786e-02,  ...,  6.4444e+00,\n",
       "              -2.6129e+00, -5.5552e+00],\n",
       "             [-4.7832e-02,  9.1389e-01, -9.5729e-02,  ...,  6.6711e+00,\n",
       "              -2.5463e+00, -5.3890e+00],\n",
       "             [-5.1681e-01, -1.5662e-01,  5.9885e-02,  ...,  6.5897e+00,\n",
       "              -2.6515e+00, -5.5742e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-1.7617e-02,  1.9100e-02,  7.4215e-02,  ...,  5.5277e-03,\n",
       "              -3.7850e-02,  7.7476e-02],\n",
       "             [-7.0967e-01,  7.4358e-01,  1.7536e+00,  ...,  9.0167e-01,\n",
       "               6.8327e-01, -1.9166e+00],\n",
       "             [-5.1703e-01,  1.5444e+00, -1.0815e+00,  ...,  5.8523e-01,\n",
       "               7.1184e-01, -6.0517e-01],\n",
       "             ...,\n",
       "             [-5.0766e-01,  3.9992e-01, -6.2626e-01,  ...,  5.6122e-01,\n",
       "               2.6277e-01,  3.5103e-01],\n",
       "             [-8.5739e-01,  2.7689e-01, -1.8926e-01,  ...,  1.0571e+00,\n",
       "               2.1564e-01, -1.1946e-01],\n",
       "             [-4.8577e-01,  3.5587e-01, -6.6658e-01,  ...,  5.6549e-01,\n",
       "               2.4234e-01,  3.8590e-01]],\n",
       "  \n",
       "            [[ 3.3914e-02, -8.8856e-02, -5.5313e-03,  ...,  1.1858e-01,\n",
       "               1.3372e-01,  4.0809e-02],\n",
       "             [-4.7111e-01,  7.0224e-01,  4.9681e-01,  ...,  1.3935e-01,\n",
       "              -7.9133e-01,  1.1294e+00],\n",
       "             [-8.0564e-01,  5.3137e-01,  2.7531e-01,  ..., -5.9608e-01,\n",
       "               3.3773e-02, -7.8056e-01],\n",
       "             ...,\n",
       "             [-2.2093e-01,  2.4333e-01, -4.2353e-01,  ..., -3.3910e-01,\n",
       "               6.4202e-01, -3.8438e-01],\n",
       "             [-6.4428e-01,  2.7128e-01, -1.1813e-01,  ..., -1.7804e-01,\n",
       "              -2.8194e-02,  3.4588e-01],\n",
       "             [-2.6635e-01,  2.7081e-01, -4.3918e-01,  ..., -3.1688e-01,\n",
       "               6.7675e-01, -3.7627e-01]],\n",
       "  \n",
       "            [[-2.5022e-02,  4.6746e-04,  7.1227e-02,  ..., -2.1691e-02,\n",
       "              -1.4388e-01,  1.2917e-01],\n",
       "             [ 1.3207e+00, -9.7167e-02, -4.3254e-02,  ..., -1.1794e-01,\n",
       "               4.7581e-01, -6.0767e-01],\n",
       "             [-5.9027e-01,  5.4489e-02,  4.0488e-01,  ...,  4.8344e-01,\n",
       "               4.6009e-01,  1.9037e-02],\n",
       "             ...,\n",
       "             [-6.4645e-01,  5.0527e-03, -9.9323e-02,  ...,  3.4828e-01,\n",
       "              -3.6676e-01, -7.5016e-01],\n",
       "             [ 5.8888e-01,  1.7997e-01,  3.4016e-01,  ..., -2.4735e-01,\n",
       "              -4.2223e-01, -5.4837e-01],\n",
       "             [-6.2072e-01,  6.2121e-03, -5.1855e-02,  ...,  3.6348e-01,\n",
       "              -3.7332e-01, -7.2578e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-1.2634e-01, -9.0111e-03,  6.1170e-02,  ..., -2.8844e-02,\n",
       "               1.1346e-02,  1.3515e-01],\n",
       "             [-4.6222e-01, -7.3574e-01,  7.6296e-01,  ..., -3.4225e-01,\n",
       "               9.4332e-01, -2.9572e-01],\n",
       "             [-6.2965e-01,  4.1410e-01,  6.0440e-01,  ...,  3.3087e-02,\n",
       "               8.8669e-01, -4.3701e-01],\n",
       "             ...,\n",
       "             [-4.1360e-01,  4.2841e-01,  3.4588e-01,  ..., -1.7670e-01,\n",
       "               2.7978e-01, -1.8215e-01],\n",
       "             [-8.1077e-01,  2.0147e-01,  5.1268e-01,  ..., -6.9632e-01,\n",
       "               5.9417e-01,  5.2836e-02],\n",
       "             [-4.2159e-01,  4.2881e-01,  3.4142e-01,  ..., -1.9776e-01,\n",
       "               2.7369e-01, -1.9171e-01]],\n",
       "  \n",
       "            [[-2.8028e-02,  8.4329e-02, -7.4562e-02,  ...,  1.1567e-01,\n",
       "               4.5594e-02,  5.4924e-02],\n",
       "             [-1.5705e-01, -1.3401e+00, -1.5046e+00,  ..., -1.4752e+00,\n",
       "               5.2599e-01,  1.6321e+00],\n",
       "             [ 4.3184e-01,  2.3969e-01, -8.8045e-01,  ..., -2.0772e+00,\n",
       "               1.4693e+00,  8.0424e-01],\n",
       "             ...,\n",
       "             [-1.1475e-01, -3.9556e-02, -2.5399e-01,  ..., -8.1381e-01,\n",
       "               2.5289e-01,  5.5467e-01],\n",
       "             [ 2.7709e-02, -2.2001e+00, -1.2571e+00,  ..., -1.8843e+00,\n",
       "               2.2824e-01,  1.2075e+00],\n",
       "             [-1.0162e-01, -5.9535e-02, -2.6836e-01,  ..., -7.9802e-01,\n",
       "               1.9247e-01,  5.9190e-01]],\n",
       "  \n",
       "            [[-1.1343e-01, -2.1909e-02,  1.3382e-01,  ...,  5.6986e-02,\n",
       "              -2.1418e-01, -6.8748e-02],\n",
       "             [ 6.5334e-01,  1.3428e+00, -1.5310e-01,  ..., -2.3468e-01,\n",
       "               2.0447e-01,  3.7732e-01],\n",
       "             [ 6.7185e-01,  3.5975e-01,  2.5317e-01,  ...,  5.9677e-01,\n",
       "              -1.3562e+00,  7.8783e-01],\n",
       "             ...,\n",
       "             [ 7.2587e-01, -1.9175e-02, -1.8541e-01,  ...,  4.8094e-01,\n",
       "              -4.2995e-01,  7.1530e-01],\n",
       "             [ 5.5944e-01,  6.2292e-01,  1.6546e-01,  ...,  8.0500e-02,\n",
       "               1.5836e-01,  1.7326e-01],\n",
       "             [ 6.9656e-01, -7.5633e-02, -2.3236e-01,  ...,  4.6187e-01,\n",
       "              -4.0555e-01,  7.0178e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 2.5721e-01,  1.2138e+00,  8.1850e-02,  ..., -1.5436e-02,\n",
       "              -2.6535e-02, -3.3984e-01],\n",
       "             [ 5.7023e-01, -1.6326e+00, -1.3452e+00,  ..., -9.7856e-01,\n",
       "              -9.3806e-02,  1.1103e-01],\n",
       "             [-6.5884e-01, -7.3669e-01,  5.5879e-01,  ..., -1.6449e+00,\n",
       "               1.3863e+00,  5.7571e-01],\n",
       "             ...,\n",
       "             [-2.5891e+00,  3.6271e+00,  7.8342e-01,  ..., -2.4252e+00,\n",
       "               3.6595e+00,  1.6286e+00],\n",
       "             [-1.9184e+00,  2.8134e+00, -1.8229e-01,  ..., -3.1005e+00,\n",
       "               2.8074e+00,  1.3658e+00],\n",
       "             [-2.6316e+00,  3.7973e+00,  7.7992e-01,  ..., -2.4313e+00,\n",
       "               3.6154e+00,  1.5645e+00]],\n",
       "  \n",
       "            [[-3.5678e-01, -2.6492e-01, -1.1649e-01,  ..., -1.0741e-01,\n",
       "               4.2998e-01, -9.8164e-02],\n",
       "             [ 3.8823e-01, -1.8031e+00, -1.3167e+00,  ...,  1.5703e+00,\n",
       "              -7.4182e-01,  2.3385e-01],\n",
       "             [ 3.9244e-01, -1.6642e+00, -7.6806e-01,  ...,  1.1689e+00,\n",
       "               4.5848e-02, -7.8316e-01],\n",
       "             ...,\n",
       "             [-1.3631e+00, -2.9359e+00,  9.6263e-01,  ..., -3.9056e-01,\n",
       "               5.3526e+00, -2.1709e+00],\n",
       "             [-1.0622e+00, -4.5258e+00,  6.2185e-01,  ...,  7.7345e-02,\n",
       "               4.8630e+00, -1.9212e+00],\n",
       "             [-1.3845e+00, -2.9444e+00,  1.0412e+00,  ..., -4.3046e-01,\n",
       "               5.4469e+00, -2.1500e+00]],\n",
       "  \n",
       "            [[-7.6571e-02, -2.1862e-02,  3.1526e-01,  ...,  4.8836e-01,\n",
       "               5.6518e-02, -2.0227e+00],\n",
       "             [-8.5462e-01,  7.1048e-01, -1.4530e+00,  ...,  5.4351e-01,\n",
       "              -3.4837e-01,  3.8102e+00],\n",
       "             [ 1.6520e+00, -7.6158e-01, -7.2883e-01,  ...,  3.7667e-01,\n",
       "              -8.7408e-02,  2.4233e+00],\n",
       "             ...,\n",
       "             [ 2.1690e+00, -1.6134e+00, -1.6237e+00,  ..., -1.6497e+00,\n",
       "              -8.1387e-02, -1.5496e+00],\n",
       "             [ 2.0887e+00, -3.2867e-01, -2.6876e+00,  ..., -1.8665e+00,\n",
       "               1.4759e-01, -6.2026e-01],\n",
       "             [ 2.1724e+00, -1.6319e+00, -1.6827e+00,  ..., -1.6649e+00,\n",
       "              -1.2454e-01, -1.6946e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-4.9549e-01, -9.6223e-01, -1.5773e+00,  ..., -8.9447e-01,\n",
       "              -1.6752e+00,  7.5425e-01],\n",
       "             [ 1.8472e+00,  1.4729e+00, -1.0867e+00,  ...,  1.3627e+00,\n",
       "              -1.8153e+00,  1.2332e+00],\n",
       "             [ 1.2498e+00,  2.6905e-01, -5.3599e-01,  ...,  7.9664e-01,\n",
       "              -2.8643e+00,  9.6770e-01],\n",
       "             ...,\n",
       "             [-4.3781e-01,  7.7877e-03, -5.2059e-01,  ..., -2.9902e+00,\n",
       "              -7.1555e+00, -3.6680e+00],\n",
       "             [ 7.1942e-01,  8.8740e-01, -8.9328e-01,  ..., -1.6693e+00,\n",
       "              -6.5624e+00, -2.7816e+00],\n",
       "             [-4.9307e-01, -4.5331e-02, -5.8609e-01,  ..., -3.1443e+00,\n",
       "              -7.1759e+00, -3.7997e+00]],\n",
       "  \n",
       "            [[-4.7654e-01, -3.9439e-01, -3.6832e-02,  ..., -7.6317e-02,\n",
       "              -2.8420e-01,  1.1539e+00],\n",
       "             [-1.6546e-01, -3.9656e-01, -2.0579e-01,  ..., -8.3005e-01,\n",
       "               1.1140e-01, -2.6167e+00],\n",
       "             [-1.1353e-01, -3.3726e-01, -6.2611e-01,  ...,  6.2819e-01,\n",
       "              -2.3303e-02, -9.2785e-01],\n",
       "             ...,\n",
       "             [-6.1267e+00,  1.4838e-01,  3.1597e+00,  ...,  8.6293e+00,\n",
       "               3.6374e-01,  4.1481e+00],\n",
       "             [-6.3406e+00,  5.6825e-01,  4.0743e+00,  ...,  7.0162e+00,\n",
       "               4.4845e-01,  3.9556e+00],\n",
       "             [-6.2856e+00,  1.6311e-01,  3.2931e+00,  ...,  8.7034e+00,\n",
       "               3.0326e-01,  4.2933e+00]],\n",
       "  \n",
       "            [[ 7.7467e-02, -2.6303e-01, -5.2029e-01,  ...,  2.1443e+00,\n",
       "               2.2853e+00,  4.4737e-01],\n",
       "             [ 1.7687e+00, -1.7020e-01, -2.1599e+00,  ..., -3.6347e-01,\n",
       "              -2.1461e+00, -8.9899e-01],\n",
       "             [-1.1482e-02, -1.4947e+00, -5.2686e-01,  ...,  1.7061e+00,\n",
       "              -1.1591e+00,  2.6533e-01],\n",
       "             ...,\n",
       "             [ 3.1832e+00, -2.3699e+00,  7.2115e-01,  ...,  1.0088e+01,\n",
       "               6.8950e+00,  1.1676e+00],\n",
       "             [ 4.6007e+00, -1.9037e+00,  1.9926e-01,  ...,  9.2790e+00,\n",
       "               6.8701e+00,  1.2638e+00],\n",
       "             [ 3.2946e+00, -2.3977e+00,  7.5841e-01,  ...,  1.0198e+01,\n",
       "               7.1239e+00,  1.1269e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-1.6332e-01,  1.4786e-01, -1.1422e-01,  ...,  3.8804e-02,\n",
       "               1.7195e-01, -1.0548e-01],\n",
       "             [-1.6032e-01,  5.5673e-01,  1.0013e+00,  ..., -8.0335e-01,\n",
       "               1.1870e+00, -3.0828e-01],\n",
       "             [ 1.3168e-01,  8.9118e-01,  9.7710e-01,  ..., -2.6267e-01,\n",
       "               9.1514e-01,  4.1614e-01],\n",
       "             ...,\n",
       "             [ 1.2225e-01,  5.3133e-01, -2.5466e-01,  ..., -9.4634e-01,\n",
       "               5.2956e-01,  1.7132e-01],\n",
       "             [-5.8868e-01,  4.4613e-01,  1.3970e-01,  ..., -5.0140e-01,\n",
       "               3.5854e-01,  5.0789e-02],\n",
       "             [ 8.7806e-02,  5.1828e-01, -2.8527e-01,  ..., -9.0446e-01,\n",
       "               4.7059e-01,  2.1138e-01]],\n",
       "  \n",
       "            [[ 1.1702e-01, -6.4699e-02,  3.1676e-02,  ..., -5.7561e-02,\n",
       "               1.1061e-01,  6.0477e-02],\n",
       "             [-1.8498e+00, -1.4362e+00,  2.8411e+00,  ..., -8.4311e-01,\n",
       "               4.4487e-02,  2.3439e+00],\n",
       "             [-9.9192e-01, -1.3068e+00,  1.3063e+00,  ..., -2.5021e-01,\n",
       "               1.6060e+00,  9.6410e-01],\n",
       "             ...,\n",
       "             [-3.5784e-01, -9.1374e-01,  6.8249e-01,  ...,  9.1868e-01,\n",
       "               6.2099e-01,  4.3478e-01],\n",
       "             [-9.0948e-01, -1.7387e+00,  2.6020e+00,  ...,  4.5886e-01,\n",
       "               8.4674e-01,  1.6683e+00],\n",
       "             [-3.9481e-01, -8.7358e-01,  6.8256e-01,  ...,  9.3054e-01,\n",
       "               6.1870e-01,  4.1275e-01]],\n",
       "  \n",
       "            [[ 1.0699e-01, -8.6197e-02, -1.5626e-01,  ..., -3.6686e-02,\n",
       "              -2.8451e-02, -1.8200e-01],\n",
       "             [ 2.0919e+00, -5.7443e-01,  5.0261e-01,  ...,  5.0879e-01,\n",
       "              -4.4538e-01,  1.1994e+00],\n",
       "             [ 1.1740e+00,  1.1131e+00, -1.0803e-01,  ...,  1.5733e+00,\n",
       "               5.6335e-01,  4.2996e-01],\n",
       "             ...,\n",
       "             [ 7.4576e-01,  3.7459e-01,  5.1748e-02,  ...,  5.9791e-01,\n",
       "               6.2805e-01, -2.7986e-03],\n",
       "             [ 1.1941e+00, -6.3472e-01,  4.1449e-01,  ...,  4.0538e-02,\n",
       "              -2.7813e-01,  5.5665e-01],\n",
       "             [ 7.5119e-01,  3.0822e-01,  4.3934e-02,  ...,  6.2068e-01,\n",
       "               6.9509e-01, -1.4949e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 7.0213e-02, -4.6690e-03, -1.3244e-02,  ..., -1.9181e-01,\n",
       "              -5.8565e-02, -1.1433e-01],\n",
       "             [-1.9068e-01,  6.4706e-01,  1.7624e+00,  ..., -1.2610e+00,\n",
       "              -6.0742e-01, -1.6792e-01],\n",
       "             [-2.5184e-01, -1.1926e-01,  3.5495e-02,  ..., -1.5271e+00,\n",
       "              -4.9171e-01,  1.1398e+00],\n",
       "             ...,\n",
       "             [ 1.5809e-01, -1.4623e+00,  2.1308e-01,  ..., -3.6758e-01,\n",
       "              -4.1551e-01, -2.6188e-02],\n",
       "             [-2.4769e-01, -1.0237e+00,  1.2874e+00,  ..., -4.7997e-01,\n",
       "               2.9049e-02, -4.7527e-01],\n",
       "             [ 1.6794e-01, -1.4549e+00,  1.7829e-01,  ..., -3.6329e-01,\n",
       "              -4.1272e-01, -3.8104e-02]],\n",
       "  \n",
       "            [[-1.8465e-01, -1.0772e-01, -6.3472e-02,  ..., -4.8459e-02,\n",
       "              -9.3733e-02,  7.8508e-03],\n",
       "             [-6.5891e-01, -7.6872e-01,  2.0198e+00,  ...,  4.1450e-01,\n",
       "              -1.1445e+00, -4.7673e-01],\n",
       "             [-1.2836e-01, -1.2783e+00,  9.8167e-01,  ..., -5.0068e-01,\n",
       "              -9.8449e-01,  1.7445e+00],\n",
       "             ...,\n",
       "             [-2.9627e-01, -1.3340e+00,  4.8251e-01,  ..., -5.8301e-01,\n",
       "              -6.2550e-01,  1.6966e+00],\n",
       "             [-1.3742e+00, -8.0175e-01,  5.1382e-01,  ..., -1.7461e-01,\n",
       "              -9.7751e-01,  1.4339e-01],\n",
       "             [-2.8782e-01, -1.3493e+00,  4.9810e-01,  ..., -5.6421e-01,\n",
       "              -5.4099e-01,  1.7088e+00]],\n",
       "  \n",
       "            [[ 1.5993e-02, -8.2934e-02, -1.0345e-01,  ...,  6.5070e-02,\n",
       "               2.6188e-02,  3.8959e-02],\n",
       "             [ 4.2417e-01, -1.5099e+00,  1.4649e-02,  ...,  1.4629e-01,\n",
       "              -1.0355e+00, -1.3556e+00],\n",
       "             [ 7.3402e-01, -2.0612e-01, -7.6275e-01,  ...,  1.2663e-01,\n",
       "              -6.5220e-01, -1.2646e+00],\n",
       "             ...,\n",
       "             [ 2.8900e-01, -1.4638e-01, -4.8337e-01,  ...,  6.8739e-01,\n",
       "              -1.4610e+00,  1.6385e-02],\n",
       "             [ 1.2888e-01, -1.1009e+00,  3.7273e-01,  ...,  9.4664e-01,\n",
       "              -1.8511e+00,  3.8415e-01],\n",
       "             [ 2.8724e-01, -1.4473e-01, -4.4987e-01,  ...,  6.7078e-01,\n",
       "              -1.4032e+00,  7.5681e-02]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-1.7468e-01,  1.0717e-01, -7.6040e-01,  ...,  3.0821e-01,\n",
       "               2.8741e-01,  2.9048e-02],\n",
       "             [ 4.2890e-01,  5.4284e-01,  4.4602e+00,  ...,  1.9793e-01,\n",
       "               1.5736e+00, -2.1584e+00],\n",
       "             [-1.5036e+00,  7.6909e-01,  1.0537e+00,  ..., -8.4576e-02,\n",
       "              -3.2861e-02, -7.8621e-01],\n",
       "             ...,\n",
       "             [ 2.4971e-01, -2.6549e-01, -1.8247e+01,  ...,  6.3037e-01,\n",
       "              -3.0734e-01,  1.1628e-01],\n",
       "             [ 1.6194e+00,  2.1027e-01, -1.7676e+01,  ...,  6.1824e-01,\n",
       "               3.2983e-01, -1.0848e+00],\n",
       "             [ 3.0269e-01, -3.2883e-01, -1.8575e+01,  ...,  5.9535e-01,\n",
       "              -2.6633e-01,  1.9165e-01]],\n",
       "  \n",
       "            [[ 5.6240e-01, -6.3630e-01,  1.1905e-02,  ...,  2.0326e-01,\n",
       "               1.9875e-01, -7.6223e-02],\n",
       "             [ 1.6348e+00, -1.4077e-01,  1.3047e-01,  ...,  8.6383e-01,\n",
       "               1.0218e+00,  2.5608e-01],\n",
       "             [ 6.7032e-01, -7.2540e-01, -9.9685e-01,  ...,  7.3853e-01,\n",
       "               4.6462e-02, -3.0084e-01],\n",
       "             ...,\n",
       "             [ 1.0973e+00, -3.2247e-01, -1.3315e+00,  ..., -2.5628e+00,\n",
       "               1.9588e+00,  1.6561e-01],\n",
       "             [ 2.2223e+00,  5.5571e-01, -2.0431e+00,  ..., -2.3944e+00,\n",
       "               1.9723e+00,  4.7768e-01],\n",
       "             [ 1.1568e+00, -3.1824e-01, -1.3313e+00,  ..., -2.6206e+00,\n",
       "               1.9909e+00,  1.3412e-01]],\n",
       "  \n",
       "            [[-1.7560e+00,  1.5315e+00,  1.0415e+00,  ...,  3.5970e-01,\n",
       "               1.6668e+00, -1.1792e+00],\n",
       "             [-1.1854e+00,  1.8038e-01,  8.0481e-01,  ...,  2.2626e-01,\n",
       "              -4.3112e-01, -1.7047e+00],\n",
       "             [ 5.1965e-01,  4.9645e-01, -2.7751e-02,  ...,  1.6743e+00,\n",
       "              -7.3849e-01, -1.1344e+00],\n",
       "             ...,\n",
       "             [ 1.1806e+00, -1.1148e+00, -1.1892e+00,  ...,  3.7135e+00,\n",
       "              -2.4685e+00,  8.7146e-01],\n",
       "             [ 2.7237e-02, -3.8074e-01, -4.3562e-01,  ...,  3.4326e+00,\n",
       "              -1.9947e+00,  7.7859e-01],\n",
       "             [ 1.1604e+00, -1.1115e+00, -1.1292e+00,  ...,  3.6589e+00,\n",
       "              -2.4985e+00,  9.7956e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 2.3085e-01,  5.2260e-02, -2.6409e-01,  ...,  1.0213e-01,\n",
       "               9.6440e-01,  3.3343e-01],\n",
       "             [-5.5340e-01,  4.6812e-02, -3.1486e-01,  ...,  4.3484e-02,\n",
       "              -9.7888e-01,  1.2114e+00],\n",
       "             [-8.7279e-01, -7.8645e-01, -7.8899e-01,  ..., -1.8146e+00,\n",
       "              -1.5181e+00,  2.3942e+00],\n",
       "             ...,\n",
       "             [ 1.1691e+00, -1.0570e+00,  1.2454e+00,  ..., -2.0210e+00,\n",
       "               1.8224e-01,  2.4187e+00],\n",
       "             [ 6.9002e-01, -1.3524e+00,  2.5004e+00,  ..., -1.9812e+00,\n",
       "               7.3888e-01,  2.0485e+00],\n",
       "             [ 1.2013e+00, -1.1386e+00,  1.3503e+00,  ..., -2.0165e+00,\n",
       "               2.3174e-01,  2.4157e+00]],\n",
       "  \n",
       "            [[ 2.1764e-01,  9.4057e-01,  5.6683e-01,  ...,  2.8146e-01,\n",
       "              -6.7593e-02,  9.5767e-01],\n",
       "             [ 5.8316e-01,  1.9273e+00,  2.3725e+00,  ...,  3.3895e-01,\n",
       "              -5.4782e-01,  2.5123e+00],\n",
       "             [ 1.6383e+00,  1.2497e-01,  1.7635e+00,  ..., -6.8835e-01,\n",
       "               1.2507e+00,  1.0928e+00],\n",
       "             ...,\n",
       "             [ 2.7377e+00, -2.4009e+00,  3.7117e+00,  ...,  1.6957e-01,\n",
       "               2.4561e+00, -1.6149e+00],\n",
       "             [ 1.6675e+00, -2.6092e+00,  4.3211e+00,  ...,  1.0568e+00,\n",
       "               1.3674e+00, -6.8008e-01],\n",
       "             [ 2.7063e+00, -2.4350e+00,  3.7069e+00,  ...,  1.7959e-01,\n",
       "               2.4409e+00, -1.6400e+00]],\n",
       "  \n",
       "            [[-1.1778e-01, -2.5513e-01, -1.5417e-01,  ...,  2.8240e-01,\n",
       "               5.9684e-01, -2.1728e-01],\n",
       "             [-8.0560e-01,  5.4791e-02,  5.7660e-01,  ..., -1.6531e+00,\n",
       "               2.5050e-01,  1.4666e+00],\n",
       "             [ 8.1632e-01,  1.5964e+00, -1.2875e+00,  ...,  2.1482e-01,\n",
       "              -1.6551e+00, -1.0226e-01],\n",
       "             ...,\n",
       "             [ 1.9715e+00,  2.5392e+00, -2.0742e+00,  ..., -2.0154e+00,\n",
       "              -1.0214e+00, -3.0341e+00],\n",
       "             [ 6.0310e-01,  2.2108e+00, -2.0675e+00,  ..., -2.1962e+00,\n",
       "              -5.8974e-01, -2.5049e+00],\n",
       "             [ 2.0665e+00,  2.5266e+00, -2.0451e+00,  ..., -2.1100e+00,\n",
       "              -1.0499e+00, -3.0856e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-4.5566e-02, -1.7480e-02, -3.8891e-02,  ...,  3.6865e-02,\n",
       "              -5.5307e-02, -7.1500e-02],\n",
       "             [ 6.2637e-01,  2.5595e-01,  6.6446e-01,  ..., -1.7559e+00,\n",
       "              -1.8878e-01,  5.3419e-01],\n",
       "             [-7.7922e-01,  1.3548e-01,  8.6319e-01,  ..., -6.4873e-01,\n",
       "              -2.8578e-01, -3.3432e-01],\n",
       "             ...,\n",
       "             [-1.0011e+00,  1.6430e-01, -2.5699e-01,  ..., -3.6857e-01,\n",
       "              -3.2400e-01, -4.7606e-01],\n",
       "             [ 9.6577e-02,  5.4291e-01,  2.0008e-01,  ..., -8.8246e-01,\n",
       "               5.1083e-01, -2.2739e-01],\n",
       "             [-9.7745e-01,  1.7431e-01, -2.9558e-01,  ..., -3.2513e-01,\n",
       "              -2.4526e-01, -4.8195e-01]],\n",
       "  \n",
       "            [[-1.2290e-01,  1.1081e-01, -4.3613e-02,  ...,  1.5664e-01,\n",
       "              -1.3631e-01,  3.4497e-02],\n",
       "             [-5.2788e-01, -7.9282e-01,  1.1273e+00,  ..., -1.0951e+00,\n",
       "               3.4573e-01, -6.2041e-01],\n",
       "             [ 2.0018e-02, -1.5128e+00,  1.2531e+00,  ...,  3.4519e-01,\n",
       "               2.3076e+00, -1.0036e+00],\n",
       "             ...,\n",
       "             [ 7.1103e-01, -1.4989e+00,  3.9438e-01,  ..., -8.2604e-02,\n",
       "               1.0878e+00, -1.1895e+00],\n",
       "             [-4.7197e-01, -6.9099e-01,  8.4547e-01,  ..., -1.1704e+00,\n",
       "               2.9250e-01, -9.6097e-01],\n",
       "             [ 6.9305e-01, -1.4659e+00,  4.1487e-01,  ..., -1.2832e-01,\n",
       "               9.6341e-01, -1.1758e+00]],\n",
       "  \n",
       "            [[-4.0915e-02, -2.5827e-02, -3.8340e-02,  ...,  6.5657e-02,\n",
       "              -6.5066e-02, -7.1902e-02],\n",
       "             [ 9.2164e-01, -1.4168e+00, -4.8031e-01,  ...,  6.4627e-01,\n",
       "              -9.3983e-01,  1.5531e+00],\n",
       "             [ 3.2302e-01,  6.6319e-01,  5.3836e-02,  ..., -9.4950e-01,\n",
       "               7.1307e-01, -2.3200e-01],\n",
       "             ...,\n",
       "             [ 9.8054e-02,  4.0679e-01,  4.1275e-01,  ..., -1.6196e-01,\n",
       "               9.0045e-01,  8.5608e-02],\n",
       "             [ 9.1775e-01, -1.7935e-01, -4.4064e-01,  ...,  1.0792e+00,\n",
       "              -2.4329e-01,  2.4545e-01],\n",
       "             [ 1.1315e-01,  4.2112e-01,  4.0122e-01,  ..., -1.0363e-01,\n",
       "               9.1427e-01,  1.2911e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-2.0744e-02,  6.6681e-02,  4.2876e-02,  ...,  6.3183e-02,\n",
       "               7.4517e-02, -2.1804e-02],\n",
       "             [ 1.2956e+00, -6.0599e-01, -7.3705e-01,  ..., -1.3554e+00,\n",
       "               1.1951e+00,  7.5260e-01],\n",
       "             [ 5.7595e-01, -6.3402e-01, -1.0337e+00,  ..., -1.4194e+00,\n",
       "              -3.7618e-01, -3.3896e-01],\n",
       "             ...,\n",
       "             [ 1.8955e-01, -8.5672e-01, -1.2462e+00,  ..., -7.5994e-01,\n",
       "              -4.3447e-01, -6.0162e-01],\n",
       "             [ 1.1331e+00, -3.4510e-01, -1.1935e+00,  ..., -1.7561e+00,\n",
       "               7.0557e-01,  2.1363e-01],\n",
       "             [ 2.3032e-01, -8.7259e-01, -1.2628e+00,  ..., -7.7210e-01,\n",
       "              -4.1047e-01, -5.7558e-01]],\n",
       "  \n",
       "            [[ 3.2518e-04,  3.7154e-02,  1.2617e-01,  ..., -3.2091e-02,\n",
       "               7.7207e-02,  1.2766e-01],\n",
       "             [ 9.3514e-01, -1.7814e-01, -2.8198e+00,  ...,  7.3082e-01,\n",
       "              -3.8584e-01, -1.1399e+00],\n",
       "             [ 3.0442e-01,  9.3584e-01,  8.9384e-02,  ...,  4.6791e-01,\n",
       "               4.5719e-01, -2.7641e-01],\n",
       "             ...,\n",
       "             [ 1.9988e-01,  9.2302e-01,  2.7621e-01,  ..., -4.0096e-01,\n",
       "               1.0985e+00,  2.7633e-01],\n",
       "             [ 1.3093e+00,  3.3545e-01, -1.9330e+00,  ..., -3.2209e-01,\n",
       "               2.5876e-01, -2.9921e-01],\n",
       "             [ 2.4153e-01,  9.2163e-01,  2.8477e-01,  ..., -3.7422e-01,\n",
       "               1.1135e+00,  2.6796e-01]],\n",
       "  \n",
       "            [[-1.4468e-02,  1.3328e-01,  2.9911e-02,  ..., -4.7124e-03,\n",
       "              -4.0540e-02, -2.9560e-02],\n",
       "             [-1.0652e+00,  1.2824e+00,  4.1322e-01,  ...,  8.1307e-01,\n",
       "               2.6433e-01, -4.2133e-01],\n",
       "             [-7.7034e-01,  5.4305e-01, -5.1909e-01,  ...,  1.3205e+00,\n",
       "               1.2389e+00, -1.8216e-01],\n",
       "             ...,\n",
       "             [-3.3312e-01,  6.6690e-01,  1.0034e-01,  ...,  7.7545e-01,\n",
       "               1.4768e-01,  1.3035e-01],\n",
       "             [-1.3296e+00,  1.1827e+00,  8.3366e-01,  ..., -7.6596e-02,\n",
       "               5.9457e-01,  1.6917e-01],\n",
       "             [-4.0216e-01,  6.6990e-01,  1.4997e-01,  ...,  7.6484e-01,\n",
       "               1.2988e-01,  9.3710e-02]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-6.8363e-02,  2.3473e-01, -1.8097e-01,  ...,  2.3013e-01,\n",
       "              -1.4967e-01, -5.4951e-02],\n",
       "             [ 3.0134e-02, -2.0588e-01,  8.5890e-01,  ..., -5.5731e-01,\n",
       "              -2.0551e+00, -4.1907e-01],\n",
       "             [-6.3715e-01,  6.3762e-03,  1.2009e+00,  ...,  1.3735e+00,\n",
       "               3.4252e-02,  2.2617e-01],\n",
       "             ...,\n",
       "             [-5.0931e-01,  1.2689e+00,  6.6942e-01,  ...,  1.5946e+00,\n",
       "              -1.1770e+00, -5.1137e-01],\n",
       "             [-4.7522e-01,  1.2451e+00,  7.3215e-01,  ...,  3.7181e-01,\n",
       "              -2.7636e+00, -1.2014e+00],\n",
       "             [-6.0890e-01,  1.2834e+00,  6.5942e-01,  ...,  1.5900e+00,\n",
       "              -1.2238e+00, -4.8055e-01]],\n",
       "  \n",
       "            [[-2.0398e-01, -4.6946e-02, -6.9316e-02,  ..., -1.3456e-01,\n",
       "              -2.3902e-01,  6.6880e-01],\n",
       "             [-1.2970e+00, -2.2308e-01, -4.1568e-01,  ..., -1.7620e-01,\n",
       "              -6.2372e-01, -1.8557e+00],\n",
       "             [-1.0259e+00,  3.8145e-02, -5.1221e-02,  ..., -3.6907e+00,\n",
       "              -1.4507e+00, -1.5562e+00],\n",
       "             ...,\n",
       "             [-3.7403e+00, -1.2280e+00, -1.4350e+00,  ..., -3.2937e+00,\n",
       "              -1.3203e+00, -2.4295e+00],\n",
       "             [-4.4497e+00, -8.6610e-01, -2.1866e+00,  ..., -1.6010e+00,\n",
       "              -6.3435e-01, -3.3133e+00],\n",
       "             [-3.7603e+00, -1.2771e+00, -1.4538e+00,  ..., -3.1839e+00,\n",
       "              -1.3174e+00, -2.4027e+00]],\n",
       "  \n",
       "            [[ 1.2101e-01, -3.3775e-01, -7.5342e-01,  ...,  1.4288e-01,\n",
       "              -5.7473e-02, -2.2517e-01],\n",
       "             [ 7.1858e-01,  1.0671e+00,  2.5207e+00,  ..., -7.8073e-01,\n",
       "               1.3278e+00,  1.7128e+00],\n",
       "             [-5.5515e-01,  4.5155e-01,  1.2876e+00,  ...,  9.0296e-02,\n",
       "               1.1994e+00,  1.3098e+00],\n",
       "             ...,\n",
       "             [-7.2239e-01,  1.0309e+00, -2.5305e+00,  ...,  3.5632e-01,\n",
       "               1.8742e+00,  2.6916e+00],\n",
       "             [ 9.3721e-03,  2.0874e+00, -2.2025e+00,  ..., -1.8252e-01,\n",
       "               3.1287e+00,  3.2005e+00],\n",
       "             [-6.3847e-01,  1.0943e+00, -2.6169e+00,  ...,  3.9455e-01,\n",
       "               1.9324e+00,  2.7169e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-3.1846e-01, -2.2966e-01, -4.1549e-01,  ...,  2.8118e-01,\n",
       "              -9.1376e-01, -1.3885e-01],\n",
       "             [ 2.2987e+00, -8.6459e-01,  2.8000e+00,  ..., -8.0268e-01,\n",
       "               2.8388e+00,  5.8557e-01],\n",
       "             [ 1.5848e-01, -8.9317e-01,  1.2131e+00,  ...,  3.5174e-02,\n",
       "               1.9402e+00,  7.1297e-01],\n",
       "             ...,\n",
       "             [-3.7386e+00,  3.4965e+00, -6.2242e+00,  ...,  7.6135e-01,\n",
       "              -1.0431e+00,  1.0505e+00],\n",
       "             [-2.5247e+00,  4.0209e+00, -5.4806e+00,  ...,  3.6015e-01,\n",
       "              -3.2000e-01,  7.0123e-01],\n",
       "             [-3.8224e+00,  3.5692e+00, -6.4203e+00,  ...,  7.5984e-01,\n",
       "              -1.2126e+00,  1.0081e+00]],\n",
       "  \n",
       "            [[ 8.2211e-01,  9.1275e-02,  4.2697e-02,  ..., -6.7359e-01,\n",
       "               1.0167e-01,  1.2876e+00],\n",
       "             [-8.9089e-01,  8.4156e-01,  1.2123e+00,  ..., -8.7723e-01,\n",
       "               5.6888e-01,  2.3910e-01],\n",
       "             [-5.4239e-01,  9.3615e-01,  1.0040e-01,  ..., -2.1924e+00,\n",
       "               4.9205e-02, -5.1112e-01],\n",
       "             ...,\n",
       "             [ 3.7273e+00,  1.4250e-01, -5.9669e+00,  ..., -1.0485e+00,\n",
       "               9.4365e-01, -1.1212e-01],\n",
       "             [ 3.9569e+00, -4.9953e-01, -6.4824e+00,  ..., -1.4632e+00,\n",
       "               1.2777e+00, -3.2002e-01],\n",
       "             [ 3.8552e+00,  1.0124e-01, -6.0035e+00,  ..., -1.0735e+00,\n",
       "               9.6882e-01, -1.1187e-01]],\n",
       "  \n",
       "            [[-3.7398e-01,  2.5305e-02,  9.7201e-01,  ...,  2.3895e-01,\n",
       "              -5.1968e-01,  6.8140e-01],\n",
       "             [ 1.1778e-01,  4.4421e-01, -2.7052e-01,  ...,  2.0774e+00,\n",
       "              -7.1972e-01,  1.1630e-01],\n",
       "             [ 1.0886e-01, -1.1758e+00, -1.5168e-01,  ...,  1.3644e+00,\n",
       "              -1.7472e+00,  4.0572e-01],\n",
       "             ...,\n",
       "             [ 5.7748e+00,  6.8894e+00,  9.0955e+00,  ..., -3.7789e+00,\n",
       "              -3.3661e+00, -6.2968e-01],\n",
       "             [ 6.8790e+00,  8.0774e+00,  1.0206e+01,  ..., -3.0138e+00,\n",
       "              -3.8751e+00, -9.1797e-01],\n",
       "             [ 5.9551e+00,  7.1900e+00,  9.2815e+00,  ..., -4.0185e+00,\n",
       "              -3.4386e+00, -6.5133e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-4.2885e-02, -1.2107e-01, -1.5820e-01,  ...,  2.0636e-01,\n",
       "               2.8759e-01,  7.4576e-02],\n",
       "             [ 1.1971e-01, -6.6895e-01, -1.8416e+00,  ...,  5.7289e-01,\n",
       "              -2.0339e-01,  1.3873e-02],\n",
       "             [-1.2138e+00,  3.6421e-01, -1.3385e+00,  ...,  4.9920e-01,\n",
       "               7.7398e-01, -7.5725e-01],\n",
       "             ...,\n",
       "             [-7.6551e-01,  1.4908e+00, -1.0417e+00,  ...,  9.3507e-01,\n",
       "               1.6497e+00,  4.3018e-02],\n",
       "             [-7.0337e-01,  1.1566e+00, -1.0160e+00,  ...,  1.4911e+00,\n",
       "               1.0859e+00,  6.0557e-01],\n",
       "             [-7.1998e-01,  1.4543e+00, -9.9271e-01,  ...,  9.9563e-01,\n",
       "               1.6108e+00,  3.3358e-02]],\n",
       "  \n",
       "            [[ 4.7294e-02,  1.2589e-02,  1.3605e-01,  ...,  7.8787e-03,\n",
       "               2.9382e-02, -2.9296e-02],\n",
       "             [-2.4300e-01,  3.3368e-01,  1.4025e-01,  ..., -1.6096e+00,\n",
       "               8.4775e-01,  1.0206e+00],\n",
       "             [ 2.5166e-01, -2.8205e-01, -4.3210e-01,  ..., -1.9428e+00,\n",
       "               2.0017e+00, -1.9764e-01],\n",
       "             ...,\n",
       "             [ 8.7309e-01,  1.3881e-01, -2.1979e-01,  ..., -1.5756e+00,\n",
       "               8.6083e-01, -7.9376e-01],\n",
       "             [-4.2063e-01,  3.9164e-01,  1.1048e-01,  ..., -1.2903e+00,\n",
       "               6.9103e-01,  8.6336e-01],\n",
       "             [ 8.5124e-01,  1.1466e-01, -1.6801e-01,  ..., -1.5125e+00,\n",
       "               8.3840e-01, -7.4542e-01]],\n",
       "  \n",
       "            [[ 1.2611e-01,  3.1437e-03, -7.3591e-02,  ...,  1.3742e-01,\n",
       "              -3.6719e-02, -9.5853e-02],\n",
       "             [ 1.2321e+00, -1.1449e+00, -3.8663e-01,  ...,  8.7898e-01,\n",
       "              -1.2717e+00,  1.0307e+00],\n",
       "             [ 8.4025e-01,  3.0522e-01, -7.7022e-01,  ...,  3.0005e-01,\n",
       "              -5.1376e-01,  1.0424e-01],\n",
       "             ...,\n",
       "             [ 7.1224e-01,  1.7528e-01, -3.9135e-01,  ...,  3.4713e-01,\n",
       "              -3.3522e-01,  2.7083e-01],\n",
       "             [ 4.6691e-01, -7.9861e-01, -5.7115e-01,  ...,  5.0911e-01,\n",
       "               1.2945e-01,  1.1278e+00],\n",
       "             [ 7.2042e-01,  1.2726e-01, -4.5077e-01,  ...,  4.2308e-01,\n",
       "              -2.6290e-01,  2.8119e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 4.6765e-02, -5.6109e-02, -1.3412e-01,  ..., -9.7240e-02,\n",
       "              -5.3328e-02, -3.9770e-02],\n",
       "             [-2.9042e-01,  1.0353e+00,  3.4273e-01,  ..., -6.2273e-01,\n",
       "               7.4122e-01, -1.0529e+00],\n",
       "             [ 3.9952e-02,  1.3833e-01,  6.6181e-02,  ...,  2.2065e-01,\n",
       "              -7.6734e-01, -3.8463e-01],\n",
       "             ...,\n",
       "             [-1.6303e-01,  9.8397e-02,  8.5066e-02,  ...,  8.9085e-01,\n",
       "              -1.0727e-01, -7.3128e-01],\n",
       "             [ 4.0338e-01,  1.1190e+00, -7.2316e-01,  ..., -4.1705e-01,\n",
       "              -4.1264e-01, -7.4593e-01],\n",
       "             [-1.4650e-01,  1.2129e-01,  7.5668e-02,  ...,  8.2638e-01,\n",
       "              -1.1043e-01, -7.1499e-01]],\n",
       "  \n",
       "            [[-5.0889e-02, -4.9232e-03,  3.7780e-02,  ..., -3.1797e-01,\n",
       "              -2.4316e-01,  1.4987e-02],\n",
       "             [-2.5397e-01,  6.5921e-01,  1.3070e+00,  ...,  4.6241e-01,\n",
       "               9.0882e-02,  2.8398e-02],\n",
       "             [-1.3102e+00,  1.0008e+00,  5.2654e-01,  ..., -1.5409e-01,\n",
       "              -1.7296e+00, -2.4269e-01],\n",
       "             ...,\n",
       "             [-7.4463e-01,  8.6569e-02,  3.1618e-02,  ..., -7.7343e-01,\n",
       "              -3.2690e-01, -2.0179e-02],\n",
       "             [ 5.9176e-01,  3.0533e-01,  5.2555e-01,  ..., -3.2570e-01,\n",
       "               1.1459e-01,  2.0796e-01],\n",
       "             [-7.0952e-01,  3.7049e-02,  2.2360e-02,  ..., -7.7559e-01,\n",
       "              -3.0874e-01,  1.1578e-02]],\n",
       "  \n",
       "            [[ 2.9618e-02,  3.8360e-02,  8.3317e-03,  ..., -2.7286e-01,\n",
       "              -4.2066e-02, -3.7188e-02],\n",
       "             [ 5.3378e-01,  1.7853e+00, -8.6738e-01,  ...,  2.3777e-01,\n",
       "               1.1519e+00, -2.4273e-02],\n",
       "             [ 3.9251e-01,  2.1904e-01, -1.6148e-01,  ...,  5.8887e-02,\n",
       "              -3.5234e-01,  1.0769e+00],\n",
       "             ...,\n",
       "             [ 4.3078e-01,  1.1279e-01, -1.4636e-01,  ...,  3.1902e-01,\n",
       "              -2.0811e-01,  8.7018e-01],\n",
       "             [ 1.5142e-01,  5.7969e-01, -1.6673e-01,  ...,  4.2616e-01,\n",
       "               8.8396e-01,  1.0264e+00],\n",
       "             [ 4.7118e-01,  5.4779e-02, -1.4001e-01,  ...,  3.0940e-01,\n",
       "              -1.9915e-01,  8.0427e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-2.4955e-02,  3.8577e-01,  2.8015e-01,  ...,  3.6721e-01,\n",
       "               1.9064e-01, -2.3876e-01],\n",
       "             [ 2.5125e-01, -1.2733e-01, -8.0235e-01,  ..., -4.6622e-01,\n",
       "               1.9686e+00,  1.0569e+00],\n",
       "             [ 2.8510e-01,  5.1489e-02, -1.5954e+00,  ..., -1.7207e-01,\n",
       "               1.2692e+00,  1.1976e-01],\n",
       "             ...,\n",
       "             [-2.5456e-01,  5.9838e-01, -5.5159e-01,  ..., -1.2533e+00,\n",
       "              -7.3917e-02,  4.2924e+00],\n",
       "             [-1.4242e-01,  1.3811e+00, -5.4571e-01,  ..., -8.9925e-01,\n",
       "              -3.9785e-01,  5.4730e+00],\n",
       "             [-2.4730e-01,  6.3387e-01, -4.7063e-01,  ..., -1.2184e+00,\n",
       "              -3.8992e-02,  4.4595e+00]],\n",
       "  \n",
       "            [[-1.9131e-01, -1.1398e-01, -1.6757e-01,  ...,  4.0200e-02,\n",
       "              -1.8549e-01,  9.0757e-02],\n",
       "             [ 4.1804e-01,  9.7727e-01,  7.1425e-01,  ..., -5.6789e-01,\n",
       "              -5.7342e-01,  9.7889e-01],\n",
       "             [-9.6520e-01, -4.1238e-01,  8.1408e-01,  ..., -1.3623e-01,\n",
       "               7.9233e-01,  9.6695e-01],\n",
       "             ...,\n",
       "             [ 7.2722e-03, -1.4795e+00,  1.7848e+00,  ...,  1.2893e+00,\n",
       "               7.8675e-01,  4.8717e-02],\n",
       "             [ 1.4567e-02, -1.5633e+00,  1.6778e+00,  ...,  7.0404e-01,\n",
       "               7.2698e-02,  1.8317e-01],\n",
       "             [ 4.7297e-02, -1.5337e+00,  1.7674e+00,  ...,  1.2655e+00,\n",
       "               8.3135e-01,  5.1514e-02]],\n",
       "  \n",
       "            [[ 3.2747e-01, -8.6707e-02, -1.4488e-01,  ...,  4.4925e-01,\n",
       "              -3.4146e-01,  3.6310e-02],\n",
       "             [ 8.1195e-01,  4.8323e-01, -7.2059e-01,  ..., -4.6087e-01,\n",
       "               1.4376e+00,  2.2213e-01],\n",
       "             [ 3.6785e-01,  3.3558e-01, -3.1029e-01,  ..., -4.0883e-01,\n",
       "               2.5137e+00,  3.6423e-01],\n",
       "             ...,\n",
       "             [ 1.8962e+00,  3.7996e+00, -1.4433e-01,  ...,  3.7024e+00,\n",
       "              -2.5669e+00,  2.5543e+00],\n",
       "             [ 2.4319e+00,  4.4623e+00, -6.1962e-01,  ...,  3.6510e+00,\n",
       "              -3.5980e+00,  3.2528e+00],\n",
       "             [ 1.9951e+00,  3.8396e+00, -2.1976e-01,  ...,  3.8463e+00,\n",
       "              -2.8782e+00,  2.5936e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 1.1725e-01, -2.1779e-01, -1.8960e-01,  ...,  3.1861e-01,\n",
       "               3.1671e-01,  1.5524e-02],\n",
       "             [-1.2444e+00,  2.4231e-01,  5.5073e-01,  ..., -5.8774e-01,\n",
       "              -8.5408e-01, -1.7633e+00],\n",
       "             [-4.7743e-01, -3.8582e-01,  1.0976e+00,  ..., -2.4307e-01,\n",
       "              -5.0690e-01, -9.9650e-01],\n",
       "             ...,\n",
       "             [-1.0695e+00,  1.0245e+00,  1.7452e+00,  ...,  5.2326e-01,\n",
       "              -8.4779e-01, -1.8184e+00],\n",
       "             [-1.1765e+00,  1.4705e+00,  1.6807e+00,  ..., -7.6910e-01,\n",
       "              -1.3749e+00, -2.8614e+00],\n",
       "             [-1.1527e+00,  1.0473e+00,  1.7507e+00,  ...,  5.2847e-01,\n",
       "              -7.8102e-01, -1.8488e+00]],\n",
       "  \n",
       "            [[-2.1762e-01,  3.3568e-01, -1.6233e-01,  ..., -6.5765e-01,\n",
       "               4.2922e-01, -7.0302e-01],\n",
       "             [-1.7727e+00, -5.9688e-01, -1.4605e+00,  ..., -2.4516e+00,\n",
       "               7.4791e-01,  9.4136e-02],\n",
       "             [-1.1862e+00,  1.9440e+00, -3.6245e-01,  ..., -1.5809e+00,\n",
       "               4.9827e-01,  6.7005e-01],\n",
       "             ...,\n",
       "             [-2.5242e+00,  3.4597e+00,  4.7155e-01,  ..., -8.3066e-01,\n",
       "               1.2267e+00,  1.3677e+00],\n",
       "             [-1.9195e+00,  1.7770e+00, -9.2155e-01,  ..., -1.9264e+00,\n",
       "               1.6124e+00,  9.0136e-01],\n",
       "             [-2.4809e+00,  3.4660e+00,  4.7123e-01,  ..., -8.4369e-01,\n",
       "               1.2412e+00,  1.3494e+00]],\n",
       "  \n",
       "            [[-3.1883e-01, -1.7511e-01, -1.6445e-01,  ...,  9.8513e-02,\n",
       "               2.0126e-01,  3.1801e-01],\n",
       "             [ 1.1070e+00, -1.5935e+00, -2.4602e-01,  ..., -8.2626e-01,\n",
       "               8.4036e-01, -1.6332e+00],\n",
       "             [ 2.8565e-01, -4.2543e-01, -9.1148e-01,  ...,  3.5443e-01,\n",
       "               9.5425e-01,  3.4728e-01],\n",
       "             ...,\n",
       "             [ 1.2397e-01, -6.4325e-01,  2.9638e-01,  ..., -2.2534e-01,\n",
       "               2.2002e+00,  5.0860e+00],\n",
       "             [ 7.1293e-01, -2.1270e+00,  2.6220e-01,  ...,  1.7734e-02,\n",
       "               1.7432e+00,  4.6476e+00],\n",
       "             [ 9.4910e-02, -6.2242e-01,  3.6751e-01,  ..., -1.8507e-01,\n",
       "               2.1614e+00,  5.2033e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 4.2250e-02,  5.0168e-02,  1.8583e-01,  ..., -1.2491e-01,\n",
       "              -1.2075e-01, -9.9351e-02],\n",
       "             [ 8.9135e-01, -3.7804e-01, -7.4224e-02,  ...,  1.8145e+00,\n",
       "              -9.0233e-02,  3.3136e-01],\n",
       "             [ 8.3002e-01, -1.5926e-01,  8.7375e-01,  ..., -2.1030e-02,\n",
       "              -6.1785e-01, -2.1124e-01],\n",
       "             ...,\n",
       "             [ 9.0745e-01, -8.4917e-01,  7.9817e-01,  ..., -3.3964e-01,\n",
       "              -4.9278e-01, -6.9301e-01],\n",
       "             [ 8.6314e-01, -5.3071e-01,  7.0246e-01,  ...,  1.5704e+00,\n",
       "              -6.0517e-01,  4.2361e-01],\n",
       "             [ 9.0642e-01, -8.4196e-01,  8.4113e-01,  ..., -3.4844e-01,\n",
       "              -4.5786e-01, -6.5748e-01]],\n",
       "  \n",
       "            [[ 6.8843e-02, -3.4312e-02,  1.1653e-01,  ...,  2.8305e-03,\n",
       "               1.8918e-01,  1.6803e-01],\n",
       "             [ 7.9128e-01,  5.6126e-01,  3.2284e-01,  ...,  1.2917e+00,\n",
       "               1.0787e+00, -5.7739e-01],\n",
       "             [ 1.7619e-01,  1.7284e-01,  5.3417e-02,  ..., -8.9633e-01,\n",
       "               1.2142e+00,  1.5870e-02],\n",
       "             ...,\n",
       "             [ 6.3668e-01,  9.8275e-01, -1.7126e-01,  ..., -1.4113e-01,\n",
       "               1.7819e+00,  2.6439e-02],\n",
       "             [-1.7387e-01,  1.0414e+00, -2.5323e-01,  ...,  7.7625e-01,\n",
       "               1.6442e+00, -3.3496e-01],\n",
       "             [ 6.5011e-01,  9.4777e-01, -1.7463e-01,  ..., -1.3352e-01,\n",
       "               1.7757e+00,  6.0550e-02]],\n",
       "  \n",
       "            [[ 4.8726e-02, -7.4730e-03,  1.2723e-01,  ..., -4.8515e-02,\n",
       "               8.2777e-02, -9.4125e-02],\n",
       "             [-7.5361e-01,  5.9208e-01,  3.8427e-01,  ...,  8.1735e-01,\n",
       "              -6.6898e-01, -1.0737e-01],\n",
       "             [-7.0942e-01,  7.7838e-01,  5.4821e-01,  ...,  1.1840e-01,\n",
       "               4.2942e-01,  3.2433e-01],\n",
       "             ...,\n",
       "             [-4.4041e-01, -3.7385e-01,  5.3134e-02,  ...,  1.3888e-01,\n",
       "               2.8167e-01,  9.5426e-01],\n",
       "             [-6.0928e-01, -4.4753e-01, -4.8937e-02,  ...,  9.0892e-02,\n",
       "              -5.0845e-01,  4.8503e-01],\n",
       "             [-4.2209e-01, -3.9506e-01,  5.1358e-02,  ...,  1.1986e-01,\n",
       "               2.4777e-01,  9.8372e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-5.2235e-03, -1.2553e-01,  8.3080e-03,  ..., -5.1718e-02,\n",
       "              -1.1923e-02,  2.9005e-02],\n",
       "             [-1.1340e+00, -2.3374e-01, -2.1158e+00,  ...,  1.3871e-01,\n",
       "              -2.7347e+00, -1.5221e+00],\n",
       "             [-1.0410e+00, -5.7328e-01,  4.0415e-01,  ...,  1.9808e-01,\n",
       "               5.3024e-01, -9.3553e-01],\n",
       "             ...,\n",
       "             [-1.5851e-02, -6.9414e-01,  1.0032e+00,  ..., -1.0471e-01,\n",
       "               7.6869e-01,  1.7194e-01],\n",
       "             [-2.5266e-01, -3.1722e-02, -6.0867e-01,  ..., -3.0899e-01,\n",
       "              -2.3465e+00,  1.1645e-01],\n",
       "             [-5.2057e-02, -7.3095e-01,  1.0170e+00,  ..., -1.3583e-01,\n",
       "               7.5799e-01,  2.1820e-01]],\n",
       "  \n",
       "            [[-9.5237e-02,  1.0166e-01, -4.5621e-02,  ..., -1.3682e-01,\n",
       "              -4.5252e-02, -7.4323e-02],\n",
       "             [-1.0420e+00, -2.3487e+00, -2.4052e+00,  ..., -2.1206e+00,\n",
       "               1.6993e+00, -5.3463e-01],\n",
       "             [ 7.7381e-01, -1.5026e+00, -2.3097e-01,  ..., -1.1356e+00,\n",
       "               7.5040e-01, -6.3723e-01],\n",
       "             ...,\n",
       "             [-6.7290e-02, -4.9291e-01,  5.6384e-01,  ...,  5.7656e-03,\n",
       "               3.1572e-01, -1.4450e+00],\n",
       "             [ 4.6952e-01, -1.1270e+00, -1.1987e+00,  ..., -1.0248e+00,\n",
       "               1.3965e+00, -8.3618e-01],\n",
       "             [-2.6378e-02, -4.3790e-01,  5.4692e-01,  ..., -5.2463e-03,\n",
       "               2.5328e-01, -1.4716e+00]],\n",
       "  \n",
       "            [[ 1.4156e-01,  1.0304e-01,  5.9696e-02,  ..., -4.3525e-02,\n",
       "               2.4092e-02, -3.3962e-02],\n",
       "             [ 6.8187e-02,  5.5612e-01, -7.5028e-01,  ..., -3.4026e-01,\n",
       "               2.9871e-01, -1.4040e+00],\n",
       "             [ 4.5204e-01,  3.0772e-01, -1.2906e-01,  ..., -1.0082e+00,\n",
       "              -2.6068e-01, -8.4508e-01],\n",
       "             ...,\n",
       "             [ 8.5006e-01,  1.0708e-01, -4.5580e-01,  ..., -5.0894e-01,\n",
       "              -8.9236e-02, -7.4427e-01],\n",
       "             [-6.1613e-01,  3.1678e-01, -7.9129e-01,  ..., -3.8556e-01,\n",
       "               8.8199e-04, -1.3910e+00],\n",
       "             [ 8.0914e-01,  1.1730e-01, -4.7282e-01,  ..., -5.1120e-01,\n",
       "              -1.0140e-01, -7.3145e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 1.6141e-02,  4.0214e-01, -2.0166e-01,  ...,  1.6615e-01,\n",
       "               2.5353e-01, -6.4479e-01],\n",
       "             [ 8.0857e-01, -3.6461e-02, -1.1835e+00,  ..., -1.9934e-01,\n",
       "              -1.7127e+00,  4.0425e-01],\n",
       "             [ 9.6605e-01, -8.8255e-01,  4.7908e-01,  ...,  5.2032e-01,\n",
       "              -1.9165e+00, -1.3385e+00],\n",
       "             ...,\n",
       "             [ 4.2081e+00, -4.3530e-01,  1.4359e+00,  ...,  3.9579e+00,\n",
       "               4.7798e+00, -1.0570e+00],\n",
       "             [ 4.5705e+00,  4.0026e-01,  7.7374e-01,  ...,  3.6120e+00,\n",
       "               5.2667e+00, -1.3971e-01],\n",
       "             [ 4.3024e+00, -3.5679e-01,  1.4794e+00,  ...,  3.9988e+00,\n",
       "               5.0204e+00, -1.1465e+00]],\n",
       "  \n",
       "            [[-1.1129e-01, -5.0358e-01,  3.5406e-01,  ..., -2.7672e-01,\n",
       "              -4.6886e-01,  1.9271e-01],\n",
       "             [-3.0500e-01,  5.2277e+00,  5.3263e-01,  ...,  1.9336e+00,\n",
       "               1.1235e+00,  7.0536e-01],\n",
       "             [ 3.4710e-01,  2.8968e+00,  1.6316e+00,  ...,  6.5483e-01,\n",
       "               8.1731e-01,  3.8100e-03],\n",
       "             ...,\n",
       "             [-2.4028e+00, -5.6856e+00,  5.9516e+00,  ..., -9.8585e-02,\n",
       "               3.7800e+00, -6.2792e-01],\n",
       "             [-3.3216e+00, -4.8009e+00,  6.5393e+00,  ...,  1.1598e+00,\n",
       "               3.9386e+00,  1.3036e-01],\n",
       "             [-2.5365e+00, -5.9112e+00,  5.9930e+00,  ..., -1.2175e-01,\n",
       "               3.7828e+00, -5.7390e-01]],\n",
       "  \n",
       "            [[ 1.5179e-01, -4.6314e-02, -1.6434e-01,  ..., -1.9313e-01,\n",
       "              -5.5654e-01, -3.4469e-01],\n",
       "             [-1.1374e-02, -2.2429e+00,  1.1760e-01,  ...,  3.0295e-01,\n",
       "              -6.4599e-01, -1.5921e+00],\n",
       "             [-7.9938e-01, -1.3096e+00,  8.3456e-02,  ..., -1.8451e+00,\n",
       "              -1.5635e+00, -1.3109e+00],\n",
       "             ...,\n",
       "             [-7.1534e-01, -3.3939e+00, -2.6312e-01,  ..., -1.7791e+00,\n",
       "              -3.6649e+00, -2.8745e+00],\n",
       "             [ 1.4202e-02, -4.1120e+00,  4.3337e-01,  ..., -1.2172e+00,\n",
       "              -2.8369e+00, -2.2511e+00],\n",
       "             [-6.5960e-01, -3.4302e+00, -2.5048e-01,  ..., -1.7801e+00,\n",
       "              -3.7233e+00, -2.8359e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 9.5145e-02, -1.4019e-02,  1.6208e-01,  ...,  1.4838e-01,\n",
       "              -6.9859e-02, -2.1973e-01],\n",
       "             [ 5.1006e-01, -1.0189e+00,  1.6536e+00,  ..., -2.2259e-02,\n",
       "              -5.9242e-01,  2.5183e-01],\n",
       "             [ 7.2814e-01, -5.6520e-01,  1.0891e+00,  ..., -3.2481e-01,\n",
       "              -7.0000e-01,  5.2532e-01],\n",
       "             ...,\n",
       "             [ 3.6993e+00, -2.8455e+00, -3.3613e-02,  ..., -4.1317e-01,\n",
       "              -5.0132e-01, -1.8119e+00],\n",
       "             [ 5.1440e+00, -3.5246e+00, -4.3941e-01,  ...,  3.9153e-01,\n",
       "              -7.9707e-01, -2.3939e+00],\n",
       "             [ 3.8572e+00, -2.8988e+00, -2.0321e-02,  ..., -4.0340e-01,\n",
       "              -5.2071e-01, -1.8964e+00]],\n",
       "  \n",
       "            [[ 2.2533e-01, -1.1276e-01,  2.4039e-01,  ...,  2.4186e-01,\n",
       "               3.9230e-03,  1.3308e-01],\n",
       "             [ 1.8568e+00, -7.0363e-01,  5.6584e-01,  ..., -9.5466e-01,\n",
       "              -7.0394e-01,  7.9250e-01],\n",
       "             [ 7.9223e-01, -5.1518e-01,  1.4123e-01,  ...,  3.0214e-01,\n",
       "              -2.0003e+00, -1.3828e-01],\n",
       "             ...,\n",
       "             [ 4.9774e+00, -1.0434e+00, -6.5068e-01,  ..., -2.6522e-01,\n",
       "              -1.9447e+00,  1.3932e+00],\n",
       "             [ 7.0939e+00, -4.5766e-01, -5.8808e-01,  ..., -1.1504e+00,\n",
       "              -1.2180e+00,  1.2815e+00],\n",
       "             [ 5.1196e+00, -1.0972e+00, -6.2979e-01,  ..., -4.3470e-01,\n",
       "              -1.9564e+00,  1.4618e+00]],\n",
       "  \n",
       "            [[ 1.9628e-01, -2.1743e-01, -5.4668e-01,  ..., -1.5320e-01,\n",
       "              -2.7920e-01,  2.3376e-01],\n",
       "             [ 1.2563e+00,  3.3390e+00, -7.0119e-01,  ..., -6.6875e-01,\n",
       "              -9.8511e-01, -3.8920e-01],\n",
       "             [ 5.3804e-01,  2.1276e+00, -8.1373e-01,  ..., -7.7831e-01,\n",
       "               8.3867e-01,  1.0519e+00],\n",
       "             ...,\n",
       "             [-1.4767e+00,  6.1488e-01, -1.5700e+00,  ...,  1.3528e+00,\n",
       "               1.8489e-01,  2.0690e+00],\n",
       "             [-1.2938e+00,  1.5394e+00, -9.2237e-01,  ...,  4.5599e-01,\n",
       "              -3.9254e-01,  6.7409e-01],\n",
       "             [-1.5100e+00,  5.1334e-01, -1.5493e+00,  ...,  1.3725e+00,\n",
       "               2.2794e-01,  2.0244e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-1.4793e-02,  7.3524e-02,  4.5829e-02,  ..., -1.1554e-02,\n",
       "              -5.7882e-02,  5.2907e-02],\n",
       "             [-1.2772e+00, -3.3786e-01, -1.1363e+00,  ...,  1.0080e+00,\n",
       "              -6.9762e-01, -1.7991e-01],\n",
       "             [-9.8204e-01, -3.0691e-01, -3.8778e-01,  ...,  1.8543e-01,\n",
       "              -1.0096e-01,  2.8559e-01],\n",
       "             ...,\n",
       "             [-7.6848e-01, -8.7848e-01, -7.2359e-01,  ..., -7.1192e-01,\n",
       "              -1.1538e-01, -2.3633e-01],\n",
       "             [-1.3124e+00, -9.9143e-01, -1.5553e+00,  ...,  3.7022e-01,\n",
       "              -5.0282e-01, -3.2339e-01],\n",
       "             [-7.5191e-01, -8.9060e-01, -7.4888e-01,  ..., -7.5845e-01,\n",
       "              -9.6685e-02, -2.3739e-01]],\n",
       "  \n",
       "            [[-1.1834e-01, -5.5375e-02,  8.5329e-02,  ..., -1.4903e-01,\n",
       "               2.6376e-02,  1.6399e-01],\n",
       "             [ 6.3696e-01,  1.5603e+00, -6.8002e-01,  ...,  4.9880e-01,\n",
       "              -1.2674e+00, -6.9325e-01],\n",
       "             [ 6.1338e-02,  6.6575e-01, -3.0962e-01,  ...,  1.4828e-01,\n",
       "               8.7651e-01,  1.8279e-01],\n",
       "             ...,\n",
       "             [ 1.6080e-01, -5.2305e-01, -3.2931e-01,  ...,  1.4840e-01,\n",
       "               1.4074e-01,  4.7458e-03],\n",
       "             [-6.3594e-01,  1.3913e-02, -3.1687e-03,  ...,  7.6394e-02,\n",
       "              -3.9832e-01, -3.6468e-01],\n",
       "             [ 4.6908e-02, -5.3697e-01, -3.0798e-01,  ...,  1.4127e-01,\n",
       "               1.6072e-01, -4.2524e-02]],\n",
       "  \n",
       "            [[-9.6596e-02, -2.2499e-01,  2.0475e-01,  ...,  6.6423e-02,\n",
       "              -3.0584e-02,  2.2477e-01],\n",
       "             [-4.7936e-01, -1.1253e+00, -1.0839e+00,  ..., -4.5152e-01,\n",
       "              -9.4806e-02,  5.9853e-02],\n",
       "             [ 6.1400e-01, -6.7186e-01,  4.9245e-01,  ..., -6.7523e-01,\n",
       "               2.2928e-01,  1.5224e+00],\n",
       "             ...,\n",
       "             [ 6.1487e-01, -8.2942e-01,  4.3423e-01,  ..., -3.0143e-01,\n",
       "               4.6173e-01,  7.8557e-01],\n",
       "             [-1.4434e+00, -1.7917e-01, -5.9628e-01,  ..., -1.2410e-01,\n",
       "               3.3434e-01, -2.2182e-02],\n",
       "             [ 5.7284e-01, -7.9262e-01,  3.9897e-01,  ..., -3.0192e-01,\n",
       "               5.2859e-01,  7.6397e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-1.2443e-03,  8.5336e-02, -1.9872e-01,  ...,  4.0273e-02,\n",
       "              -1.5836e-01, -2.1388e-01],\n",
       "             [-4.1405e-01, -7.2114e-01, -3.8220e-01,  ..., -1.9422e-01,\n",
       "               5.5711e-01, -1.1581e-01],\n",
       "             [-5.1784e-01, -1.0053e-01, -2.1552e+00,  ...,  1.0807e+00,\n",
       "              -1.6824e+00, -1.3177e+00],\n",
       "             ...,\n",
       "             [-6.9235e-01, -6.1264e-01, -9.7350e-01,  ..., -6.7548e-03,\n",
       "              -1.1592e+00, -1.8305e+00],\n",
       "             [-7.5757e-01, -3.3887e-01, -6.8633e-01,  ..., -3.2268e-01,\n",
       "               1.2986e+00,  1.8509e-01],\n",
       "             [-6.9719e-01, -5.5009e-01, -9.4186e-01,  ..., -1.0114e-02,\n",
       "              -1.1904e+00, -1.7452e+00]],\n",
       "  \n",
       "            [[-8.8972e-03,  4.0978e-02, -1.3444e-01,  ...,  7.5752e-02,\n",
       "              -2.2658e-01,  3.1502e-03],\n",
       "             [ 4.1815e-01, -5.8090e-01, -2.9797e-01,  ...,  7.1130e-01,\n",
       "               7.8117e-01,  1.2556e+00],\n",
       "             [ 8.4496e-01, -7.0487e-01, -5.1064e-01,  ..., -7.1248e-01,\n",
       "               1.2750e+00,  6.7227e-01],\n",
       "             ...,\n",
       "             [ 5.5512e-01, -1.8391e+00, -9.0667e-01,  ..., -5.9074e-01,\n",
       "               8.6677e-01,  1.5167e-01],\n",
       "             [-1.0410e-01, -9.6551e-01, -7.8450e-01,  ...,  1.7369e-01,\n",
       "               2.4163e-01,  5.1076e-01],\n",
       "             [ 5.6207e-01, -1.8085e+00, -8.5340e-01,  ..., -5.7552e-01,\n",
       "               7.6965e-01,  1.8303e-01]],\n",
       "  \n",
       "            [[-5.1970e-02, -4.6209e-02,  1.6716e-01,  ...,  1.0012e-01,\n",
       "              -2.4189e-02, -3.5561e-02],\n",
       "             [ 1.2953e+00,  4.1384e-01, -3.6409e-01,  ...,  1.2637e-01,\n",
       "              -1.8758e+00,  3.0507e-01],\n",
       "             [ 3.4642e-01, -6.4915e-01,  8.6135e-01,  ..., -7.9244e-01,\n",
       "               8.5907e-01, -1.3510e+00],\n",
       "             ...,\n",
       "             [-1.3831e-01,  8.3662e-02,  7.0042e-01,  ...,  3.8570e-01,\n",
       "               7.4740e-01, -1.0022e+00],\n",
       "             [ 9.8044e-01,  1.2741e+00,  3.7253e-02,  ...,  8.5164e-01,\n",
       "              -1.0858e+00, -1.0824e-01],\n",
       "             [-1.2115e-01,  1.0846e-01,  7.2400e-01,  ...,  3.6335e-01,\n",
       "               7.2299e-01, -9.2417e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-1.7704e-01,  2.5098e-01, -1.3918e-01,  ...,  1.0624e-01,\n",
       "              -3.9388e-01, -3.4023e-01],\n",
       "             [-1.6987e-01,  1.4030e-01,  1.0399e-01,  ...,  3.1858e-01,\n",
       "              -3.9741e-01,  1.9997e+00],\n",
       "             [-8.0378e-01,  2.6730e-01, -2.9187e-01,  ...,  9.7025e-01,\n",
       "              -5.4433e-01,  2.3530e-01],\n",
       "             ...,\n",
       "             [ 2.3442e-01,  2.5107e+00, -3.5300e+00,  ...,  1.4958e+00,\n",
       "               1.1240e+00, -2.7029e-01],\n",
       "             [ 7.3228e-01,  2.2478e+00, -3.0260e+00,  ...,  1.3465e+00,\n",
       "               1.1869e+00,  1.4907e+00],\n",
       "             [ 3.0713e-01,  2.5970e+00, -3.7267e+00,  ...,  1.4601e+00,\n",
       "               1.1870e+00, -3.0428e-01]],\n",
       "  \n",
       "            [[-4.5692e-01, -4.5598e-01,  4.6643e-01,  ..., -6.5904e-01,\n",
       "              -1.9306e-01, -5.6085e-01],\n",
       "             [ 3.4918e-01, -5.3679e-02, -1.2179e+00,  ..., -4.9807e-01,\n",
       "              -2.1204e-01,  9.1124e-01],\n",
       "             [ 7.9751e-01,  5.2607e-01, -3.4294e-01,  ..., -3.7007e-01,\n",
       "               2.8444e-01, -9.3577e-01],\n",
       "             ...,\n",
       "             [ 2.4496e+00,  1.1858e+00, -3.1055e-01,  ...,  1.0241e+00,\n",
       "              -2.8110e+00, -5.0356e+00],\n",
       "             [ 2.2789e+00,  4.5656e-02, -4.5705e-01,  ...,  1.0229e+00,\n",
       "              -1.8912e+00, -4.4007e+00],\n",
       "             [ 2.4304e+00,  1.1487e+00, -3.6415e-01,  ...,  9.8561e-01,\n",
       "              -2.8631e+00, -5.1554e+00]],\n",
       "  \n",
       "            [[ 1.8130e-01,  2.0221e+00,  1.3832e+00,  ...,  5.1169e-01,\n",
       "               2.4475e-01,  7.3984e-02],\n",
       "             [-7.8292e-01, -2.1030e+00, -1.4412e+00,  ...,  1.3658e+00,\n",
       "              -2.3080e+00,  1.2692e+00],\n",
       "             [-3.3811e-01, -5.7479e-01, -9.9980e-01,  ...,  3.0638e-01,\n",
       "              -1.5366e+00,  1.5604e+00],\n",
       "             ...,\n",
       "             [-2.7012e+00,  1.1177e+01, -6.1279e-02,  ...,  9.2151e-01,\n",
       "              -1.1434e+00,  1.0019e-01],\n",
       "             [-4.1771e+00,  1.0677e+01, -5.1034e-01,  ...,  1.9042e+00,\n",
       "              -2.4349e+00,  1.3552e-01],\n",
       "             [-2.8427e+00,  1.1470e+01, -1.4490e-02,  ...,  9.7658e-01,\n",
       "              -1.1251e+00,  2.8623e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 3.7110e-02,  3.0914e-01,  4.5760e-01,  ...,  1.7316e-01,\n",
       "               8.9155e-03,  7.0790e-02],\n",
       "             [-2.8040e-01,  8.4698e-02,  1.1572e+00,  ...,  3.7928e-01,\n",
       "               7.9117e-01,  1.3659e+00],\n",
       "             [-4.2214e-01,  5.1926e-01,  8.5266e-01,  ...,  1.1217e+00,\n",
       "               4.2534e-01,  1.9072e+00],\n",
       "             ...,\n",
       "             [-3.4201e+00,  3.6615e-01,  3.6774e+00,  ...,  1.3953e+00,\n",
       "              -1.9797e+00,  2.0195e+00],\n",
       "             [-3.7788e+00,  2.4771e-01,  4.0974e+00,  ...,  1.1594e+00,\n",
       "              -2.7271e+00,  2.7430e+00],\n",
       "             [-3.4812e+00,  3.9650e-01,  3.7584e+00,  ...,  1.4194e+00,\n",
       "              -2.0235e+00,  2.0380e+00]],\n",
       "  \n",
       "            [[ 1.8872e-01,  4.7173e-02, -4.8206e-02,  ...,  9.8904e-02,\n",
       "              -9.0675e-03,  3.8710e-02],\n",
       "             [-7.4598e-01,  2.2388e+00,  4.8148e-01,  ...,  1.0424e+00,\n",
       "               1.7593e-01, -5.6965e-01],\n",
       "             [ 1.3636e+00,  2.3075e+00,  1.1324e+00,  ...,  1.4751e+00,\n",
       "               1.5005e+00, -1.4745e-01],\n",
       "             ...,\n",
       "             [ 1.0379e+00,  4.3442e-01,  9.4968e-01,  ...,  6.0330e-01,\n",
       "               1.1024e+00, -1.0083e+00],\n",
       "             [-7.8777e-01,  1.1156e+00,  1.2716e+00,  ...,  5.6302e-02,\n",
       "              -1.4833e-01, -9.6032e-01],\n",
       "             [ 1.0270e+00,  4.1547e-01,  1.0022e+00,  ...,  5.2029e-01,\n",
       "               1.0537e+00, -1.0053e+00]],\n",
       "  \n",
       "            [[ 7.9454e-02,  7.0977e-02,  3.5175e-02,  ...,  5.5455e-02,\n",
       "               3.2942e-01,  2.7173e-01],\n",
       "             [-1.5891e+00,  6.4638e-01,  3.0005e-01,  ..., -1.8071e-01,\n",
       "               1.1370e+00, -1.0484e+00],\n",
       "             [-1.3361e+00,  1.3833e+00, -9.3750e-01,  ...,  5.2912e-02,\n",
       "               2.0330e+00, -1.2787e+00],\n",
       "             ...,\n",
       "             [-4.0762e+00,  8.7766e-01, -2.0543e-01,  ...,  8.0790e-01,\n",
       "               2.6155e+00,  7.3779e-02],\n",
       "             [-4.4705e+00,  1.4958e+00, -2.5387e-01,  ...,  4.4849e-01,\n",
       "               1.4788e+00,  6.9613e-01],\n",
       "             [-4.1150e+00,  9.0303e-01, -2.1573e-01,  ...,  8.5942e-01,\n",
       "               2.6103e+00,  1.9477e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-2.4876e-02,  6.3550e-02,  2.7732e-02,  ...,  1.2231e-03,\n",
       "               2.5960e-01,  8.1726e-03],\n",
       "             [ 1.0898e+00,  2.0889e+00,  9.9452e-01,  ...,  5.2285e-01,\n",
       "              -1.7416e+00,  6.1093e-01],\n",
       "             [ 1.8767e-01,  9.9919e-01, -1.4214e+00,  ..., -8.6920e-01,\n",
       "               2.8190e-01,  4.3446e-01],\n",
       "             ...,\n",
       "             [-3.2562e-01,  1.2323e+00, -1.5686e+00,  ..., -1.9024e-01,\n",
       "               6.0371e-01,  9.4988e-02],\n",
       "             [-3.4477e-01,  2.4342e+00,  1.4290e+00,  ...,  6.3836e-01,\n",
       "              -1.7070e+00,  7.1635e-02],\n",
       "             [-2.8222e-01,  1.2455e+00, -1.5270e+00,  ..., -1.3148e-01,\n",
       "               5.9510e-01,  9.1886e-02]],\n",
       "  \n",
       "            [[ 1.5006e-01,  8.6397e-02, -2.8647e-01,  ..., -7.7012e-02,\n",
       "               9.6815e-02, -1.4891e-01],\n",
       "             [ 1.7671e+00,  1.8874e-02,  4.0821e-01,  ...,  7.5981e-01,\n",
       "               6.7134e-01,  1.3102e+00],\n",
       "             [ 1.0771e+00, -3.3804e-01, -1.2949e+00,  ...,  3.2510e-01,\n",
       "               1.1067e+00, -1.0542e+00],\n",
       "             ...,\n",
       "             [ 1.9251e+00, -1.4540e+00, -6.8589e-01,  ...,  4.9009e-01,\n",
       "               2.9731e-01, -1.1636e+00],\n",
       "             [ 2.6139e+00, -1.1586e+00, -9.8465e-02,  ...,  9.2550e-01,\n",
       "               4.5506e-01,  1.0919e+00],\n",
       "             [ 2.0306e+00, -1.4759e+00, -7.1596e-01,  ...,  5.8672e-01,\n",
       "               3.2487e-01, -1.1108e+00]],\n",
       "  \n",
       "            [[ 1.7615e-01, -7.7676e-02,  7.6348e-03,  ..., -7.3023e-02,\n",
       "               1.6742e-03,  9.7209e-03],\n",
       "             [-1.2699e+00,  2.3622e+00,  2.3304e-01,  ...,  1.2093e+00,\n",
       "              -1.1019e+00,  2.9163e-02],\n",
       "             [-1.5788e-01,  2.2549e+00,  6.3066e-01,  ..., -1.9863e+00,\n",
       "              -4.3600e-01, -7.6353e-02],\n",
       "             ...,\n",
       "             [-6.0184e-01,  8.1608e-01,  4.3826e-01,  ..., -1.9377e+00,\n",
       "              -8.2105e-01,  2.5787e-01],\n",
       "             [-1.8052e+00,  3.0268e+00,  3.5748e-02,  ..., -6.3478e-01,\n",
       "              -1.2507e-01,  2.0221e-01],\n",
       "             [-6.6504e-01,  7.2733e-01,  5.0961e-01,  ..., -1.9192e+00,\n",
       "              -7.7049e-01,  2.6718e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-9.1487e-02, -1.1333e-01,  1.7586e-01,  ..., -1.1592e-01,\n",
       "               1.1683e-01,  1.9515e-01],\n",
       "             [-1.7625e+00, -6.8811e-01, -1.1842e+00,  ...,  1.1516e+00,\n",
       "               5.2354e-01,  2.1580e+00],\n",
       "             [-1.5248e+00,  2.5224e-01, -4.5799e-01,  ..., -7.6002e-01,\n",
       "              -7.3545e-01,  1.8108e+00],\n",
       "             ...,\n",
       "             [-1.3440e+00,  7.2174e-01,  5.9245e-01,  ..., -6.7168e-01,\n",
       "              -3.8330e-01,  5.0656e-01],\n",
       "             [-1.8809e+00, -2.2718e-01, -6.3792e-01,  ...,  5.5448e-01,\n",
       "               8.4136e-01,  1.6922e+00],\n",
       "             [-1.3130e+00,  6.7752e-01,  5.9598e-01,  ..., -7.0757e-01,\n",
       "              -3.8928e-01,  5.3699e-01]],\n",
       "  \n",
       "            [[ 1.5977e-01, -2.4339e-03,  1.7304e-01,  ...,  1.5837e-01,\n",
       "              -5.6212e-02,  7.2709e-02],\n",
       "             [ 7.0068e-01, -2.7764e-01,  1.3608e+00,  ...,  1.6692e+00,\n",
       "               1.2503e+00, -1.0444e+00],\n",
       "             [ 1.6338e-01,  9.1919e-01,  1.7184e+00,  ...,  4.8779e-01,\n",
       "               1.6966e+00, -1.6504e-01],\n",
       "             ...,\n",
       "             [ 5.4991e-01,  6.3887e-01,  1.7825e-01,  ...,  1.0669e+00,\n",
       "               6.7639e-01, -3.3838e-01],\n",
       "             [ 7.1695e-01,  3.6230e-01,  1.4782e+00,  ...,  1.4439e+00,\n",
       "               2.8586e-01, -8.2015e-01],\n",
       "             [ 5.8005e-01,  5.8963e-01,  1.7634e-01,  ...,  1.1358e+00,\n",
       "               6.3790e-01, -3.1256e-01]],\n",
       "  \n",
       "            [[-1.3735e-01, -5.0504e-02,  7.0306e-02,  ...,  5.5682e-02,\n",
       "               8.5924e-02, -1.5147e-02],\n",
       "             [-1.1011e+00,  1.2676e-01, -1.0486e+00,  ..., -7.0738e-03,\n",
       "               1.1977e+00, -7.8149e-01],\n",
       "             [-4.9343e-01, -1.2560e+00, -5.2777e-02,  ...,  7.2532e-01,\n",
       "               7.7925e-01,  3.7413e-01],\n",
       "             ...,\n",
       "             [ 1.4137e-01, -2.3489e-01,  3.3385e-01,  ..., -2.7325e-02,\n",
       "               1.3163e-01,  9.3181e-02],\n",
       "             [-5.4350e-01, -9.1463e-01, -7.8626e-01,  ...,  4.5531e-01,\n",
       "               1.3796e-01, -3.2892e-01],\n",
       "             [ 1.1929e-01, -2.4079e-01,  3.7288e-01,  ..., -2.8434e-02,\n",
       "               4.0973e-02,  8.1618e-02]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[-5.4882e-01, -4.6021e-01,  2.0816e-01,  ...,  3.2185e-02,\n",
       "               1.9677e-01, -4.0632e-01],\n",
       "             [-1.0208e+00,  3.4483e-01, -5.4078e+00,  ...,  1.0608e+00,\n",
       "              -8.1824e-01,  1.1844e-01],\n",
       "             [-1.2334e+00,  4.0289e-01, -4.1039e+00,  ...,  2.7255e-01,\n",
       "              -9.4773e-01, -7.6409e-01],\n",
       "             ...,\n",
       "             [-9.0423e-01,  1.4264e+00, -2.3019e-01,  ..., -8.1958e-01,\n",
       "               1.2976e-01, -1.0081e+00],\n",
       "             [ 2.7279e-01,  1.6665e+00, -1.7681e+00,  ..., -5.6377e-01,\n",
       "              -1.2213e-02, -4.4814e-01],\n",
       "             [-8.7353e-01,  1.4625e+00, -7.5520e-02,  ..., -8.5398e-01,\n",
       "               1.3444e-01, -1.0402e+00]],\n",
       "  \n",
       "            [[ 1.7843e+00,  1.7307e-01,  1.8474e+00,  ...,  1.3269e+00,\n",
       "              -8.6536e-01, -1.1275e+00],\n",
       "             [-1.7347e+00, -4.2126e-01,  1.8031e+00,  ..., -9.7250e-01,\n",
       "              -1.2086e-02, -8.2845e-03],\n",
       "             [ 1.2622e+00, -8.0226e-01,  1.9230e+00,  ...,  1.0388e+00,\n",
       "               9.7922e-01, -3.4361e-01],\n",
       "             ...,\n",
       "             [ 6.4515e+00, -2.5691e+00,  7.3559e+00,  ...,  7.6330e+00,\n",
       "              -3.2111e+00, -3.2274e+00],\n",
       "             [ 5.9927e+00, -2.9345e+00,  8.1582e+00,  ...,  6.6971e+00,\n",
       "              -4.7485e+00, -2.6457e+00],\n",
       "             [ 6.6104e+00, -2.6805e+00,  7.5273e+00,  ...,  7.8273e+00,\n",
       "              -3.3536e+00, -3.2021e+00]],\n",
       "  \n",
       "            [[ 1.3848e-01,  1.3382e-01, -1.0545e-01,  ...,  1.8984e-01,\n",
       "              -3.4012e-01, -2.5155e-02],\n",
       "             [-9.5574e-01, -2.9557e-01,  1.5038e+00,  ...,  4.4021e-01,\n",
       "              -1.8121e-01,  1.2305e+00],\n",
       "             [-1.1769e-01,  1.5981e-01, -2.7390e-01,  ...,  5.8114e-02,\n",
       "               8.0108e-01, -7.8723e-02],\n",
       "             ...,\n",
       "             [ 2.5359e+00, -1.1967e+00,  4.8737e-01,  ...,  1.0169e+00,\n",
       "               3.1231e+00,  1.3402e-01],\n",
       "             [ 2.0860e+00, -1.3404e+00,  1.4930e+00,  ...,  1.6625e+00,\n",
       "               3.3391e+00,  6.2623e-01],\n",
       "             [ 2.5595e+00, -1.2567e+00,  4.8819e-01,  ...,  1.1011e+00,\n",
       "               3.1878e+00,  1.1695e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-1.4330e-01, -1.1125e+00,  1.2784e-01,  ..., -9.6856e-02,\n",
       "               3.9662e-02,  7.5653e-02],\n",
       "             [-4.6940e-01,  3.0301e+00, -1.8223e+00,  ..., -5.6182e-01,\n",
       "              -1.9719e+00, -4.3371e-01],\n",
       "             [-1.3592e+00,  7.8946e-01, -6.9511e-01,  ...,  8.3361e-01,\n",
       "              -1.0995e+00,  1.5486e-01],\n",
       "             ...,\n",
       "             [-1.4273e+00, -3.0889e+00, -1.7460e-01,  ...,  2.0498e+00,\n",
       "              -2.1883e+00, -1.8069e+00],\n",
       "             [-1.3390e+00, -2.2423e+00, -3.2350e-01,  ...,  1.6105e+00,\n",
       "              -3.9416e+00, -2.6857e+00],\n",
       "             [-1.3849e+00, -3.2182e+00, -1.4738e-01,  ...,  2.0622e+00,\n",
       "              -2.2773e+00, -1.8409e+00]],\n",
       "  \n",
       "            [[ 1.3482e-01, -2.9507e-01,  5.6703e-01,  ...,  1.0898e+00,\n",
       "               3.2482e-01, -2.3599e-02],\n",
       "             [-5.6551e-01, -6.4587e-01, -1.2605e+00,  ..., -4.7155e+00,\n",
       "              -2.9846e-01, -8.6159e-01],\n",
       "             [-1.1860e-02, -1.5711e+00, -1.3479e+00,  ..., -1.2150e+00,\n",
       "               4.4109e-01, -1.5147e+00],\n",
       "             ...,\n",
       "             [ 1.4615e+00, -1.3878e+00,  1.8757e+00,  ...,  3.5135e+00,\n",
       "              -6.5171e-01, -4.9758e-01],\n",
       "             [ 4.3581e-01, -8.9862e-01,  1.8466e+00,  ...,  2.4336e+00,\n",
       "              -9.1073e-01, -8.0294e-02],\n",
       "             [ 1.5089e+00, -1.4183e+00,  1.9243e+00,  ...,  3.7652e+00,\n",
       "              -6.5864e-01, -3.8773e-01]],\n",
       "  \n",
       "            [[ 9.2106e-01, -6.0047e-01,  8.7529e-02,  ...,  4.1618e-01,\n",
       "               4.3836e-01,  1.6913e-01],\n",
       "             [ 1.4848e+00, -1.8421e+00,  1.2808e+00,  ...,  1.0000e+00,\n",
       "              -1.1346e+00,  1.2150e+00],\n",
       "             [ 1.4640e+00, -3.7169e-01, -1.2154e-02,  ...,  1.1695e+00,\n",
       "               4.9029e-01,  5.4550e-01],\n",
       "             ...,\n",
       "             [ 1.2602e+00, -1.8831e+00, -1.4602e+00,  ...,  2.8330e+00,\n",
       "               1.6484e+00, -1.4097e+00],\n",
       "             [ 1.7661e+00, -2.6651e+00, -1.0273e+00,  ...,  2.1976e+00,\n",
       "              -5.0749e-02, -1.7200e+00],\n",
       "             [ 1.2591e+00, -1.8880e+00, -1.4761e+00,  ...,  2.8063e+00,\n",
       "               1.5843e+00, -1.4621e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-1.4193e-01,  2.1976e-01,  1.2551e-01,  ..., -9.5327e-02,\n",
       "              -1.0484e-01, -8.7670e-02],\n",
       "             [ 1.7174e+00,  2.0334e+00, -9.2068e-01,  ...,  1.2619e-01,\n",
       "               1.9880e+00, -1.7541e+00],\n",
       "             [ 3.2213e-01,  9.6757e-01,  4.8397e-01,  ...,  6.8499e-01,\n",
       "               4.6540e-01,  1.1371e-01],\n",
       "             ...,\n",
       "             [ 3.8857e-01,  6.7228e-01,  1.9031e+00,  ...,  8.2911e-01,\n",
       "              -6.1086e-02,  3.0047e-01],\n",
       "             [ 1.4374e+00,  1.8598e+00,  3.8459e-01,  ...,  1.3647e+00,\n",
       "               1.0759e+00, -1.5020e+00],\n",
       "             [ 4.4441e-01,  6.4935e-01,  1.8172e+00,  ...,  7.1834e-01,\n",
       "              -9.0189e-02,  2.0138e-01]],\n",
       "  \n",
       "            [[-1.6054e-01, -2.9684e-01, -1.8202e-01,  ..., -8.9942e-02,\n",
       "              -4.9850e-02, -7.4437e-02],\n",
       "             [ 1.2724e+00,  2.4158e+00,  8.4902e-01,  ..., -7.9608e-03,\n",
       "              -7.3562e-01, -2.3337e-01],\n",
       "             [ 2.9652e-01, -2.7902e-01, -4.7435e-01,  ..., -1.2875e+00,\n",
       "               5.5154e-01, -1.1449e-02],\n",
       "             ...,\n",
       "             [-7.9831e-01, -1.1952e-01, -7.7449e-01,  ..., -1.9535e+00,\n",
       "              -1.6713e-01, -4.9686e-01],\n",
       "             [-2.8111e-01,  8.3772e-01, -9.8066e-01,  ..., -1.9979e+00,\n",
       "              -7.4465e-01, -1.7014e-01],\n",
       "             [-8.3804e-01, -1.5263e-01, -7.6826e-01,  ..., -1.9346e+00,\n",
       "              -1.5150e-01, -4.9915e-01]],\n",
       "  \n",
       "            [[-1.7407e-01, -3.7663e-02,  1.0152e-01,  ...,  6.1656e-02,\n",
       "               1.8936e-01,  1.3889e-01],\n",
       "             [ 2.5385e-01,  5.5175e-01,  3.8710e-01,  ..., -4.9093e-01,\n",
       "              -7.3466e-01,  1.0957e+00],\n",
       "             [ 1.1181e-01, -2.1531e-01, -8.0064e-01,  ...,  2.3635e-02,\n",
       "              -2.8929e-01, -6.5819e-01],\n",
       "             ...,\n",
       "             [-8.4514e-02,  2.8505e-01, -5.6647e-01,  ...,  9.9958e-02,\n",
       "               1.1791e+00,  9.2019e-02],\n",
       "             [ 1.9857e-01,  9.6409e-01, -6.8282e-01,  ..., -3.1281e-01,\n",
       "               3.3856e-01,  1.5680e+00],\n",
       "             [-4.2736e-02,  3.1813e-01, -5.7075e-01,  ...,  9.7042e-02,\n",
       "               1.1830e+00,  9.7430e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-6.1854e-02,  1.5219e-01,  3.8176e-02,  ...,  1.1879e-01,\n",
       "              -1.0173e-01,  2.5631e-01],\n",
       "             [ 2.5597e-01, -1.5963e+00, -3.8434e-01,  ..., -9.2291e-01,\n",
       "              -1.6733e-01, -1.0866e+00],\n",
       "             [ 4.3841e-01,  6.9563e-01,  7.5351e-01,  ...,  1.5627e-01,\n",
       "               1.3347e+00,  1.0270e+00],\n",
       "             ...,\n",
       "             [ 1.2977e+00, -6.3770e-01,  1.2925e+00,  ...,  4.2165e-01,\n",
       "               6.1576e-01,  1.3251e+00],\n",
       "             [ 4.9071e-01, -2.2035e+00,  8.9428e-02,  ..., -1.0772e-01,\n",
       "              -8.1806e-01, -1.1161e-01],\n",
       "             [ 1.3188e+00, -6.5788e-01,  1.2972e+00,  ...,  4.6899e-01,\n",
       "               5.2662e-01,  1.3232e+00]],\n",
       "  \n",
       "            [[ 8.0342e-02,  3.1236e-02,  1.5771e-01,  ...,  1.8027e-01,\n",
       "               2.7733e-02,  1.5062e-01],\n",
       "             [-4.8490e-01, -2.1014e-01,  1.5100e-01,  ...,  1.6893e+00,\n",
       "               1.5314e+00, -3.8153e-01],\n",
       "             [-1.4744e+00,  3.7450e-01,  8.2778e-01,  ...,  1.0125e+00,\n",
       "               1.3094e+00,  1.6447e-01],\n",
       "             ...,\n",
       "             [-6.0433e-01,  1.4829e-01,  3.6789e-01,  ...,  1.0004e+00,\n",
       "               2.1375e+00, -6.7600e-01],\n",
       "             [-5.0285e-01,  1.5441e-01,  9.8772e-01,  ...,  1.7075e+00,\n",
       "               1.1775e+00, -3.1753e-01],\n",
       "             [-5.2505e-01,  1.3058e-01,  4.1900e-01,  ...,  9.7927e-01,\n",
       "               2.0930e+00, -6.5377e-01]],\n",
       "  \n",
       "            [[-1.0647e-01, -1.0394e-01, -1.2336e-01,  ...,  2.2990e-01,\n",
       "              -7.5569e-02, -8.2221e-02],\n",
       "             [-1.5905e-01,  3.1457e-01,  3.1631e-01,  ..., -5.9599e-01,\n",
       "              -6.2033e-01,  1.9923e-01],\n",
       "             [-6.1743e-01, -6.5234e-02, -1.3838e-01,  ..., -2.1462e-01,\n",
       "              -9.4081e-01, -1.0362e+00],\n",
       "             ...,\n",
       "             [-5.5370e-01,  2.1646e-01, -1.1931e-01,  ...,  3.3600e-01,\n",
       "              -9.0798e-01, -3.7384e-01],\n",
       "             [-1.5450e-01, -3.3756e-01,  1.2874e+00,  ..., -3.5643e-01,\n",
       "               1.3048e-01,  5.9807e-01],\n",
       "             [-5.1705e-01,  2.3171e-01, -6.3643e-02,  ...,  3.3825e-01,\n",
       "              -8.8192e-01, -3.9846e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 1.8027e-01,  7.5461e-04,  2.3850e-01,  ...,  1.3871e+00,\n",
       "               2.9911e-01,  2.1594e-01],\n",
       "             [-7.8824e-01,  4.3092e-01,  1.8194e+00,  ..., -2.6896e+00,\n",
       "              -2.5671e+00, -1.7007e-01],\n",
       "             [-2.8774e-01,  1.2918e+00,  6.2638e-01,  ..., -2.5072e+00,\n",
       "              -1.9606e-01, -7.8911e-01],\n",
       "             ...,\n",
       "             [-8.7987e-01,  1.4322e-01,  7.0794e-01,  ..., -3.2964e-02,\n",
       "              -5.0382e-01, -6.6921e-01],\n",
       "             [-1.0716e+00, -3.4049e-01,  1.6845e+00,  ...,  5.7890e-01,\n",
       "              -2.2676e+00, -2.7539e-01],\n",
       "             [-8.6890e-01,  4.8599e-02,  7.3749e-01,  ...,  1.3483e-01,\n",
       "              -5.3672e-01, -6.7282e-01]],\n",
       "  \n",
       "            [[ 5.4798e-02, -2.1423e-01,  7.4569e-01,  ...,  2.2229e-01,\n",
       "              -8.7351e-02,  2.0732e-01],\n",
       "             [ 1.8219e+00,  5.8821e-01, -2.0124e+00,  ...,  5.6362e-01,\n",
       "               2.5628e-01,  1.3906e+00],\n",
       "             [ 2.5506e-01, -1.2909e+00, -2.3909e+00,  ..., -2.7489e-01,\n",
       "               1.6423e+00,  1.2155e+00],\n",
       "             ...,\n",
       "             [ 1.7150e+00, -1.7978e+00, -4.7017e-01,  ...,  3.4562e-01,\n",
       "               2.1228e+00,  2.1023e+00],\n",
       "             [ 2.7706e+00, -1.7052e+00,  3.2495e-01,  ...,  5.4775e-01,\n",
       "               2.3870e+00,  2.2331e+00],\n",
       "             [ 1.7369e+00, -1.8217e+00, -3.1760e-01,  ...,  3.6189e-01,\n",
       "               2.1813e+00,  2.1321e+00]],\n",
       "  \n",
       "            [[ 8.7318e-02,  3.3052e-01,  6.2967e-01,  ...,  1.4984e-01,\n",
       "               1.8454e-01,  1.2409e-01],\n",
       "             [-1.2808e+00,  6.0221e-01, -2.0623e-01,  ...,  3.7366e-01,\n",
       "               1.2254e+00, -7.6948e-01],\n",
       "             [-1.7421e-01,  7.9473e-01,  3.3923e-01,  ..., -1.2332e+00,\n",
       "               7.6260e-01, -1.6328e-01],\n",
       "             ...,\n",
       "             [-1.6184e-01,  8.6737e-01,  5.7822e-01,  ..., -4.6709e-01,\n",
       "               6.6804e-01,  8.0891e-02],\n",
       "             [-9.6585e-01,  9.0728e-01, -5.2769e-01,  ...,  2.7699e-01,\n",
       "              -2.2544e-01,  3.0169e-01],\n",
       "             [-1.9067e-01,  8.4514e-01,  5.8380e-01,  ..., -4.6943e-01,\n",
       "               6.6226e-01,  5.1599e-02]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 2.6856e-01,  8.8726e-02,  3.2200e-01,  ..., -5.2333e-01,\n",
       "               1.2339e-02, -1.2732e-01],\n",
       "             [-4.1138e-01,  8.1252e-01, -9.5610e-01,  ...,  2.4976e-01,\n",
       "              -1.6647e+00,  4.8598e-01],\n",
       "             [ 9.8685e-01, -1.7914e+00, -9.0734e-01,  ..., -9.0234e-01,\n",
       "               1.2767e+00,  4.7515e-01],\n",
       "             ...,\n",
       "             [ 2.7181e-01, -3.6437e+00,  7.1021e-01,  ..., -1.3013e+00,\n",
       "               7.3182e-01, -2.0994e-01],\n",
       "             [ 1.7475e-01, -2.7206e+00,  4.8408e-01,  ..., -1.1127e+00,\n",
       "              -8.7733e-01,  2.6706e-01],\n",
       "             [ 2.0927e-01, -3.6462e+00,  7.6479e-01,  ..., -1.2971e+00,\n",
       "               7.2158e-01, -2.3324e-01]],\n",
       "  \n",
       "            [[-2.9320e-01,  6.2091e-02,  8.8453e-02,  ..., -3.6115e-01,\n",
       "              -3.5452e-02,  7.3534e-03],\n",
       "             [ 1.4868e+00,  2.2568e-01, -8.0983e-01,  ...,  1.8122e+00,\n",
       "               1.6197e+00,  2.5910e+00],\n",
       "             [ 8.1656e-01,  7.0976e-01, -6.3347e-02,  ...,  5.5968e-01,\n",
       "               1.3582e-01,  2.0041e-01],\n",
       "             ...,\n",
       "             [ 5.2555e-01,  2.6536e-01,  8.2882e-02,  ..., -2.1166e+00,\n",
       "               8.9868e-01, -2.3135e+00],\n",
       "             [ 2.1523e+00, -1.3651e-01, -1.6191e-01,  ..., -1.6805e+00,\n",
       "               1.3857e+00, -1.2237e+00],\n",
       "             [ 5.6323e-01,  2.4931e-01,  1.1835e-01,  ..., -2.2247e+00,\n",
       "               9.1748e-01, -2.4350e+00]],\n",
       "  \n",
       "            [[ 2.1357e-02, -2.3502e-01,  5.3432e-02,  ...,  1.2665e-01,\n",
       "               1.0988e-01, -1.8252e-01],\n",
       "             [ 1.1956e+00,  7.2363e-01,  6.2911e-02,  ..., -6.1963e-01,\n",
       "               1.1160e-01, -1.3090e-01],\n",
       "             [-3.1462e-01,  1.1621e+00,  2.3795e-01,  ...,  1.1620e+00,\n",
       "              -3.5170e-01,  5.7834e-01],\n",
       "             ...,\n",
       "             [-2.3509e+00,  1.0813e+00,  7.5645e-01,  ...,  8.9207e-01,\n",
       "              -4.9020e-01,  5.0270e-01],\n",
       "             [-2.1497e+00,  4.0503e-01,  4.1496e-01,  ..., -3.4130e-02,\n",
       "               5.1769e-01, -8.0151e-01],\n",
       "             [-2.4305e+00,  1.0686e+00,  7.1002e-01,  ...,  8.6243e-01,\n",
       "              -4.3973e-01,  5.0854e-01]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 1.6860e-01,  9.3417e-02, -1.3401e-01,  ...,  1.6013e-01,\n",
       "              -2.1127e-01,  6.0922e-02],\n",
       "             [-6.9370e-01, -1.1658e+00, -6.6914e-01,  ...,  7.5296e-01,\n",
       "              -1.2386e-01,  3.1232e-01],\n",
       "             [ 1.6283e+00, -3.0457e-01,  6.4167e-01,  ...,  9.8505e-01,\n",
       "               7.1228e-01,  3.8351e-01],\n",
       "             ...,\n",
       "             [ 1.5811e+00, -6.7422e-01,  3.0336e-01,  ...,  9.9743e-01,\n",
       "               7.3817e-01, -8.3021e-01],\n",
       "             [ 7.1862e-01, -7.5777e-01, -6.8733e-01,  ...,  9.8309e-01,\n",
       "               6.5858e-01, -1.0394e+00],\n",
       "             [ 1.6077e+00, -6.2839e-01,  2.7645e-01,  ...,  9.4632e-01,\n",
       "               8.1335e-01, -9.0022e-01]],\n",
       "  \n",
       "            [[-1.1954e-02, -1.1543e-01, -3.0793e-01,  ...,  5.6343e-02,\n",
       "              -3.3205e-01,  1.9663e-01],\n",
       "             [ 1.1990e+00, -7.0437e-01,  1.4361e+00,  ..., -1.3195e+00,\n",
       "              -1.2408e-01, -1.5006e+00],\n",
       "             [ 4.9311e-01,  5.4443e-01,  1.0154e+00,  ...,  8.1392e-01,\n",
       "              -6.9691e-01, -1.3208e-01],\n",
       "             ...,\n",
       "             [ 6.0459e-01,  3.8829e-03, -2.1965e-01,  ...,  9.2987e-01,\n",
       "              -4.0680e-01, -3.6630e-01],\n",
       "             [ 1.6519e+00, -6.8339e-01,  6.8179e-02,  ..., -9.6331e-01,\n",
       "               9.5155e-01, -1.5415e+00],\n",
       "             [ 5.4861e-01, -5.9308e-02, -2.2522e-01,  ...,  9.2706e-01,\n",
       "              -3.3390e-01, -3.9962e-01]],\n",
       "  \n",
       "            [[-5.1941e-02, -2.2396e-01, -2.0413e-01,  ..., -8.7052e-02,\n",
       "               5.6029e-02, -1.4984e-01],\n",
       "             [ 3.3276e-01,  1.6351e-01,  1.9216e-01,  ...,  1.4765e+00,\n",
       "              -9.2817e-01,  1.8998e+00],\n",
       "             [-1.7456e+00,  3.6567e-01, -1.2842e+00,  ..., -4.3035e-01,\n",
       "              -8.0880e-01,  5.2751e+00],\n",
       "             ...,\n",
       "             [-1.1211e+00,  6.4422e-01, -1.6814e+00,  ...,  2.0298e-01,\n",
       "              -1.7585e-01,  3.6025e+00],\n",
       "             [ 1.2902e+00, -4.3128e-01, -8.1505e-02,  ...,  1.0756e+00,\n",
       "              -3.6664e-01,  7.2480e-02],\n",
       "             [-1.0806e+00,  6.2305e-01, -1.6903e+00,  ...,  2.0071e-01,\n",
       "              -1.8220e-01,  3.4788e+00]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-3.8027e-01, -4.0654e-01,  3.1190e-01,  ...,  4.7975e-01,\n",
       "               2.1680e-01, -3.0893e-01],\n",
       "             [-1.2741e-01,  1.9662e-01,  1.2317e-02,  ..., -6.7248e-01,\n",
       "              -1.8503e+00, -9.4523e-01],\n",
       "             [-2.2417e-01, -2.0096e-01,  1.0359e+00,  ...,  3.3461e-01,\n",
       "              -3.9151e-01, -4.2620e-01],\n",
       "             ...,\n",
       "             [ 3.2101e-01, -2.0099e-01,  4.2574e-01,  ...,  1.8685e-01,\n",
       "              -6.1960e-01,  1.8035e-01],\n",
       "             [ 1.6138e-01,  4.5141e-01, -2.6567e-01,  ..., -4.4470e-01,\n",
       "              -2.2374e+00, -3.6056e-02],\n",
       "             [ 2.7775e-01, -1.6329e-01,  3.8470e-01,  ...,  1.4558e-01,\n",
       "              -6.3315e-01,  1.4248e-01]],\n",
       "  \n",
       "            [[ 1.1950e-01, -6.1611e-02, -1.8663e-01,  ...,  6.6358e-02,\n",
       "               7.8154e-02, -1.0299e-01],\n",
       "             [-2.8338e+00, -1.1946e+00,  5.1238e-01,  ..., -1.5235e+00,\n",
       "               5.0046e-02,  2.6793e-01],\n",
       "             [-6.8270e-01,  4.2171e-01, -2.9975e-01,  ...,  2.3492e-01,\n",
       "               9.1121e-01, -1.8729e-01],\n",
       "             ...,\n",
       "             [ 7.9067e-01,  4.7576e-01,  3.3231e-02,  ...,  6.5831e-01,\n",
       "               8.6210e-01,  1.8020e-01],\n",
       "             [-1.3089e+00, -1.7519e+00,  5.0876e-01,  ..., -5.0022e-02,\n",
       "               5.2008e-01,  7.4356e-01],\n",
       "             [ 8.2782e-01,  4.4182e-01,  8.7032e-03,  ...,  6.9404e-01,\n",
       "               9.0272e-01,  1.4671e-01]],\n",
       "  \n",
       "            [[-7.7069e-02,  1.0782e-01,  1.1658e-02,  ...,  6.1299e-02,\n",
       "              -9.2770e-02, -1.4468e-01],\n",
       "             [ 5.9941e-01,  1.4788e-01, -4.8524e-01,  ..., -1.5537e+00,\n",
       "              -8.7137e-01,  1.2707e-02],\n",
       "             [ 5.9306e-01,  2.6453e-02,  5.0147e-01,  ...,  2.2775e-01,\n",
       "               6.1684e-01,  3.9077e-01],\n",
       "             ...,\n",
       "             [ 4.6960e-01,  9.4719e-01,  2.9457e-01,  ..., -6.7474e-01,\n",
       "               8.0134e-01,  7.1939e-01],\n",
       "             [ 8.7411e-01,  6.4449e-01, -1.1855e+00,  ..., -2.1342e+00,\n",
       "              -7.4908e-01, -9.9778e-02],\n",
       "             [ 4.9481e-01,  9.4906e-01,  3.2488e-01,  ..., -6.4252e-01,\n",
       "               7.3193e-01,  7.3225e-01]]]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[[[ 6.6615e-02,  6.0832e-01, -4.7964e-01,  ...,  1.3632e-01,\n",
       "              -9.9073e-02, -9.0911e-01],\n",
       "             [ 3.6406e-01, -4.9409e-01, -2.9283e-01,  ...,  5.3283e-01,\n",
       "               1.8498e-02,  5.7925e-03],\n",
       "             [ 7.3907e-01, -1.7766e+00, -8.5893e-01,  ..., -1.2064e-02,\n",
       "              -3.8492e-02, -7.3187e-01],\n",
       "             ...,\n",
       "             [ 1.1354e+00, -2.1696e+00, -9.5136e-01,  ...,  1.1017e+00,\n",
       "               4.1215e-01, -8.2494e-01],\n",
       "             [ 1.2213e+00, -1.1367e+00, -2.2582e-01,  ...,  2.1398e+00,\n",
       "               5.6479e-01, -1.1099e+00],\n",
       "             [ 1.1778e+00, -2.1122e+00, -9.5004e-01,  ...,  1.1893e+00,\n",
       "               3.7140e-01, -8.9399e-01]],\n",
       "  \n",
       "            [[ 1.2187e-02,  7.8845e-01,  4.9964e-01,  ...,  5.8986e-02,\n",
       "               2.7161e-01, -4.4144e-01],\n",
       "             [ 7.8673e-01,  1.1204e+00,  3.5772e-01,  ..., -4.7365e-01,\n",
       "               3.2460e-01, -1.1671e+00],\n",
       "             [ 5.4521e-01,  1.2924e+00, -1.9031e-01,  ...,  4.7603e-02,\n",
       "               1.7241e+00, -1.0002e+00],\n",
       "             ...,\n",
       "             [ 4.9768e-02,  1.6369e+00, -1.8572e-01,  ..., -7.5291e-02,\n",
       "               3.6897e+00, -1.1238e+00],\n",
       "             [-9.6341e-02,  1.3252e+00, -1.1345e-01,  ..., -9.0504e-01,\n",
       "               3.9211e+00, -1.0618e+00],\n",
       "             [ 3.9368e-02,  1.6338e+00, -1.7518e-01,  ..., -3.9679e-02,\n",
       "               3.7395e+00, -1.1432e+00]],\n",
       "  \n",
       "            [[-8.5318e-02, -5.0549e-03,  1.8199e+00,  ...,  3.3701e-01,\n",
       "              -1.2126e+00, -7.9893e-02],\n",
       "             [ 5.0266e-01, -4.3630e-01, -1.7187e+00,  ...,  1.6472e-01,\n",
       "               4.6963e-01,  1.2524e+00],\n",
       "             [ 1.9592e-01,  1.5726e-03,  1.1578e+00,  ...,  6.4496e-01,\n",
       "              -6.1938e-01,  1.2030e-01],\n",
       "             ...,\n",
       "             [ 6.2853e-01, -7.6749e-01,  3.8992e+00,  ...,  7.2322e-01,\n",
       "              -2.1454e+00,  2.1041e-01],\n",
       "             [ 9.9720e-01, -1.0300e+00,  2.0191e+00,  ...,  3.2456e-01,\n",
       "              -1.1709e+00,  6.0874e-02],\n",
       "             [ 6.5969e-01, -8.0843e-01,  4.0804e+00,  ...,  7.6266e-01,\n",
       "              -2.2430e+00,  1.7994e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[ 3.6574e-01,  1.1903e+00,  9.8930e-01,  ..., -3.6188e-01,\n",
       "              -6.4226e-01,  4.6684e-01],\n",
       "             [ 2.4320e+00, -4.2003e-01,  5.5116e-01,  ..., -2.1063e+00,\n",
       "              -2.5617e-01, -8.3429e-01],\n",
       "             [ 1.0009e+00,  4.1185e-01,  1.8455e+00,  ..., -2.2559e-01,\n",
       "              -3.6371e-01,  4.9991e-01],\n",
       "             ...,\n",
       "             [ 1.3961e+00,  5.7823e-01,  3.3300e+00,  ...,  5.7331e+00,\n",
       "               1.2755e-01, -2.1673e+00],\n",
       "             [ 2.7557e+00,  2.1018e-01,  3.1832e+00,  ...,  5.2571e+00,\n",
       "              -4.7276e-01, -2.9170e+00],\n",
       "             [ 1.3755e+00,  5.1930e-01,  3.3370e+00,  ...,  5.9363e+00,\n",
       "               1.1298e-01, -2.2010e+00]],\n",
       "  \n",
       "            [[ 5.2429e-02, -1.4332e-01,  8.8792e-02,  ..., -2.7514e-01,\n",
       "              -2.6421e-01, -3.7105e-01],\n",
       "             [ 3.1488e-01, -6.8475e-01, -2.4824e-01,  ..., -9.7153e-01,\n",
       "              -9.3588e-01, -1.6398e+00],\n",
       "             [ 7.3513e-01,  4.5702e-01, -9.7447e-01,  ..., -7.1012e-01,\n",
       "              -9.8893e-01, -3.9196e-01],\n",
       "             ...,\n",
       "             [-2.2036e-01,  7.8090e-01, -9.4945e-01,  ..., -1.1649e+00,\n",
       "              -4.0440e+00, -5.5184e-01],\n",
       "             [-1.1075e+00,  2.4947e-01, -6.5430e-01,  ..., -1.7041e+00,\n",
       "              -4.9874e+00, -1.0262e+00],\n",
       "             [-2.7656e-01,  8.4039e-01, -9.3745e-01,  ..., -1.2238e+00,\n",
       "              -4.1358e+00, -5.8057e-01]],\n",
       "  \n",
       "            [[-1.9041e-01, -1.0983e+00, -1.3844e-01,  ..., -1.7704e-01,\n",
       "              -1.6084e+00, -1.6774e+00],\n",
       "             [-8.3762e-01, -1.4886e+00, -4.6962e-01,  ...,  1.7815e-01,\n",
       "              -1.3161e+00, -6.0492e-01],\n",
       "             [-1.0768e-01, -8.0412e-01, -1.3242e+00,  ..., -4.3617e-01,\n",
       "              -2.0145e+00, -8.9839e-01],\n",
       "             ...,\n",
       "             [ 7.9296e-01, -3.6966e-01,  1.2761e-01,  ..., -5.1042e-01,\n",
       "              -1.4039e+00, -1.4803e+00],\n",
       "             [ 1.5778e-01, -9.9760e-01,  3.2951e-01,  ...,  2.6292e-01,\n",
       "              -1.2221e+00, -1.6887e+00],\n",
       "             [ 8.4111e-01, -3.8233e-01,  2.3182e-01,  ..., -5.1156e-01,\n",
       "              -1.3926e+00, -1.5235e+00]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[-4.0222e-01, -1.3492e-01,  2.3634e-01,  ...,  6.6472e-02,\n",
       "               3.5365e-01,  1.7240e-02],\n",
       "             [ 9.9387e-01,  1.4693e+00,  1.7769e+00,  ..., -1.1020e+00,\n",
       "               5.5229e-01, -2.3767e-01],\n",
       "             [-6.0168e-01, -6.5283e-02,  2.0450e-01,  ..., -4.3109e-02,\n",
       "               3.8749e-01,  5.2931e-01],\n",
       "             ...,\n",
       "             [-9.7931e-01, -5.7004e-01,  9.1523e-01,  ..., -5.6859e-02,\n",
       "               8.1573e-01,  1.2169e+00],\n",
       "             [ 2.3384e-01,  9.2300e-01,  2.3591e+00,  ..., -1.4308e+00,\n",
       "               8.9592e-01,  8.2579e-01],\n",
       "             [-9.9361e-01, -5.7807e-01,  9.2539e-01,  ..., -8.7061e-02,\n",
       "               7.7379e-01,  1.1757e+00]],\n",
       "  \n",
       "            [[-3.2203e-01,  2.2255e-01, -3.0696e-02,  ..., -1.0983e-01,\n",
       "              -1.5164e-01, -1.3908e-01],\n",
       "             [-5.7625e-01,  1.3223e-01,  5.5197e-01,  ...,  1.2983e+00,\n",
       "               1.6463e-01,  1.6416e+00],\n",
       "             [-8.8620e-04,  1.3972e-01, -1.0652e+00,  ...,  7.9735e-01,\n",
       "              -4.8895e-01, -5.7817e-01],\n",
       "             ...,\n",
       "             [-2.9517e-01,  3.9489e-01, -4.7385e-01,  ...,  1.5720e-01,\n",
       "              -4.0356e-01, -8.1057e-01],\n",
       "             [-7.6714e-02,  4.5322e-01,  8.6148e-01,  ...,  6.5187e-01,\n",
       "              -3.1327e-01,  7.1030e-01],\n",
       "             [-2.3619e-01,  3.7388e-01, -4.7233e-01,  ...,  1.4813e-01,\n",
       "              -4.2750e-01, -8.3750e-01]],\n",
       "  \n",
       "            [[-3.4737e-01, -6.6538e-01, -1.4102e-01,  ..., -3.7922e-01,\n",
       "               7.7607e-02, -3.1144e-01],\n",
       "             [ 4.1450e-01, -1.2975e+00,  8.3287e-02,  ...,  1.1104e+00,\n",
       "               1.5150e+00,  1.5179e+00],\n",
       "             [-4.3630e-02, -1.9480e+00, -3.4169e-01,  ..., -1.9637e-01,\n",
       "               7.5193e-02, -1.2153e-01],\n",
       "             ...,\n",
       "             [-9.5914e-02, -4.2690e-01, -4.3369e-01,  ..., -7.2699e-02,\n",
       "               9.6740e-02, -3.9675e-01],\n",
       "             [-3.1023e-02, -5.7642e-01,  3.6555e-01,  ...,  6.7368e-01,\n",
       "               1.3400e+00,  8.5771e-01],\n",
       "             [-5.4967e-02, -3.6839e-01, -4.1169e-01,  ..., -6.9794e-02,\n",
       "               1.0722e-01, -4.1704e-01]],\n",
       "  \n",
       "            ...,\n",
       "  \n",
       "            [[-4.7797e-02, -2.0053e-01,  1.6933e-01,  ..., -3.2388e-01,\n",
       "              -1.0444e-01, -3.7925e-02],\n",
       "             [-2.8723e-01, -7.0007e-02,  5.4297e-01,  ..., -7.1863e-01,\n",
       "               6.3360e-02,  1.5822e-01],\n",
       "             [-6.5045e-01, -2.5599e-01,  9.9382e-01,  ..., -4.8836e-01,\n",
       "               8.2334e-01,  4.3500e-02],\n",
       "             ...,\n",
       "             [-1.7043e-01, -6.8230e-01,  7.3170e-01,  ...,  2.6212e-01,\n",
       "               8.2788e-01, -3.0503e-01],\n",
       "             [-4.2758e-01, -1.6562e-01,  3.4579e-01,  ..., -1.8357e-01,\n",
       "               4.3252e-01, -3.0873e-01],\n",
       "             [-1.6493e-01, -6.7484e-01,  6.8031e-01,  ...,  2.5228e-01,\n",
       "               8.2553e-01, -3.1823e-01]],\n",
       "  \n",
       "            [[ 8.3323e-02,  3.0383e-01, -6.9427e-02,  ...,  4.5188e-02,\n",
       "              -6.4996e-02,  7.5110e-02],\n",
       "             [ 2.9699e-01, -9.5230e-01, -7.1017e-01,  ..., -7.6028e-01,\n",
       "              -9.2141e-01, -1.4632e+00],\n",
       "             [-2.8251e-01, -3.9354e-01, -8.7663e-01,  ...,  2.3163e-01,\n",
       "              -2.4260e-01,  2.0104e-01],\n",
       "             ...,\n",
       "             [-3.9468e-01, -1.3828e-01,  6.3829e-01,  ...,  8.4154e-01,\n",
       "              -8.1455e-01,  4.0797e-01],\n",
       "             [-6.9137e-02, -5.6917e-01,  4.7718e-02,  ..., -3.1876e-01,\n",
       "              -1.0225e+00, -4.3917e-01],\n",
       "             [-4.0118e-01, -1.0799e-01,  6.8332e-01,  ...,  8.2821e-01,\n",
       "              -8.1812e-01,  3.9673e-01]],\n",
       "  \n",
       "            [[-1.1143e-01, -3.3076e-02, -6.2504e-02,  ..., -5.3652e-02,\n",
       "               1.4412e-01, -1.1772e-01],\n",
       "             [ 6.3368e-01,  5.2622e-01, -8.3218e-01,  ..., -3.6903e-01,\n",
       "              -2.9545e-01, -2.1842e-01],\n",
       "             [ 1.0448e-01,  8.1367e-01,  5.6495e-01,  ...,  2.1679e-01,\n",
       "              -1.7677e+00, -6.6624e-02],\n",
       "             ...,\n",
       "             [ 1.9913e-01,  4.0581e-01,  9.2101e-01,  ...,  6.9523e-01,\n",
       "              -9.0900e-01,  5.6784e-01],\n",
       "             [ 5.1563e-01,  6.7310e-01,  4.6081e-01,  ...,  1.4200e-01,\n",
       "              -2.2363e-01,  8.0402e-01],\n",
       "             [ 1.6800e-01,  4.1659e-01,  9.6482e-01,  ...,  7.0922e-01,\n",
       "              -8.6311e-01,  5.3320e-01]]]]], grad_fn=<StackBackward>)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.forward(chat_history_ids)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model(chat_history_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(q[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = answer - context\n",
    "if len(answer) < 2:\n",
    "    r2 = 0\n",
    "else:\n",
    "    vec_a, vec_b = answer, answer\n",
    "    r2 = sum(vec_a*vec_b) / sum(abs(vec_a)*abs(vec_b))\n",
    "    r2 = -F.logsigmoid(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Adam' object has no attribute 'zerograd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-67e55e3af91e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzerograd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mr2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Adam' object has no attribute 'zerograd'"
     ]
    }
   ],
   "source": [
    "optimizer.zerograd()\n",
    "r2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = q\n",
    "v2 = a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50256, 15496, 50256, 15496, 50256, 15496,  5145, 50256, 15496,  5145,\n",
       "         50256, 17250,  5145, 50256]),\n",
       " tensor([15496,  5145,  1058,    35, 50256]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1,v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0057,  0.0238, -0.0812,  ..., -0.0227, -0.0378, -0.0031],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings()(v2).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(size=(3,tokenizer.vocab_size))\n",
    "x[:,-1] = 1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[103.3320,  96.6074,  96.4836,  ..., 109.7336, 112.1754, 125.7009],\n",
       "         [107.1582,  98.9198,  98.5507,  ..., 115.2873, 114.8348, 128.2074],\n",
       "         [116.1806, 104.2458, 106.2476,  ..., 122.1931, 125.4122, 139.5822]],\n",
       "        grad_fn=<MmBackward>),\n",
       " tensor([[ -11.4564,  -17.4578,  -19.1987,  ...,  -16.9864,  -16.7421,\n",
       "            -7.3800],\n",
       "         [-206.5140, -198.9877, -200.6138,  ..., -230.4619, -229.1432,\n",
       "          -207.2097],\n",
       "         [-180.3867, -184.7284, -181.1404,  ..., -203.2211, -201.3414,\n",
       "          -167.4208],\n",
       "         [-157.9265, -160.9077, -163.2302,  ..., -188.6371, -188.0802,\n",
       "          -162.3012],\n",
       "         [-200.9722, -208.1657, -210.0039,  ..., -236.2414, -233.3469,\n",
       "          -213.2841],\n",
       "         [-179.1016, -196.8122, -194.7751,  ..., -215.5321, -212.9505,\n",
       "          -185.1048]], grad_fn=<MmBackward>))"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = model(v1)[0]\n",
    "B = model(v2)[0]\n",
    "A,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.1308, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "A = model(v1)[0]\n",
    "B = model(v2)[0]\n",
    "max_len = np.max([len(A.detach().numpy()),len(B.detach().numpy())])\n",
    "print(max_len)\n",
    "extra = torch.zeros(size=(max_len-len(A.detach().numpy()),tokenizer.vocab_size))\n",
    "extra[:,-1] = 1\n",
    "# A = torch.cat([A , extra ])\n",
    "B = torch.cat((B , torch.tensor([torch.ones(tokenizer.vocab_size)]*(max_len-len(B.detach().numpy()))) ))\n",
    "A, B\n",
    "\n",
    "A , B = torch.cat([torch.softmax(A, dim=-1), extra]), torch.softmax(B, dim=-1)\n",
    "A , B\n",
    "# loss = F.cosine_similarity(A,B, dim=-2)\n",
    "loss = -torch.mm(A, B.transpose(1, 0)).sum()\n",
    "\n",
    "# loss = -F.cosine_similarity(torch.softmax(A, dim=-1),torch.softmax(B, dim=-1))\n",
    "# loss = 0\n",
    "# for i,j in zip(v1,v2):\n",
    "#     loss += i*j\n",
    "# # loss = -model(v1, past=None)[0].sum()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' shore Term marine'"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "logits = model(v1)[0]\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "prev_input = torch.multinomial(probs, num_samples=1)\n",
    "decode(prev_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[75.3702, 73.6392, 72.8180,  ..., 81.0872, 82.3673, 87.0727],\n",
       "        [74.3895, 72.5374, 71.8523,  ..., 80.4320, 81.0682, 86.7291],\n",
       "        [73.9969, 72.2922, 71.6807,  ..., 80.3298, 80.4573, 86.1744]],\n",
       "       grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! :D'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9997, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.get_output_embeddings()(model.get_input_embeddings()(v1)).mean(dim=0)\n",
    "y = model.get_output_embeddings()(model.get_input_embeddings()(v2)).mean(dim=0)\n",
    "-torch.log(F.cosine_similarity(x,y, dim=-1))\n",
    "F.cosine_similarity(x,y, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-122.8433, -150.5232, -149.6123,  ..., -168.9333, -161.6223,\n",
       "         -133.7793],\n",
       "        [-204.0837, -205.4568, -208.4964,  ..., -240.3041, -233.0705,\n",
       "         -217.4363],\n",
       "        [-205.8837, -210.4453, -214.1883,  ..., -246.4192, -239.3104,\n",
       "         -219.0015]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(v1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-242-6ee6c6e0f6f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-10d376e69163>\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(token_ids)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces)\u001b[0m\n\u001b[0;32m    753\u001b[0m                 \u001b[0mcurrent_sub_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcurrent_sub_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m             \u001b[0msub_texts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_tokens_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_sub_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\tokenization_gpt2.py\u001b[0m in \u001b[0;36mconvert_tokens_to_string\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconvert_tokens_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;34m\"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyte_decoder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, NoneType found"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['<pad>'])\n",
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check learned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-e5569e72ff68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# generated a response while limiting the total chat history to 1000 tokens,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     chat_history_ids = B.generate(input_ids, max_length=1000, \n\u001b[1;32m---> 24\u001b[1;33m                                       \u001b[0mpad_token_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m#                                       num_beams=3,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#                                       early_stopping=True,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_kwargs)\u001b[0m\n\u001b[0;32m    492\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m             )\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36m_generate_no_beam_search\u001b[1;34m(self, input_ids, cur_len, max_length, min_length, do_sample, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id, batch_size, attention_mask, use_cache, model_kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             )\n\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    729\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 731\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    732\u001b[0m         )\n\u001b[0;32m    733\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m             )\n\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcross_attn_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# add cross attentions if we output attention weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m         \u001b[0mfeed_forward_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         \u001b[1;31m# residual connection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfeed_forward_hidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[0mh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1017\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1019\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1020\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msize_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "A = model\n",
    "B = model\n",
    "chat_history_ids = tokenizer.encode(tokenizer.bos_token + \"Hello\" + tokenizer.eos_token, return_tensors='pt')\n",
    "for frame in range(15):\n",
    "    epsilon = epsilon_by_frame(frame)\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "#     new_user_input_ids = tokenizer.encode(tokenizer.eos_token, return_tensors='pt')\n",
    "    input_ids = chat_history_ids\n",
    "    chat_history_ids = A.generate(input_ids, max_length=1000, \n",
    "                             pad_token_id=tokenizer.eos_token_id, \n",
    "#                              num_beams=3,\n",
    "# #                              num_return_sequences=1,\n",
    "#                              early_stopping=True,\n",
    "#                              no_repeat_ngram_size=3\n",
    "                            ) if frame > 0 else input_ids\n",
    "    question = chat_history_ids[:, input_ids.shape[-1]:][0] if frame > 0 else input_ids[0]\n",
    "    print(\"User: {}\".format(decode(question)))\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    input_ids = chat_history_ids # if step > 0 else new_user_input_ids\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = B.generate(input_ids, max_length=1000, \n",
    "                                      pad_token_id=tokenizer.eos_token_id,\n",
    "#                                       num_beams=3,\n",
    "#                                       early_stopping=True,\n",
    "#                                       num_return_sequences=3,\n",
    "#                                       no_repeat_ngram_size=3\n",
    "                                     )\n",
    "\n",
    "    # pretty print last output tokens from bot\n",
    "    answer = chat_history_ids[:, input_ids.shape[-1]:][0]\n",
    "    print(\"DialoGPT: {}\".format(decode(answer)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292, 292)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.base_model.parameters())), len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-5168ea7c7dbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "model.get_head_mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
